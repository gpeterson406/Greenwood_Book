<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="A Second Semester Statistics Course with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A Second Semester Statistics Course with R">

<title>A Second Semester Statistics Course with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Beanplots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Chapter summary</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Summary of important R code</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for Prisoner Rating data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and table plots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient (Optional section)</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomizing inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section8-8" class="section level2">
<h2><span class="header-section-number">8.8</span> Case study: First year college GPA and SATs</h2>
<p>Many universities require students to have certain test scores in order to be admitted into their institutions. They obviously must think that those scores are useful predictors of student success to use them in this way. Quality assessments of recruiting classes are also based on their test scores. The Educational Testing Service (the company behind such fun exams as the SAT and GRE) collected a data set to validate their SAT on <span class="math inline">\(n=1000\)</span> students from an unnamed Midwestern university; the data set is available in the <code>openintro</code> package <span class="citation">(Diez, Barr, and Cetinkaya-Rundel <a href="#ref-R-openintro">2017</a>)</span> in the <code>satGPA</code> data set. It is unclear from the documentation whether a random sample was collected, in fact it looks like it certainly wasn’t a random sample of all incoming students at a large university (more later). What potential issues would arise if a company was providing a data set to show the performance of their test and it was not based on a random sample?</p>
<p>We will proceed assuming they used good methods in developing their test (there are sophisticated statistical models underlying the development of the SAT and GRE) and in obtaining a data set for testing out the performance of their tests that is at least representative of the students (or some types of students) at this university. They provided information on the <em>Sex</em> (<code>sex</code>) of the students (coded 1 and 2 with possibly 1 for males and 2 for females – but should this even be displayed in a plot with correlations?), <em>SAT Verbal</em> (<code>SATV</code>) and <em>Math</em> (<code>SATM</code>) percentiles (these are not the scores but the ranking percentile that each score translated to in a particular year), <em>High School GPA</em> (<code>HSGPA</code>), and <em>First Year</em> of college <em>GPA</em> (<code>FYGPA</code>). Our interests here are in whether the two SAT percentiles are (together?) related to the first year college GPA, describing the size of their impacts and assessing the predictive potential of SAT-based measures for first year in college GPA. There are certainly other possible research questions that can be addressed with these data but this will keep us focused.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(openintro)
<span class="kw">data</span>(satGPA)
<span class="kw">require</span>(tibble)
satGPA &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(satGPA)
<span class="kw">require</span>(psych)
<span class="kw">pairs.panels</span>(satGPA[,<span class="op">-</span><span class="dv">4</span>], <span class="dt">ellipse=</span>F, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure8-13"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-13-1.png" alt="Scatterplot matrix of SAT and GPA data set." width="672" />
<p class="caption">
Figure 2.153: Scatterplot matrix of SAT and GPA data set.
</p>
</div>
<p>There are positive relationships in Figure <a href="8-8-section8-8.html#fig:Figure8-13">2.153</a> among all the pre-college measures and the <em>college GPA</em> but none are above the moderate strength level. The <em>HSGPA</em> has a highest correlation with first year of college results but its correlation is not that strong. Maybe together in a model the SAT percentiles can also be useful… Also note that plot shows an odd <em>HSGPA</em> of 4.5 that probably should be removed<a href="#fn100" class="footnoteRef" id="fnref100"><sup>100</sup></a> if that variable is going to be used (<em>HSGPA</em> was not used in the following models so the observation remains in the data).</p>
<p>In MLR, the modeling process is a bit more complex and often involves more than one model, so we will often avoid the 6+ steps in testing initially and try to generate a model we can use in that more specific process. In this case, the first model of interest using the two SAT percentiles,</p>
<p><span class="math display">\[\text{FYGPA}_i = \beta_0 + \beta_{\text{SATV}}\text{SATV}_i 
+ \beta_{\text{SATM}}\text{SATM}_i +\varepsilon_i,\]</span></p>
<p>looks like it might be worth interrogating further so we can jump straight into considering the 6+ steps involved in hypothesis testing for the two slope coefficients. We will use <span class="math inline">\(t\)</span>-based inferences, assuming that we can trust the assumptions.</p>
<p>Note that this is not a randomized experiment but we can assume that it is representative of the students at that single university. We would not want to extend these inferences to other universities (who might be more or less selective) or to students who did not get into this university and, especially, not to students that failed to complete the first year. The second and third constraints point to a severe limitation in this research – only students who were accepted, went to, and finished one year at this university could be studied. Lower SAT percentile students might not have been allowed in or may not have finished the first year and higher SAT students might have been attracted to other more prestigious institutions. So the scope of inference is just limited to students that were invited and chose to attend this institution and successfully completed one year of courses. It is hard to know if the SAT “works” when the inferences are so restricted in who they might apply to…</p>
<p>The following code fits the model of interest, provides a model summary, and the diagnostic plots, allowing us to consider the tests of interest:</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gpa1 &lt;-<span class="st"> </span><span class="kw">lm</span>(FYGPA<span class="op">~</span>SATV<span class="op">+</span>SATM, <span class="dt">data=</span>satGPA)
<span class="kw">summary</span>(gpa1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = FYGPA ~ SATV + SATM, data = satGPA)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.19647 -0.44777  0.02895  0.45717  1.60940 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 0.007372   0.152292   0.048    0.961
## SATV        0.025390   0.002859   8.879  &lt; 2e-16
## SATM        0.022395   0.002786   8.037 2.58e-15
## 
## Residual standard error: 0.6582 on 997 degrees of freedom
## Multiple R-squared:  0.2122, Adjusted R-squared:  0.2106 
## F-statistic: 134.2 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(gpa1, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for GPA model with SATV and SATM&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure8-14"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-14-1.png" alt="Diagnostic plots for the \(\text{FYGPA}\sim\text{ SATV }+\text{ SATM}\) model." width="960" />
<p class="caption">
Figure 2.154: Diagnostic plots for the <span class="math inline">\(\text{FYGPA}\sim\text{ SATV }+\text{ SATM}\)</span> model.
</p>
</div>
<ol style="list-style-type: decimal">
<li><p>Hypotheses of interest:</p>
<ul>
<li><p><span class="math inline">\(H_0: \beta_\text{SATV}=0\)</span> given <em>SATM</em> in the model vs <span class="math inline">\(H_A: \beta_\text{SATV}\ne 0\)</span> given <em>SATM</em> in the model.</p></li>
<li><p><span class="math inline">\(H_0: \beta_\text{SATM}=0\)</span> given <em>SATV</em> in the model vs <span class="math inline">\(H_A: \beta_\text{SATM}\ne 0\)</span> given <em>SATV</em> in the model.</p></li>
</ul></li>
<li><p>Validity conditions:</p>
<ul>
<li><p><strong>Quantitative variables condition:</strong></p>
<ul>
<li>The variables used here in this model are quantitative. Note that <em>sex</em> was plotted in the previous scatterplot matrix and is not quantitative – we will explore its use later.</li>
</ul></li>
</ul>

<ul>
<li><p><strong>Independence of observations:</strong></p>
<ul>
<li>With a sample from a single university from (we are assuming) a single year of students, there is no particular reason to assume a violation of the independence assumption.</li>
</ul></li>
<li><p><strong>Linearity of relationships:</strong></p>
<ul>
<li><p>The initial scatterplots (Figure <a href="8-8-section8-8.html#fig:Figure8-13">2.153</a>) do not show any clear nonlinearities with each predictor used in this model.</p></li>
<li><p>The Residuals vs Fitted and Scale-Location plots (Figure <a href="8-8-section8-8.html#fig:Figure8-14">2.154</a>) do not show much more than a football shape, which is our desired result.</p>
<ul>
<li>Together, there is no suggestion of a violation of the linearity assumption.</li>
</ul></li>
</ul></li>
<li><p><strong>Multicollinearity checked for:</strong></p>
<ul>
<li><p>The original scatterplots suggest that there is some collinearity between the two SAT percentiles with a correlation of 0.47. That is actually a bit lower than one might expect and suggests that each score must be measuring some independent information about different characteristics of the students.</p></li>
<li><p>VIFs also do not suggest a major issue with multicollinearity in the model with the VIFs for both variables the same at 1.278<a href="#fn101" class="footnoteRef" id="fnref101"><sup>101</sup></a>. This suggests that both SEs are about 13% larger than they otherwise would have been due to shared information between the two predictor variables.</p></li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(gpa1)</code></pre></div>
<pre><code>##     SATV     SATM 
## 1.278278 1.278278</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">vif</span>(gpa1))</code></pre></div>
<pre><code>##    SATV    SATM 
## 1.13061 1.13061</code></pre></li>
<li><p><strong>Equal (constant) variance:</strong></p>
<ul>
<li>There is no clear change in variability as a function of fitted values so no indication of a violation of the constant variance of residuals assumption.</li>
</ul></li>
<li><p><strong>Normality of residuals:</strong></p>
<ul>
<li>There is a minor deviation in the upper tail of the residual distribution from normality. It is not pushing towards having larger values than a normal distribution would generate so should not cause us any real problems with inferences from this model. Note that this upper limit is likely due to using GPA as a response variable and it has an upper limit. This is an example of a potentially <strong><em>censored</em></strong> variable. For a continuous variable it is possible that the range of a measurement scale doesn’t distinguish among subjects who differ once they pass a certain point. For example, a 4.0 high school student is likely going to have a high first year college GPA, on average, but there is no room for variability in college GPA up, just down once you are at the top of the GPA scale. For students more in the middle of the range, they can vary up or down. So in some places you can get symmetric distributions around the mean and in others you cannot. There are specific statistical models for these types of responses that are beyond our scope. In this situation, failing to account for the censoring may push some slopes toward 0 a little because we can’t have responses over 4.0 in college GPA to work with.</li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li>There are no influential points. In large data sets, the influence of any point is decreased and even high leverage and outlying points can struggle to have any impacts at all on the results.</li>
</ul></li>
</ul></li>
</ol>
<p>So we are fairly comfortable with all the assumptions being at least not clearly violated and so the inferences from our model should be relatively trustworthy.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Calculate the test statistics:</p>
<ul>
<li><p>For <em>SATV</em>: <span class="math inline">\(t=\dfrac{0.02539}{0.002859}=8.88\)</span> with <span class="math inline">\(df=997\)</span>.</p></li>
<li><p>For <em>SATM</em>: <span class="math inline">\(t=\dfrac{0.02240}{0.002786}=8.04\)</span> with <span class="math inline">\(df=997\)</span>.</p></li>
</ul></li>
<li><p>Find the p-values:</p>
<ul>
<li><p>For <em>SATV</em>: p-value <span class="math inline">\(&lt;0.0001\)</span></p></li>
<li><p>For <em>SATM</em>: p-value <span class="math inline">\(&lt;0.0001\)</span></p></li>
</ul></li>
<li><p>Decisions:</p>
<ul>
<li><p>For <em>SATV</em>: Reject <span class="math inline">\(H_0\)</span> because there is almost no chance of observing a test statistic as extreme or more extreme than was observed if there really were no linear relationship between <em>FYGPA</em> and <em>SATV</em>, in a model that controls for <em>SATM</em>.</p></li>
<li><p>For <em>SATM</em>: Reject <span class="math inline">\(H_0\)</span> because there is almost no chance of observing a test statistic as extreme or more extreme than was observed if there really were no linear relationship between <em>FYGPA</em> and <em>SATM</em>, in a model that controls for <em>SATV</em>.</p></li>
</ul></li>
<li><p>Conclusions and Scope of Inference:</p>
<ul>
<li><p>For <em>SATV</em>: There is strong evidence to reject the null hypothesis of no linear relationship between <em>SATV</em> and <em>FYGPA</em> (<span class="math inline">\(t_{997}=8.88\)</span>, p-value &lt; 0.0001) and conclude that, in fact, there is a linear relationship between <em>SATV</em> percentile and the first year of college <em>GPA</em>, after controlling for the <em>SATM</em> percentile, in the population of students that completed their first year at this university.</p></li>
<li><p>For <em>SATM</em>: There is strong evidence to reject the null hypothesis of no linear relationship between <em>SATM</em> and <em>FYGPA</em> (<span class="math inline">\(t_{997}=8.04\)</span>, p-value &lt; 0.0001)and conclude that, in fact, there is a linear relationship between <em>SATM</em> percentile and the first year of college <em>GPA</em>, after controlling for the <em>SATV</em> percentile, in the population of students that completed their first year at this university.</p></li>
<li><p>Note that neither inference is causal because there was no random assignment of SAT percentiles to the subjects. The inferences are also limited to students who stayed in school long enough to get a <em>GPA</em> from their first year of college at this university.</p></li>
</ul></li>
</ol>
<p>The model seems to valid and have predictors with small p-values, but note how much of the variation is not explained by the model. It only explains 21.22% of the variation in the responses. So we found evidence that these variables are useful in predicting the responses, but are they useful enough to use for decisions on admitting students? By quantifying the size of the estimated results, we can add to the information about how potentially useful this model might be. The estimated MLR model is</p>
<p><span class="math display">\[\widehat{\text{FYGPA}}_i=0.00737+0.0254\cdot\text{SATV}_i
+0.0224\cdot\text{SATM}_i\ .\]</span></p>
<p>So for a 1 percent increase in the <em>SATV</em> percentile, we expect, on average, to get a 0.0254 point change in <em>GPA</em>, after controlling for <em>SATM</em> percentile. Similarly, for a 1 percent increase in the <em>SATM</em> percentile, we expect, on average, to get a 0.0224 point change in <em>GPA</em>, after controlling for <em>SATV</em> percentile. While this is a correct interpretation of the slope coefficients, it is often easier to assess “practical” importance of the results by considering how much change this implies over the range of observed predictor values.</p>
<p>The term-plots (Figure <a href="8-8-section8-8.html#fig:Figure8-15">2.155</a>) provide a visualization of the “size” of the differences in the response variable explained by each predictor. The <em>SATV</em> term-plot shows that for the range of percentiles from around the 30<sup>th</sup> percentile to the 70<sup>th</sup> percentile, the mean first year <em>GPA</em> is predicted to go from approximately 1.9 to 3.0. That is a pretty wide range of differences in GPAs across the range of observed percentiles. This looks like a pretty interesting and important change in the mean first year GPA across that range of different SAT percentiles. Similarly, the <em>SATM</em> term-plot shows that the <em>SATM</em> percentiles were observed to range between around the 30<sup>th</sup> percentile and 70<sup>th</sup> percentile and predict mean GPAs between 1.95 and 2.8. It seems that the SAT Verbal percentiles produce slightly more impacts in the model, holding the other variable constant, but that both are important variables. The 95% confidence intervals for the means in both plots suggest that the results are fairly precisely estimated – there is little variability around the predicted means in each plot. This is mostly a function of the sample size as opposed to the model itself explaining most of the variation in the responses.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(gpa1))</code></pre></div>

<div class="figure"><span id="fig:Figure8-15"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-15-1.png" alt="Term-plots for the \(\text{FYGPA}\sim\text{SATV} + \text{SATM}\) model." width="576" />
<p class="caption">
Figure 2.155: Term-plots for the <span class="math inline">\(\text{FYGPA}\sim\text{SATV} + \text{SATM}\)</span> model.
</p>
</div>
<p>These plots also inform the types of students attending this university and successfully completing the first year of school. This seems like a good, but maybe not great, institution with few students scoring over the 75<sup>th</sup> percentile on either SAT Verbal or Math (at least that ended up in this data set). This result makes questions about their sampling mechanism re-occur as to who this data set might actually be representative of…</p>
<p>The confidence intervals also help us pin down the uncertainty in each estimated slope coefficient. As always, the “easy” way to get 95% confidence intervals is using the <code>confint</code> function:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(gpa1)</code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.29147825 0.30622148
## SATV         0.01977864 0.03100106
## SATM         0.01692690 0.02786220</code></pre>
<p>So, for a 1 percent increase in the <em>SATV</em> percentile, we are 95% confident that the true mean <em>FYGPA</em> changes between 0.0198 and 0.031 points, in the population of students who completed this year at this institution, after controlling for <em>SATM</em>. The <em>SATM</em> result is similar with an interval from 0.0169 and 0.0279. Both of these intervals might benefit from re-scaling the interpretation to, say, a 10 percentile increase in the predictor variable, with the change in the <em>FYGPA</em> for that level of increase of <em>SATV</em> providing an interval from 0.198 to 0.31 points and for <em>SATM</em> providing an interval from 0.169 to 0.279. So a boost of 10% in either exam percentile likely results in a noticeable but not huge average <em>FYGPA</em> increase.</p>
<p>One final use of these methods is to do prediction and generate prediction intervals, which could be quite informative for a student considering going to this university who has a particular set of SAT scores. For example, suppose that the student is interested in the average <em>FYGPA</em> to expect with <em>SATV</em> at the 30<sup>th</sup> percentile and <em>SATM</em> at the 60<sup>th</sup> percentile. The predicted mean value is</p>
<p><span class="math display">\[\begin{array}{rl}
\hat{\mu}_{\text{GPA}_i} &amp;= 0.00737 + 0.0254\cdot\text{SATV}_i 
+ 0.0224\cdot\text{SATM}_i \\
&amp;= 0.00737 + 0.0254*30 + 0.0224*60 = 2.113.
\end{array}\]</span></p>
<p>This result and the 95% confidence interval for the mean student <em>GPA</em> at these scores can be found using the <code>predict</code> function as:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>))</code></pre></div>
<pre><code>##       1 
## 2.11274</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>), <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)</code></pre></div>
<pre><code>##       fit      lwr      upr
## 1 2.11274 1.982612 2.242868</code></pre>
<p>For students at the 30<sup>th</sup> percentile of <em>SATV</em> and 60<sup>th</sup> percentile of <em>SATM</em>, we are 95% confident that the true mean first year GPA is between 1.98 and 2.24 points. For an individual student, we would want the 95% prediction interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1,<span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>),<span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre></div>
<pre><code>##       fit       lwr      upr
## 1 2.11274 0.8145859 3.410894</code></pre>
<p>For a student with <em>SATV</em>=30 and <em>SATM</em>=60, we are 95% sure that their first year GPA will be between 0.81 and 3.4 points. You can see that while we are very certain about the mean in this situation, there is a lot of uncertainty in the predictions for individual students. The PI is so wide as to almost not be useful.</p>
<p>To support this difficulty in getting a precise prediction for a new student, review the original scatterplots: there is quite a bit of vertical variability in first year <em>GPA</em>s for each level of any of the predictors. The residual SE, <span class="math inline">\(\hat{\sigma}\)</span>, is also informative in this regard – remember that it is the standard deviation of the residuals around the regression line. It is 0.6582, so the SD of new observations around the line is 0.66 GPA points and that is pretty large on a GPA scale. Remember that if the residuals meet our assumptions and follow a normal distribution around the line, observations within 2 or 3 SDs of the mean would be expected which is a large range of GPA values. Figure <a href="8-8-section8-8.html#fig:Figure8-16">2.156</a> remakes both term-plots, holding the other predictor at its mean, and adds the 95% prediction intervals to show the difference in variability between estimating the mean and pinning down the value of a new observation. The R code is very messy and rarely needed, but hopefully this helps reinforce the differences in these two types of intervals – to make them in MLR, you have to fix all but one of the predictor variables and we usually do that by fixing the other variables at their means.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Remake effects plots with 95% PIs</span>
dv1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">24</span>,<span class="dt">to=</span><span class="dv">76</span>,<span class="dt">length.out=</span><span class="dv">50</span>), <span class="dt">SATM=</span><span class="kw">rep</span>(<span class="fl">54.4</span>,<span class="dv">50</span>))
dm1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="kw">rep</span>(<span class="fl">48.93</span>,<span class="dv">50</span>), <span class="dt">SATM=</span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">29</span>,<span class="dt">to=</span><span class="dv">77</span>,<span class="dt">length.out=</span><span class="dv">50</span>))

mv1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dv1, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>))
pv1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dv1, <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>))

mm1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dm1, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>))
pm1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dm1, <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>))

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

<span class="kw">plot</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>fit, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">ylim=</span><span class="kw">c</span>(pv1<span class="op">$</span>lwr[<span class="dv">1</span>],pv1<span class="op">$</span>upr[<span class="dv">50</span>]), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;SATV Percentile&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">main=</span><span class="st">&quot;SATV Effect, CI and PI&quot;</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, pv1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, pv1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;CI&quot;</span>,<span class="st">&quot;PI&quot;</span>), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>,<span class="st">&quot;grey&quot;</span>))

<span class="kw">plot</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>fit, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">ylim=</span><span class="kw">c</span>(pm1<span class="op">$</span>lwr[<span class="dv">1</span>],pm1<span class="op">$</span>upr[<span class="dv">50</span>]), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;SATM Percentile&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">main=</span><span class="st">&quot;SATM Effect, CI and PI&quot;</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, pm1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, pm1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure8-16"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-16-1.png" alt="Term-plots for the \(\text{FYGPA}\sim\text{SATV} + \text{SATM}\) model with 95% confidence intervals (red, dashed lines) and 95% PIs (light grey, dotted lines)." width="960" />
<p class="caption">
Figure 2.156: Term-plots for the <span class="math inline">\(\text{FYGPA}\sim\text{SATV} + \text{SATM}\)</span> model with 95% confidence intervals (red, dashed lines) and 95% PIs (light grey, dotted lines).
</p>
</div>

</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-openintro">
<p>Diez, David M, Christopher D Barr, and Mine Cetinkaya-Rundel. 2017. <em>Openintro: Data Sets and Supplemental Functions from ’Openintro’ Textbooks</em>. <a href="https://CRAN.R-project.org/package=openintro" class="uri">https://CRAN.R-project.org/package=openintro</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="100">
<li id="fn100"><p>Either someone had a weighted GPA with bonus points, or more likely here, there was a coding error in the data set since only one observation was over 4.0 in the GPA data. Either way, we could remove it and note that our inferences for HSGPA do not extend above 4.0.<a href="8-8-section8-8.html#fnref100">↩</a></p></li>
<li id="fn101"><p>When there are just two predictors, the VIFs have to be the same since the proportion of information shared is the same in both directions. With more than two predictors, each variable can have a different VIF value.<a href="8-8-section8-8.html#fnref101">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="8-7-section8-7.html"><button class="btn btn-default">Previous</button></a>
<a href="8-9-section8-9.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
