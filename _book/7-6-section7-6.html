<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="Intermediate Statistics with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="Intermediate Statistics with R">

<title>Intermediate Statistics with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Beanplots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Chapter summary</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Summary of important R code</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for Prisoner Rating data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and tableplots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient (Optional section)</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomizing inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section7-6" class="section level2">
<h2><span class="header-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</h2>

<p>The previous attempts to linearize relationships imply a desire to be able to fit SLR models.  The <em>log</em>-transformations, when successful, provide the potential to apply our SLR model. There are then two options for interpretations: you can either interpret the model on the transformed scale or you can translate the SLR model on the transformed scale back to the original scale of the variables. It ends up that <em>log</em>-transformations have special interpretations on the original scales depending on whether the <em>log</em> was applied to the response variable, the explanatory variable, or both.</p>
<p><strong>Scenario 1: log(y) vs x model:</strong></p>
<p>First consider the <span class="math inline">\(\log(y) \sim x\)</span> situations where the estimated model is of the form <span class="math inline">\(\widehat{\log(y)} = b_0 + b_1x\)</span>. When only the response is <em>log</em>-transformed, some people call this a <strong><em>semi-log model</em></strong>.  But many researchers will use this model without any special considerations, as long as it provides a situation where the SLR assumptions are reasonably well-satisfied. To understand the properties and eventually the interpretation of transformed-variables models, we need to try to “reverse” our transformation. If we exponentiate<a href="#fn88" class="footnoteRef" id="fnref88"><sup>88</sup></a> both sides of <span class="math inline">\(\log(y)=b_0 + b_1x\)</span>, we get:</p>
<div class="figure"><span id="fig:Figure7-18"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-18-1.png" alt="Plot of the estimated SLR (a) and implied model for the median on the original Hectares scale (b) for the area burned vs temperature data." width="960" />
<p class="caption">
Figure 2.134: Plot of the estimated SLR (a) and implied model for the median on the original Hectares scale (b) for the area burned vs temperature data.
</p>
</div>
<ul>
<li><p><span class="math inline">\(\exp(\log(y))=\exp(b_0 + b_1x)\)</span>, <em>which is</em></p></li>
<li><p><span class="math inline">\(y=\exp(b_0 + b_1x)\)</span>, <em>which can be re-written as</em></p></li>
<li><p><span class="math inline">\(y=\exp(b_0)\exp(b_1x)\)</span>. <em>This is based on the rules for</em> <code>exp()</code> <em>where</em> <span class="math inline">\(\exp(a+b)=\exp(a)\exp(b)\)</span>.</p></li>
<li><p>Now consider what happens if we increase <span class="math inline">\(x\)</span> by 1 unit, going from <span class="math inline">\(x\)</span> to <span class="math inline">\(x+1\)</span>, providing a new predicted <span class="math inline">\(y\)</span> that we can call <span class="math inline">\(y^*\)</span>: <span class="math inline">\(y^*=\exp(b_0)\exp[b_1(x+1)]\)</span>:</p></li>
<li><p><span class="math inline">\(y^*={\color{red}{\underline{\boldsymbol{\exp(b_0)\exp(b_1x)}}}}\exp(b_1)\)</span>. <em>Now note that the underlined, bold component was the y-value for</em> <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(y^* = {\color{red}{\boldsymbol{y}}}\exp(b_1)\)</span>. <em>Found by replacing</em> <span class="math inline">\(\color{red}{\mathbf{\exp(b_0)\exp(b_1x)}}\)</span> <em>with</em> <span class="math inline">\(\color{red}{\mathbf{y}}\)</span>, <em>the value for</em> <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>So the difference in fitted values between <span class="math inline">\(x\)</span> and <span class="math inline">\(x+1\)</span> is to multiply the result for <span class="math inline">\(x\)</span> (that was predicting <span class="math inline">\(\color{red}{\mathbf{y}}\)</span>) by <span class="math inline">\(\exp(b_1)\)</span> to get to the predicted result for <span class="math inline">\(x+1\)</span> (called <span class="math inline">\(y^*\)</span>). We can then use this result to form our <span class="math inline">\(\mathit{\boldsymbol{\log(y)\sim x}}\)</span> <strong><em>slope interpretation</em></strong>:  for a 1 unit increase in <span class="math inline">\(x\)</span>, we observe a multiplicative change of <span class="math inline">\(\mathbf{exp(b_1)}\)</span> in the response. When we compute a mean on logged variables that are symmetrically distributed (this should occur if our transformation was successful) and then exponentiate the results, the proper interpretation is that the changes are happening in the <strong>median</strong> of the original responses. This is the only time in the course that we will switch our inferences to medians instead of means, and we don’t do this because we want to, we do it because it is result of modeling on the <span class="math inline">\(\log(y)\)</span> scale if successful.</p>
<p>When we are working with regression equations, slopes can either be positive or negative and our interpretations change based on this result to either result in growth (<span class="math inline">\(b_1&gt;0\)</span>) or decay (<span class="math inline">\(b_1&lt;0\)</span>) in the responses as the explanatory variable is increased. As an example, consider <span class="math inline">\(b_1=0.4\)</span> and <span class="math inline">\(\exp(b_1)=\exp(0.4)=1.492\)</span>. There are a couple of ways to interpret this on the original scale of the response variable <span class="math inline">\(y\)</span>:</p>
<p>For <span class="math inline">\(\mathbf{b_1&gt;0}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>For a 1 unit increase in <span class="math inline">\(x\)</span>, the multiplicative change in the median of <span class="math inline">\(y\)</span> is 1.492.</p></li>
<li><p>We can convert this into a <strong>percentage increase</strong> by subtracting 1 from <span class="math inline">\(\exp(0.4)\)</span>, <span class="math inline">\(1.492-1.0=0.492\)</span> and multiplying the result by 100, <span class="math inline">\(0.492*100=49.2\%\)</span>. This is interpreted as: For a 1 unit increase in <span class="math inline">\(x\)</span>, the median of <span class="math inline">\(y\)</span> increases by 49.2%.</p></li>
</ol>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">exp</span>(<span class="fl">0.4</span>)</code></pre></div>
<pre><code>## [1] 1.491825</code></pre>
<p>For <span class="math inline">\(\mathbf{b_1&lt;0}\)</span>, the change on the <em>log</em>-scale is negative and that implies on the original scale that the curve decays to 0. For example, consider <span class="math inline">\(b_1=-0.3\)</span> and <span class="math inline">\(\exp(-0.3)=0.741\)</span>. Again, there are two versions of the interpretation possible:</p>
<ol style="list-style-type: decimal">
<li><p>For a 1 unit increase in <span class="math inline">\(x\)</span>, the multiplicative change in the median of <span class="math inline">\(y\)</span> is 0.741.</p></li>
<li><p>For negative slope coefficients, the percentage decrease is calculated as <span class="math inline">\((1-\exp(b_1))*100\%\)</span>. For <span class="math inline">\(\exp(-0.3)=0.741\)</span>, this is <span class="math inline">\((1-0.741)*100=25.9\%\)</span>. This is interpreted as: For a 1 unit increase in <span class="math inline">\(x\)</span>, the median of <span class="math inline">\(y\)</span> decreases by 25.9%.</p></li>
</ol>
<p>We suspect that you will typically prefer interpretation #1 for both directions but it is important to be able think about the results in terms of <strong><em>% change of the medians</em></strong> to make the scale of change more understandable. Some examples will help us see how these ideas can be used in applications.</p>
<p>For the area burned data set, the estimated regression model is <span class="math inline">\(\log(\widehat{\text{hectares}})=-69.8+1.39\cdot\text{ Temp}\)</span>. On the original scale, this implies that the model is <span class="math inline">\(\widehat{\text{hectares}}=\exp(-69.8)\exp(1.39\text{ Temp})\)</span>. Figure <a href="7-6-section7-6.html#fig:Figure7-18">2.134</a> provides the <span class="math inline">\(\log(y)\)</span> scale version of the model and the model transformed to the original scale of measurement. On the log-hectares scale, the interpretation of the slope is: For a 1<span class="math inline">\(^\circ F\)</span> increase in summer temperature, we expect a 1.39 log-hectares/1<span class="math inline">\(^\circ F\)</span> change, on average, in the log-area burned. On the original scale: A 1<span class="math inline">\(^\circ F\)</span> increase in temperature is related to multiplicative change in the median number of hectares burned of <span class="math inline">\(\exp(1.39)=4.01\)</span>. That seems like a big rate of growth but the curve does grow rapidly as shown in panel (b), especially for values over 58<span class="math inline">\(^\circ F\)</span> where the area burned is starting to be really large. You can think of the multiplicative change here in the following way: the median number of hectares burned is 4 times higher at 58<span class="math inline">\(^\circ F\)</span> than at 57<span class="math inline">\(^\circ F\)</span> and the median area burned is 4 times larger at 59<span class="math inline">\(^\circ F\)</span> than at 58<span class="math inline">\(^\circ F\)</span>… This can also be interpreted on a % change scale: A 1<span class="math inline">\(^\circ F\)</span> increase in temperature is related to a <span class="math inline">\((4.01-1)*100 = 301\%\)</span> increase in the median number of hectares burned.</p>

<p><strong>Scenario 2: y vs log(x) model:</strong></p>
<p>When only the explanatory variable is log-transformed, it has a different sort of impact on the regression model interpretation.  Effectively we move the percentage change onto the x-scale and modify the first part of our slope interpretation when we consider the results on the original scale for <span class="math inline">\(x\)</span>. Once again, we will consider the mathematics underlying the changes in the model and then work on applying it to real situations. When the explanatory variable is logged, the estimated regression model is <span class="math inline">\(\color{red}{\boldsymbol{y=b_0+b_1\log(x)}}\)</span>. This models the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> in terms of multiplicative changes in <span class="math inline">\(x\)</span> having an effect on the average <span class="math inline">\(y\)</span>. To develop an interpretation on the x-scale (not <span class="math inline">\(\log(x)\)</span>), consider the impact of doubling <span class="math inline">\(x\)</span>. This change will take us from the point (<span class="math inline">\(x,\color{red}{\boldsymbol{y=b_0+b_1\log(x)}}\)</span>) to the point <span class="math inline">\((2x,\boldsymbol{y^*=b_0+b_1\log(2x)})\)</span>. Now the impact of doubling <span class="math inline">\(x\)</span> can be simplified using the rules for logs to be:</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{y^*=b_0+b_1\log(2x)}\)</span>,</p></li>
<li><p><span class="math inline">\(\boldsymbol{y^*}={\color{red}{\underline{\boldsymbol{b_0+b_1\log(x)}}}} + b_1\log(2)\)</span>.   <em>Based on the rules for logs:</em> <span class="math inline">\(log(2x)=log(x)+log(2)\)</span>.</p></li>
<li><p><span class="math inline">\(y^* = {\color{red}{\boldsymbol{y}}}+b_1\log(2)\)</span></p></li>
<li><p>So if we double <span class="math inline">\(x\)</span>, we change the <strong>mean</strong> of <span class="math inline">\(y\)</span> by <span class="math inline">\(b_1\log(2)\)</span>.</p></li>
</ul>
<p>As before, there are couple of ways to interpret these sorts of results, </p>
<ol style="list-style-type: decimal">
<li><p><strong><em>log-scale interpretation of log(x) only model</em></strong>: for a 1 log-unit increase in <span class="math inline">\(x\)</span>, we expect a <span class="math inline">\(b_1\)</span> unit change in the mean of <span class="math inline">\(y\)</span> or</p></li>
<li><p><strong><em>original scale interpretation of log(x) only model</em></strong>: for a doubling of <span class="math inline">\(x\)</span>, we expect a <span class="math inline">\(b_1\log(2)\)</span> change in the mean of <span class="math inline">\(y\)</span>. Note that both interpretations are for the mean of the <span class="math inline">\(y\text{&#39;s}\)</span> since we haven’t changed the <span class="math inline">\(y\sim\)</span> part of the model.</p></li>
</ol>

<div class="figure"><span id="fig:Figure7-19"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-19-1.png" alt="Plot of the observations and estimated SLR model (mortality~ log(GDP)) (top) and implied model (bottom) for the infant mortality data." width="960" />
<p class="caption">
Figure 2.135: Plot of the observations and estimated SLR model (mortality~ log(GDP)) (top) and implied model (bottom) for the infant mortality data.
</p>
</div>
<p>While it is not a perfect model (no model is), let’s consider the model for <em>infant mortality</em> <span class="math inline">\(\sim\)</span> <em>log(GDP)</em> in order to practice the interpretation using this type of model. This model was estimated to be <span class="math inline">\(\widehat{\text{infantmortality}}=168.648-16.6\cdot\log(\text{GDP})\)</span>. The first (simplest) interpretation of the slope coefficient is: For a 1 log-dollar increase in GDP per capita, we expect infant mortality to change, on average, by -16.6 deaths/1000 births per log-dollar. The second interpretation is on the original GDP scale: For a doubling of GDP, we expect infant mortality to change, on average, by <span class="math inline">\(-16.6\log(2) = -11.51\)</span> deaths/1000 live births. Or, the mean infant mortality is reduced by 11.51 deaths per 1000 live births for each doubling of GDP. Both versions of the model are displayed in Figure <a href="7-6-section7-6.html#fig:Figure7-19">2.135</a> – one on the scale the SLR model was fit (panel a) and the other on the original x-scale (panel b) that matches these last interpretations.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ID1 &lt;-<span class="st"> </span><span class="kw">lm</span>(infantMortality<span class="op">~</span><span class="kw">log</span>(ppgdp), <span class="dt">data=</span>UN)
<span class="kw">summary</span>(ID1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = infantMortality ~ log(ppgdp), data = UN)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.239 -11.609  -2.829   8.122  82.183 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 155.7698     7.2431   21.51   &lt;2e-16
## log(ppgdp)  -14.8617     0.8468  -17.55   &lt;2e-16
## 
## Residual standard error: 18.14 on 191 degrees of freedom
##   (20 observations deleted due to missingness)
## Multiple R-squared:  0.6172, Adjusted R-squared:  0.6152 
## F-statistic:   308 on 1 and 191 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">-</span><span class="fl">16.6</span><span class="op">*</span><span class="kw">log</span>(<span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] -11.50624</code></pre>
<p>It appears that our model does not fit too well and that there might be some non-constant variance so we should check the diagnostic plots (available in Figure <a href="7-6-section7-6.html#fig:Figure7-20">2.136</a>) before we trust any of those previous interpretations.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(ID1)</code></pre></div>
<div class="figure"><span id="fig:Figure7-20"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-20-1.png" alt="Diagnostics plots of the infant mortality model with log(GDP)." width="960" />
<p class="caption">
Figure 2.136: Diagnostics plots of the infant mortality model with log(GDP).
</p>
</div>
<p>There appear to be issues with outliers and a long right tail violating the normality assumption. There is curvature and non-constant variance in the results as well. There are no influential points, but we are far from happy with this model and will be revisiting this example with the responses also transformed. Remember that the log-transformation of the response can potentially fix non-constant variance, normality, and curvature issues.</p>
<p><strong>Scenario 3: log(y)~log(x) model</strong></p>
<p>A final model combines log-transformations of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, combining the interpretations used in the previous two situations. This model is called the <strong><em>log-log model</em></strong> and in some fields is also called the <strong><em>power law model</em></strong>.   The power-law model is usually written as <span class="math inline">\(y = \beta_0x^{\beta_1}+\varepsilon\)</span>, where <span class="math inline">\(y\)</span> is thought to be proportional to <span class="math inline">\(x\)</span> raised to an estimated power of <span class="math inline">\(\beta_1\)</span> (linear if <span class="math inline">\(\beta_1=1\)</span> and quadratic if <span class="math inline">\(\beta_1=2\)</span>). It is one of the models that has been used in Geomorphology to model the shape of glaciated valley elevation profiles (that classic U-shape that comes with glacier-eroded mountain valleys). If you ignore the error term, it is possible to estimate the power-law model using our SLR approach. Consider the log-transformation of both sides of this equation starting with the power-law version:</p>
<ul>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0x^{\beta_1})\)</span>,</p></li>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0) + \log(x^{\beta_1}).\)</span>   <em>Based on the rules for logs:</em> <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>.</p></li>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0) + \beta_1\log(x).\)</span>   <em>Based on the rules for logs:</em> <span class="math inline">\(\log(x^b) = b\log(x)\)</span>.</p></li>
</ul>

<div class="figure"><span id="fig:Figure7-21"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-21-1.png" alt="Plot of the observations and estimated SLR model log(mortality) \(\sim\) log(GDP) (left) and implied model (right) for the infant mortality data." width="576" />
<p class="caption">
Figure 2.137: Plot of the observations and estimated SLR model log(mortality) <span class="math inline">\(\sim\)</span> log(GDP) (left) and implied model (right) for the infant mortality data.
</p>
</div>
<p>So other than <span class="math inline">\(\log(\beta_0)\)</span> in the model, this looks just like our regular SLR model with <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> both log-transformed. The slope coefficient for <span class="math inline">\(\log(x)\)</span> is the power coefficient in the original power law model and determines whether the relationship between the original <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in <span class="math inline">\(y=\beta_0x^{\beta_1}\)</span> is linear <span class="math inline">\((y=\beta_0x^1)\)</span> or quadratic <span class="math inline">\((y=\beta_0x^2)\)</span> or even quartic <span class="math inline">\((y=\beta_0x^4)\)</span> in some really heavily glacier carved U-shaped valleys. There are some issues with “ignoring the errors” in using SLR to estimate these models <span class="citation">(Greenwood and Humphrey <a href="#ref-Greenwood2002">2002</a>)</span> but it is still a pretty powerful result to be able to estimate the coefficients in <span class="math inline">\((y=\beta_0x^{\beta_1})\)</span> using SLR.</p>
<p>We don’t typically use the previous ideas to interpret the typical log-log regression model, instead we combine our two previous interpretation techniques to generate our interpretation. We need to work out the mathematics of doubling <span class="math inline">\(x\)</span> and the changes in <span class="math inline">\(y\)</span> starting with the <span class="math inline">\(\mathit{\boldsymbol{\log(y)\sim \log(x)}}\)</span> <strong><em>model</em></strong> that we would get out of fitting the SLR with both variables log-transformed:</p>
<ul>
<li><p><span class="math inline">\(\log(y) = b_0 + b_1\log(x)\)</span>,</p></li>
<li><p><span class="math inline">\(y = \exp(b_0 + b_1\log(x))\)</span>.   <em>Exponentiate both sides</em>.</p></li>
<li><p><span class="math inline">\(y = \exp(b_0)\exp(b_1\log(x))=\exp(b_0)x^{b_1}\)</span>.   <em>Rules for exponents and logs, simplifying.</em></p></li>
</ul>
<p>Now we can consider the impacts of doubling <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, going from <span class="math inline">\((x,{\color{red}{\boldsymbol{y=\exp(b_0)x^{b_1}}}})\)</span> to <span class="math inline">\((2x,y^*)\)</span> with</p>
<ul>
<li><p><span class="math inline">\(y^* = \exp(b_0)(2x)^{b_1}\)</span>,</p></li>
<li><p><span class="math inline">\(y^* = \exp(b_0)2^{b_1}x^{b_1} = 2^{b_1}{\color{red}{\boldsymbol{\exp(b_0)x^{b_1}}}}=2^{b_1}{\color{red}{\boldsymbol{y}}}\)</span></p></li>
</ul>
<p>So doubling <span class="math inline">\(x\)</span> leads to a multiplicative change in the median of <span class="math inline">\(y\)</span> of <span class="math inline">\(2^{b_1}\)</span>.</p>
<p>Let’s apply this idea to the GDP and infant mortality data where a <span class="math inline">\(\log(x) \sim \log(y)\)</span> transformation actually made the resulting relationship look like it might be close to meeting the SLR assumptions. The regression line in Figure <a href="7-6-section7-6.html#fig:Figure7-21">2.137</a> actually looks pretty good on both the estimated log-log scale (panel a) and on the original scale (panel b) as it captures the severe nonlinearity in the relationship between the two variables.</p>

<div class="figure"><span id="fig:Figure7-22"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-22-1.png" alt="Diagnostic plots for the log-log infant mortality model." width="960" />
<p class="caption">
Figure 2.138: Diagnostic plots for the log-log infant mortality model.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ID2 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(infantMortality)<span class="op">~</span><span class="kw">log</span>(ppgdp), <span class="dt">data=</span>UN)
<span class="kw">summary</span>(ID2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(infantMortality) ~ log(ppgdp), data = UN)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.16789 -0.36738 -0.02351  0.24544  2.43503 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  8.10377    0.21087   38.43   &lt;2e-16
## log(ppgdp)  -0.61680    0.02465  -25.02   &lt;2e-16
## 
## Residual standard error: 0.5281 on 191 degrees of freedom
##   (20 observations deleted due to missingness)
## Multiple R-squared:  0.7662, Adjusted R-squared:  0.765 
## F-statistic: 625.9 on 1 and 191 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The estimated regression model is <span class="math inline">\(\log(\widehat{\text{infantmortality}})=7.0452-0.493\cdot\log(\text{GDP})\)</span>. The slope coefficient can be interpreted two ways. </p>
<ol style="list-style-type: decimal">
<li><p><strong><em>On the log-log scale:</em></strong> For a 1 log-dollar increase in <em>GDP</em>, we expect, on average, a change of <span class="math inline">\(-0.493\)</span> log(deaths/1000 live births) in <em>infant mortality</em>.</p></li>
<li><p><strong><em>On the original scale:</em></strong> For a doubling of <em>GDP</em>, we expect a <span class="math inline">\(2^{b_1} = 2^{-0.493} = 0.7105\)</span> multiplicative change in the median <em>infant mortality</em>. That is a 28.95% decrease in the median <em>infant mortality</em> for each doubling of <em>GDP</em>.</p></li>
</ol>
<p>The diagnostics of the log-log SLR model (Figure <a href="7-6-section7-6.html#fig:Figure7-22">2.138</a>) show minimal evidence of violations of assumptions although the tails of the residuals are a little heavy (more spread out than a normal distribution) and there might still be a little pattern remaining in the residuals vs fitted values.  There are no influential points to be concerned about in this situation.</p>
<p>While we will not revisit this at all except in the case-studies in Chapter <a href="9-chapter9.html#chapter9"><strong>??</strong></a>, log-transformations can be applied to the response variable in ONE and TWO-WAY ANOVA models when we are concerned about non-constant variance and non-normality issues<a href="#fn89" class="footnoteRef" id="fnref89"><sup>89</sup></a>. The remaining methods in this chapter return to SLR and assuming that the model assumptions have been met. In fact, the methods in Section <a href="7-7-section7-7.html#section7-7">7.7</a> are some of the most sensitive results to violations of the assumptions that we will explore.</p>


</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Greenwood2002">
<p>Greenwood, Mark C., and N.F. Humphrey. 2002. “Glaciated Valley Profiles: An Application of Nonlinear Regression.” <em>Computing Science and Statistics</em> 34: 452–60.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="88">
<li id="fn88"><p>Note <code>exp(x)</code> is the same as <span class="math inline">\(e^{(x)}\)</span> but easier to read in-line and <code>exp()</code> is the R function name to execute this calculation.<a href="7-6-section7-6.html#fnref88">↩</a></p></li>
<li id="fn89"><p>This transformation could not be applied directly to the education growth score data in Chapter <a href="5-chapter5.html#chapter5"><strong>??</strong></a> because there were negative “growth” scores.<a href="7-6-section7-6.html#fnref89">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="7-5-section7-5.html"><button class="btn btn-default">Previous</button></a>
<a href="7-7-section7-7.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
