<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Second Semester Statistics Course with R</title>
  <meta name="description" content="A Second Semester Statistics Course with R">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="A Second Semester Statistics Course with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Second Semester Statistics Course with R" />
  
  
  

<meta name="author" content="Mark C Greenwood">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapter1.html">
<link rel="next" href="chapter3.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Beanplots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Chapter summary</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and table plots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Second Semester Statistics Course with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter2" class="section level1">
<h1><span class="header-section-number">Chapter 2</span> (R)e-Introduction to statistics</h1>
<p>The previous material served to get us started in R and to get a quick review of same basic descriptive statistics. Now we will begin to engage some new material and exploit the power of R to do some statistical inference. Because inference is one of the hardest topics to master in statistics, we will also review some basic terminology that is required to move forward in learning more sophisticated statistical methods. To keep this “review” as short as possible, we will not consider every situation you learned in introductory statistics and instead focus exclusively on the situation where we have a quantitative response variable measured on two groups, adding a new graphic called a “bean plot” to help us see the differences in the observations in the groups.</p>
<div id="section2-1" class="section level2">
<h2><span class="header-section-number">2.1</span> Histograms, boxplots, and density curves</h2>
<p>Part of learning statistics is learning to correctly use the terminology, some of which is used colloquially differently than it is used in formal statistical settings. The most commonly “misused” statistical term is <strong><em>data</em></strong>. In statistical parlance, we want to note the plurality of data. Specifically, <strong><em>datum</em></strong> is a single measurement, possibly on multiple random variables, and so it is appropriate to say that “<strong>a datum is…</strong>”. Once we move to discussing data, we are now referring to more than one observation, again on one, or possibly more than one, random variable, and so we need to use “<strong>data are…</strong>” when talking about our observations. We want to distinguish our use of the term “data” from its more colloquial<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a> usage that often involves treating it as singular. In a statistical setting “data” refers to measurements of our cases or units. When we summarize the results of a study (say providing the mean and SD), that information is not “data”. We used our data to generate that information. Sometimes we also use the term “data set” to refer to all our observations and this is a singular term to refer to the group of observations and this makes it really easy to make mistakes on the usage of “data”<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a>.</p>
<p>It is also really important to note that <strong><em>variables</em></strong> have to vary – if you measure the sex of your subjects but are only measuring females, then you do not have a “variable”. You may not know if you have real variability in a “variable” until you explore the results you obtained.</p>
<p>The last, but probably most important, aspect of data is the context of the measurement. The “who, what, when, and where” of the collection of the observations is critical to the sort of conclusions we can make based on the results. The information on the study design provides information required to assess the scope of inference of the study. Generally, remember to think about the research questions the researchers were trying to answer and whether their study actually would answer those questions. There are no formulas to help us sort some of these things out, just critical thinking about the context of the measurements.</p>
<p>To make this concrete, consider the data collected from a study <span class="citation">(Plaster <a href="#ref-Plaster1989">1989</a>)</span> to investigate whether perceived physical attractiveness had an impact on the sentences or perceived seriousness of a crime that male jurors might give to female defendants. The researchers showed the participants in the study (men who volunteered from a prison) pictures of one of three young women. Each picture had previously been decided to be either beautiful, average, or unattractive by the researchers. Each “juror” was randomly assigned to one of three levels of this factor (which is a categorical <em>predictor</em> or <em>explanatory</em> variable) and then each participant rated their picture on a variety of traits such as how warm or sincere the woman appeared. Finally, they were told the women had committed a crime (also randomly assigned to either be told she committed a burglary or a swindle) and were asked to rate the seriousness of the crime and provide a suggested length of sentence. We will bypass some aspects of their research and just focus on differences in the sentence suggested among the three pictures. To get a sense of these data, let’s consider the first and last parts of the data set:</p>

<table style="width:100%;">
<caption><span id="tab:Table2-1">Table 2.1: </span> First 5 and last 7 rows of the Mock Jury data set.</caption>
<colgroup>
<col width="13%" />
<col width="16%" />
<col width="14%" />
<col width="10%" />
<col width="13%" />
<col width="18%" />
<col width="12%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">Subject</th>
<th align="center">Attr</th>
<th align="center">Crime</th>
<th align="center">Years</th>
<th align="center">Serious</th>
<th align="center">Independent</th>
<th align="center">Sincere</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">10</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">2</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">3</td>
<td align="center">8</td>
<td align="center">9</td>
<td align="center">3</td>
</tr>
<tr class="odd">
<td align="center">3</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">5</td>
<td align="center">5</td>
<td align="center">6</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="center">4</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">1</td>
<td align="center">3</td>
<td align="center">9</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center">5</td>
<td align="center">Beautiful</td>
<td align="center">Burglary</td>
<td align="center">7</td>
<td align="center">9</td>
<td align="center">5</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
</tr>
<tr class="odd">
<td align="center">108</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">3</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">4</td>
</tr>
<tr class="even">
<td align="center">109</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">3</td>
<td align="center">2</td>
<td align="center">9</td>
<td align="center">9</td>
</tr>
<tr class="odd">
<td align="center">110</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">2</td>
<td align="center">1</td>
<td align="center">8</td>
<td align="center">8</td>
</tr>
<tr class="even">
<td align="center">111</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">7</td>
<td align="center">4</td>
<td align="center">9</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">112</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">6</td>
<td align="center">3</td>
<td align="center">5</td>
<td align="center">2</td>
</tr>
<tr class="even">
<td align="center">113</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">12</td>
<td align="center">9</td>
<td align="center">9</td>
<td align="center">1</td>
</tr>
<tr class="odd">
<td align="center">114</td>
<td align="center">Average</td>
<td align="center">Swindle</td>
<td align="center">8</td>
<td align="center">8</td>
<td align="center">1</td>
<td align="center">5</td>
</tr>
</tbody>
</table>
<p>When working with data, we should always start with summarizing the sample size. We will use <strong><em>n</em></strong> for the number of subjects in the sample and denote the population size (if available) with <strong><em>N</em></strong>. Here, the sample size is <strong><em>n=114</em></strong>. In this situation, we do not have a random sample from a population (these were volunteers from the population of prisoners at the particular prison) so we cannot make inferences from our sample to a larger group. But we can assess whether there is a <strong><em>causal effect</em></strong><a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a>: if sufficient evidence is found to conclude that there is some difference in the responses across the treated groups, we can attribute those differences to the treatments applied, since the groups should be same otherwise due to the pictures being randomly assigned to the “jurors”. The story of the data set – that it was collected on prisoners – becomes pretty important in thinking about the ramifications of any results. Are male prisoners different from the population of college males or all residents of a state such as Montana? If so, then we should not assume that the detected differences, if detected, would also exist in some other group of male subjects. The lack of a random sample makes it impossible to assume that this set of prisoners might be like other prisoners. So there are definite limitations to the inferences in the following results. But it is still interesting to see if the pictures caused a difference in the suggested mean sentences, even though the inferences are limited to this group of prisoners. If this had been an observational study (suppose that the prisoners could select one of the three pictures), then we would have to avoid any of the “causal” language that we can consider here because the pictures were not randomly assigned to the subjects. Without random assignment, the explanatory variable of picture choice could be <strong><em>confounded</em></strong> with another characteristic of prisoners that was related to which picture they selected and the rating they provided. Confounding is not the only reason to avoid causal statements with non-random assignment but the inability to separate the effect of other variables (measured or unmeasured) from the differences we are observing means that our inferences in these situations need to be carefully stated to avoid implying causal effects.</p>
<p>Instead of loading this data set into R using the “Import Dataset” functionality, we can load an R package that contains the data, making for easy access to this data set. The package called <code>heplots</code> <span class="citation">(Fox and Friendly <a href="#ref-R-heplots">2017</a>)</span> contains a data set called <code>MockJury</code> that contains the results of the study. We also rely on the R package called <code>mosaic</code> <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaic">2017</a>)</span> that was introduced previously. First (but only once), you need to install both packages, which can be done either using the Packages tab in the lower right panel of RStudio or using the <code>install.packages</code> function with quotes around the package name:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="kw">install.packages</span>(<span class="st">&quot;heplots&quot;</span>)</code></pre></div>
<p>After making sure that both packages are installed, we use the <code>require</code> function around the package name (no quotes now!) to load the package, something that you need to do any time you want to use features of a package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(heplots)
<span class="kw">require</span>(mosaic)</code></pre></div>
<p>When you are loading a package, R might mention a need to install other packages. If the output says that it needs a package that is unavailable, then follow the same process noted above to install that package and then repeat trying to load the package you wanted. These are called package “dependencies” and are due to one package developer relying on functions that already exist in another package.</p>
<p>To load the data set that is available in an active package, we use the <code>data</code> function.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(MockJury)</code></pre></div>
<p>Some data sets, like this one, will load in a different data set framework called a “data frame” (its class in R is “data.frame”). You can leave it in this format but for consistency of output, we will change it to a <em>tibble</em> using the <code>as.tibble</code> function in the code that follows. Now there will be a tibble called <code>MockJury</code> available for us to analyze and some information about it in the <strong>Environment</strong> tab. Again, we can find out more about the data set in a couple of ways. First, we can use the <code>View</code> function to provide a spreadsheet type of display in the upper left panel. Second, we can use the <code>head</code> and <code>tail</code> functions to print out the beginning and end of the data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(tibble)
MockJury&lt;-<span class="kw">as.tibble</span>(MockJury)
<span class="kw">View</span>(MockJury)
<span class="kw">head</span>(MockJury)</code></pre></div>
<pre><code>##        Attr    Crime Years Serious exciting calm independent sincere warm
## 1 Beautiful Burglary    10       8        6    9           9       8    5
## 2 Beautiful Burglary     3       8        9    5           9       3    5
## 3 Beautiful Burglary     5       5        3    4           6       3    6
## 4 Beautiful Burglary     1       3        3    6           9       8    8
## 5 Beautiful Burglary     7       9        1    1           5       1    8
## 6 Beautiful Burglary     7       9        1    5           7       5    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 1       9        9    9           6      9             9     5     9
## 2       9        9    4           9      5             5     5     7
## 3       7        4    2           4      5             4     5     5
## 4       9        9    9           9      9             9     9     9
## 5       8        9    4           7      9             9     8     7
## 6       8        9    5           8      9             9     9     9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(MockJury)</code></pre></div>
<pre><code>##        Attr   Crime Years Serious exciting calm independent sincere warm
## 109 Average Swindle     3       2        7    6           9       9    6
## 110 Average Swindle     2       1        8    8           8       8    8
## 111 Average Swindle     7       4        1    6           9       1    1
## 112 Average Swindle     6       3        5    3           5       2    4
## 113 Average Swindle    12       9        1    9           9       1    1
## 114 Average Swindle     8       8        1    9           1       5    1
##     phyattr sociable kind intelligent strong sophisticated happy ownPA
## 109       4        7    6           8      6             5     7     2
## 110       8        9    9           9      9             9     9     6
## 111       1        9    4           1      1             1     1     9
## 112       1        4    9           3      3             9     5     3
## 113       1        9    1           9      9             1     9     1
## 114       1        9    1           1      9             5     1     1</code></pre>
<p>When data sets are loaded from packages, there is often extra documentation available about the data set which can be accessed using the <code>help</code> function. In this case, it will bring up a screen with information about the study and each variable that was measured.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">help</span>(MockJury)</code></pre></div>
<p>The <code>help</code> function is also useful with functions in R to help you understand options and, at the bottom of the help, see examples of using the function.</p>
<p>With many variables in a data set, it is often useful to get some quick information about all of them; the <code>summary</code> function provides useful information whether the variables are categorical or quantitative and notes if any values were missing.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(MockJury)</code></pre></div>
<pre><code>##            Attr         Crime        Years           Serious     
##  Beautiful   :39   Burglary:59   Min.   : 1.000   Min.   :1.000  
##  Average     :38   Swindle :55   1st Qu.: 2.000   1st Qu.:3.000  
##  Unattractive:37                 Median : 3.000   Median :5.000  
##                                  Mean   : 4.693   Mean   :5.018  
##                                  3rd Qu.: 7.000   3rd Qu.:6.750  
##                                  Max.   :15.000   Max.   :9.000  
##     exciting          calm        independent       sincere     
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:3.000   1st Qu.:4.250   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.000   Median :6.500   Median :6.500   Median :5.000  
##  Mean   :4.658   Mean   :5.982   Mean   :6.132   Mean   :4.789  
##  3rd Qu.:6.000   3rd Qu.:8.000   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##       warm         phyattr        sociable          kind      
##  Min.   :1.00   Min.   :1.00   Min.   :1.000   Min.   :1.000  
##  1st Qu.:2.00   1st Qu.:2.00   1st Qu.:5.000   1st Qu.:3.000  
##  Median :5.00   Median :5.00   Median :7.000   Median :5.000  
##  Mean   :4.57   Mean   :4.93   Mean   :6.132   Mean   :4.728  
##  3rd Qu.:7.00   3rd Qu.:8.00   3rd Qu.:8.000   3rd Qu.:7.000  
##  Max.   :9.00   Max.   :9.00   Max.   :9.000   Max.   :9.000  
##   intelligent        strong      sophisticated       happy      
##  Min.   :1.000   Min.   :1.000   Min.   :1.000   Min.   :1.000  
##  1st Qu.:4.000   1st Qu.:4.000   1st Qu.:3.250   1st Qu.:3.000  
##  Median :7.000   Median :6.000   Median :5.000   Median :5.000  
##  Mean   :6.096   Mean   :5.649   Mean   :5.061   Mean   :5.061  
##  3rd Qu.:8.750   3rd Qu.:7.000   3rd Qu.:7.000   3rd Qu.:7.000  
##  Max.   :9.000   Max.   :9.000   Max.   :9.000   Max.   :9.000  
##      ownPA      
##  Min.   :1.000  
##  1st Qu.:5.000  
##  Median :6.000  
##  Mean   :6.377  
##  3rd Qu.:9.000  
##  Max.   :9.000</code></pre>
<p>If we take a few moments to explore the output we can discover some useful aspects of the data set. The output is organized by variable, providing summary information based on the type of variable, either counts by category for categorical variables (<code>Attr</code> and <code>Crime</code>) or the 5-number summary plus the mean for quantitative variables. If present, you would also get a count of missing values that are called “NAs” in R. For the first variable, called <code>Attr</code> in the data.frame and that we might more explicitly name <em>Attractiveness</em>, we find counts of the number of subjects shown each picture: <span class="math inline">\(37\)</span> out of <span class="math inline">\(114\)</span> viewed the “Unattractive” picture, <span class="math inline">\(38\)</span> viewed “Average”, and <span class="math inline">\(39\)</span> viewed “Beautiful”. We can also see that suggested prison sentences (variable <code>Years</code>) ranged from 1 year to 15 years with a median of 3 years. It seems that all the other variables except for <em>Crime</em> (type of crime that they were told the pictured woman committed) contained responses between 1 and 9 based on rating scales from 1 = low to 9 = high.</p>
<p>To accompany the numerical summaries, histograms and boxplots can provide some initial information on the shape of the distribution of the responses for the suggested sentences in <em>Years</em>. Figure <a href="chapter2.html#fig:Figure2-1">2.1</a> contains the histogram and boxplot of <em>Years</em>, ignoring any information on which picture the “jurors” were shown. The calls to the two plotting functions are enhanced slightly to add better labels.</p>

<div class="figure"><span id="fig:Figure2-1"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-1-1.png" alt="Histogram and boxplot of suggested sentences in years." width="576" />
<p class="caption">
Figure 2.1: Histogram and boxplot of suggested sentences in years.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">labels=</span>T, <span class="dt">main=</span><span class="st">&quot;Histogram of Years&quot;</span>)
<span class="kw">boxplot</span>(MockJury<span class="op">$</span>Years, <span class="dt">ylab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Boxplot of Years&quot;</span>)</code></pre></div>
<p>The distribution appears to have a strong right skew with three observations at 15 years flagged as potential outliers. You can only tell that there are three observations and that they are at 15 by looking at both plots – the bar around 15 years in the histogram has a count of three and the boxplot only shows a single point at 15 which is actually three tied points at exactly 15 years plotted on top of each other (we call this “overplotting”). These three observations really seem to be the upper edge of the overall pattern of a strongly right skewed distribution, so even though they are flagged in the boxplot, they really are not outside the pattern we observed so are not outliers. In real data sets, outliers are commonly encountered and the first step is to verify that they were not errors in recording (if so, fixing or removing them is easily justified). If they cannot be easily dismissed or fixed, the next step is to study their impact on the statistical analyses performed, potentially considering reporting results with and without the influential observation(s) in the results. If the analysis is unaffected by the “unusual” observations, then it matters little whether they are dropped or not. If they do affect the results, then reporting both versions of results allows the reader to judge the impacts for themselves. It is important to remember that sometimes the outliers are the most interesting part of the data set.</p>
<p>Often when statisticians think of distributions of data, we think of the smooth underlying shape that led to the data set that is being displayed in the histogram. Instead of binning up observations and making bars in the histogram, we can estimate what is called a <strong><em>density curve</em></strong> as a smooth curve that represents the observed distribution of the responses. Density curves can sometimes help us see features of the data sets more clearly.</p>
<p>To understand the density curve, it is useful to initially see the histogram and density curve together. The height of the density curve is scaled so that the total area under the curve<a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a> is 1. To make a comparable histogram, the y-axis needs to be scaled so that the histogram is also on the “density” scale which makes the bar heights adjust so that the proportion of the total data set in each bar is represented by the area in each bar (remember that area is height times width). So the height depends on the width of the bars and the total area across all the bars has to be 1. In the <code>hist</code> function, the <code>freq=F</code> option does this required re-scaling to get density-scaled histogram bars. The density curve is added to the histogram using the R code of <code>lines(density())</code>, producing the result in Figure <a href="chapter2.html#fig:Figure2-2">2.2</a> with added modifications of options for <code>lwd</code> (line width) and <code>col</code> (color) to make the plot more interesting. You can see how the density curve somewhat matches the histogram bars but deals with the bumps up and down and edges a little differently. We can pick out the strong right skew using either display and will rarely make both together.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Histogram of Years&quot;</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(MockJury<span class="op">$</span>Years), <span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure2-2"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-2-1.png" alt="Histogram and density curve of Years data." width="480" />
<p class="caption">
Figure 2.2: Histogram and density curve of Years data.
</p>
</div>
<p>Histograms can be sensitive to the choice of the number of bars and even the cut-offs used to define the bins for a given number of bars. Small changes in the definition of cut-offs for the bins can have noticeable impacts on the shapes observed but this does not impact density curves. We are not going to tinker with the default choices for bars in histogram as they are reasonably selected, but we can add information on the original observations being included in each bar to better understand the choices that <code>hist</code> is making. In the previous display, we can add what is called a <strong><em>rug</em></strong> to the plot, where a tick mark is made on the x-axis for each observation. Because the responses were provided as whole years (1, 2, 3, …, 15), we need to use a graphical technique called <strong><em>jittering</em></strong> to add a little noise<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a> to each observation so all the observations at each year value do not plot as a single line. In Figure <a href="chapter2.html#fig:Figure2-3">2.3</a>, the added tick marks on the x-axis show the approximate locations of the original observations. We can see how there are 3 observations at 15 (all were 15 and the noise added makes it possible to see them all). The limitations of the histogram arise around the 10 year sentence area where there are many responses at 10 years and just one at both 9 and 11 years, but the histogram bars sort of miss this aspect of the data set. The density curve did show a small bump at 10 years. Density curves are, however, not perfect and this one shows area for sentences less than 0 years which is not possible here.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(MockJury<span class="op">$</span>Years, <span class="dt">freq=</span>F, <span class="dt">xlab=</span><span class="st">&quot;Years&quot;</span>,
     <span class="dt">main=</span><span class="st">&quot;Histogram of Years with density curve and rug&quot;</span>)
<span class="kw">lines</span>(<span class="kw">density</span>(MockJury<span class="op">$</span>Years), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">rug</span>(<span class="kw">jitter</span>(MockJury<span class="op">$</span>Years), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure2-3"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-3-1.png" alt="Histogram with density curve and rug plot of the jittered responses." width="480" />
<p class="caption">
Figure 2.3: Histogram with density curve and rug plot of the jittered responses.
</p>
</div>
<p>The graphical tools we’ve just discussed are going to help us move to comparing the distribution of responses across more than one group. We will have two displays that will help us make these comparisons. The simplest is the <strong><em>side-by-side boxplot</em></strong>, where a boxplot is displayed for each group of interest using the same y-axis scaling. In R, we can use its <strong><em>formula</em></strong> notation to see if the response (<code>Years</code>) differs based on the group (<code>Attr</code>) by using something like <code>Y~X</code> or, here, <code>Years~Attr</code>. We also need to tell R where to find the variables – use the last option in the command, <code>data=DATASETNAME</code> , to inform R of the tibble to look in to find the variables. In this example, <code>data=MockJury</code>. We will use the formula and <code>data=...</code> options in almost every function we use from here forward. Figure <a href="chapter2.html#fig:Figure2-4">2.4</a> contains the side-by-side boxplots showing right skew for all the groups, slightly higher median and more variability for the <em>Unattractive</em> group along with some potential outliers indicated in two of the three groups.</p>

<div class="figure"><span id="fig:Figure2-4"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-4-1.png" alt="Side-by-side boxplot of Years based on picture groups." width="480" />
<p class="caption">
Figure 2.4: Side-by-side boxplot of Years based on picture groups.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre></div>
<p>The “~” (which is read as the <em>tilde</em> symbol, which you can find in the upper left corner of your keyboard) notation will be used in two ways this semester. The formula use in R employed previously declares that the response variable here is <em>Years</em> and the explanatory variable is <em>Attr</em>. The other use for “~” is as shorthand for “is distributed as” and is used in the context of <span class="math inline">\(Y\sim N(0,1)\)</span>, which translates (in statistics) to defining the random variable <em>Y</em> as following a Normal distribution<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a> with mean 0 and standard deviation of 1. In the current situation, we could ask whether the <code>Years</code> variable seems like it may follow a normal distribution, in other words, is <span class="math inline">\(\text{Years}\sim N(\mu,\sigma^2)\)</span>? Since the responses are right skewed with some groups having outliers, it is not reasonable to assume that the <em>Years</em> variable for any of the three groups may follow a Normal distribution (more later on the issues this creates!). Remember that <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are parameters where <span class="math inline">\(\mu\)</span> (“mu”) is our standard symbol for the <strong><em>population mean</em></strong> and that <span class="math inline">\(\sigma\)</span> (“sigma”) is the symbol of the <strong><em>population standard deviation</em></strong>.</p>
</div>
<div id="section2-2" class="section level2">
<h2><span class="header-section-number">2.2</span> Beanplots</h2>
<p>The other graphical display for comparing multiple groups we will use is a display called a <strong><em>beanplot</em></strong> <span class="citation">(Kampstra <a href="#ref-Kampstra2008">2008</a>)</span>. Figure <a href="chapter2.html#fig:Figure2-5">2.5</a> shows an example of a beanplot that provides a side-by-side display that contains the density curves, the original observations that generated the density curve in a (jittered) rug-plot, the mean of each group, and the overall mean of the entire data set. For each group, the density curves are mirrored to aid in visual assessment of the shape of the distribution, which makes a “bean” of sorts. This mirroring also creates a shape that resembles a violin with skewed distributions so this display has also been called a “violin plot”. The beanplot includes bold horizontal lines at the mean for each group and adds a lighter dashed line for the overall mean to allow comparison of that global mean with the individual group means. All together this plot shows us information on the center (mean), spread, and shape of the distributions of the responses. Our inferences typically focus on the means of the groups and this plot allows us to compare those across the groups while gaining information on the shapes of the distributions of responses in each group.</p>
<p>To use the <code>beanplot</code> function we need to install and then load the <code>beanplot</code> package <span class="citation">(Kampstra <a href="#ref-R-beanplot">2014</a>)</span>. The function works like the boxplot used previously except that options for <code>log</code>, <code>col</code>, and <code>method</code> need to be specified. Use these<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a> options for any beanplots you make: <code>log=&quot;&quot;, col=&quot;bisque&quot;, method=&quot;jitter&quot;</code>.</p>

<div class="figure"><span id="fig:Figure2-5"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-5-1.png" alt="Beanplot of Years by picture group. Long, bold lines correspond to mean of each group, dashed line for overall or global mean." width="480" />
<p class="caption">
Figure 2.5: Beanplot of Years by picture group. Long, bold lines correspond to mean of each group, dashed line for overall or global mean.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(beanplot)
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre></div>
<p>Figure <a href="chapter2.html#fig:Figure2-5">2.5</a> reinforces the strong right skews that were also detected in the boxplots previously. The three large sentences of 15 years can now be clearly identified, with one in the <em>Beautiful</em> group and two in the <em>Unattractive</em> group. The <em>Unattractive</em> group seems to have more high observations than the other groups even though the <em>Beautiful</em> group had the largest number of observations around 10 years. The mean sentence was highest for the <em>Unattractive</em> group and the difference in the means between <em>Beautiful</em> and <em>Average</em> was small.</p>
<p>In this example, it appears that the mean for <em>Unattractive</em> is larger than the other two groups. But is this difference real? We will never know the answer to that question, but we can assess how likely we are to have seen a result as extreme or more extreme than our result, assuming that there is no difference in the means of the groups. And if the observed result is (extremely) unlikely to occur, then we can reject the hypothesis that the groups have the same mean and conclude that there is evidence of a real difference. To start exploring whether there are differences in the means, we need to have numerical values to compare. We can get means and standard deviations by groups easily using the same formula notation with the <code>mean</code> and <code>sd</code> functions if the <code>mosaic</code> package is loaded.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">mean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre></div>
<pre><code>##    Beautiful      Average Unattractive 
##     4.333333     3.973684     5.810811</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sd</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre></div>
<pre><code>##    Beautiful      Average Unattractive 
##     3.405362     2.823519     4.364235</code></pre>
<p>We can also use the <code>favstats</code> function to get those summaries and others by groups.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury)</code></pre></div>
<pre><code>##           Attr min Q1 median   Q3 max     mean       sd  n missing
## 1    Beautiful   1  2      3  6.5  15 4.333333 3.405362 39       0
## 2      Average   1  2      3  5.0  12 3.973684 2.823519 38       0
## 3 Unattractive   1  2      5 10.0  15 5.810811 4.364235 37       0</code></pre>
<p>Based on these results, we can see that there is an estimated difference of almost 2 years in the mean sentence between <em>Average</em> and <em>Unattractive</em> groups. Because there are three groups being compared in this study, we will have to wait until Chapter 3 and the One-Way ANOVA test to fully assess evidence related to some difference among the three groups. For now, we are going to focus on comparing the mean <em>Years</em> between <em>Average</em> and <em>Unattractive</em> groups – which is a <strong><em>2 independent sample mean</em></strong> situation and something you should have seen before. Remember that the “independent” sample part of this refers to observations that are independently observed for the two groups as opposed to the paired sample situation that you may have explored where one observation from the first group is related to an observation in the second group (two measures on the same person (we generically call this “repeated measures”) or the famous “twin” studies with one twin assigned to each group).</p>
<p>Here we are going to use the “simple” two independent group scenario to review some basic statistical concepts and connect two different frameworks for conducting statistical inference: randomization and parametric inference techniques. <strong><em>Parametric</em></strong> statistical methods involve making assumptions about the distribution of the responses and obtaining confidence intervals and/or p-values using a <em>named</em> distribution (like the <span class="math inline">\(z\)</span> or <span class="math inline">\(t\)</span>-distributions). Typically these results are generated using formulas and looking up areas under curves or cutoffs using a table or a computer. <strong><em>Randomization</em></strong>-based statistical methods use a computer to shuffle, sample, or simulate observations in ways that allow you to obtain distributions of possible results to find areas and cutoffs without resorting to using tables and named distributions. Randomization methods are what are called <strong><em>nonparametric</em></strong> methods that often make fewer assumptions (they are <strong><em>not free of assumptions</em></strong>!) and so can handle a larger set of problems more easily than parametric methods. When the assumptions involved in the parametric procedures are met by a data set, the randomization methods often provide very similar results to those provided by the parametric techniques. To be a more sophisticated statistical consumer, it is useful to have some knowledge of both of these techniques for performing statistical inference and the fact that they can provide similar results might deepen your understanding of both approaches.</p>
<p>We will start with comparing the <em>Average</em> and <em>Unattractive</em> groups to compare these two ways of doing inference. We could remove the <em>Beautiful</em> group observations in a spreadsheet program and read that new data set back into R, but it is actually pretty easy to use R to do data management once the data set is loaded. To remove the observations that came from the <em>Beautiful</em> group, we are going to generate a new variable that we will call <code>NotBeautiful</code> that is true when observations came from another group (<em>Average</em> or <em>Unattractive</em>) and false for observations from the <em>Beautiful</em> group. To do this, we will apply the <strong><em>not equal</em></strong> logical function (<code>!=</code> ) to the variable <code>Attr</code>, inquiring whether it was different from the <code>&quot;Beautiful&quot;</code> level. You can see the content of the new variable in the output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury<span class="op">$</span>NotBeautiful &lt;-<span class="st"> </span>MockJury<span class="op">$</span>Attr <span class="op">!=</span><span class="st"> &quot;Beautiful&quot;</span>
MockJury<span class="op">$</span>NotBeautiful</code></pre></div>
<pre><code>##   [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [12] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE
##  [23]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [34]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [45]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [56]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
##  [67]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE FALSE
##  [78] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE
##  [89] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE
## [100]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE
## [111]  TRUE  TRUE  TRUE  TRUE</code></pre>
<p>This new variable is only FALSE for the <em>Beautiful</em> responses as we can see if we compare some of the results from the original and new variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(<span class="kw">tibble</span>(MockJury<span class="op">$</span>Attr, MockJury<span class="op">$</span>NotBeautiful))</code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   `MockJury$Attr` `MockJury$NotBeautiful`
##   &lt;fctr&gt;          &lt;lgl&gt;                  
## 1 Beautiful       F                      
## 2 Beautiful       F                      
## 3 Beautiful       F                      
## 4 Beautiful       F                      
## 5 Beautiful       F                      
## 6 Beautiful       F</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tail</span>(<span class="kw">tibble</span>(MockJury<span class="op">$</span>Attr, MockJury<span class="op">$</span>NotBeautiful))</code></pre></div>
<pre><code>## # A tibble: 6 x 2
##   `MockJury$Attr` `MockJury$NotBeautiful`
##   &lt;fctr&gt;          &lt;lgl&gt;                  
## 1 Average         T                      
## 2 Average         T                      
## 3 Average         T                      
## 4 Average         T                      
## 5 Average         T                      
## 6 Average         T</code></pre>
<p>To get rid of one of the responses that are in one of the groups, we need to learn a little bit about data management in R. <strong><em>Brackets</em></strong> <code>([,])</code> are used to access and possibly modify the rows or columns in a tibble with entries before the comma operating on rows and entries after the comma on the columns. For example, if you want to see the results for the 5<sup>th</sup> subject, you can reference the 5<sup>th</sup> row of the tibble using <code>[5,]</code> after the tibble name:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury[<span class="dv">5</span>,]</code></pre></div>
<pre><code>##        Attr    Crime Years Serious exciting calm independent sincere warm
## 5 Beautiful Burglary     7       9        1    1           5       1    8
##   phyattr sociable kind intelligent strong sophisticated happy ownPA
## 5       8        9    4           7      9             9     8     7
##   NotBeautiful
## 5        FALSE</code></pre>
<p>We could just extract the <em>Years</em> response for the 5<sup>th</sup> subject by incorporating information on the row and column of interest (<code>Years</code> is the 3<sup>rd</sup> column):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury[<span class="dv">5</span>,<span class="dv">3</span>]</code></pre></div>
<pre><code>## [1] 7</code></pre>
<p>In R, we can use logical vectors to keep any rows of the tibble where the variable is true and drop any rows where it is false by placing the logical variable in the first element of the brackets. The reduced version of the data set should be saved with a different name such as <code>MockJury2</code> that is used here to reduce the chances of confusing it with the previous full data set:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury2 &lt;-<span class="st"> </span>MockJury[MockJury<span class="op">$</span>NotBeautiful,]</code></pre></div>
<p>You will always want to check that the correct observations were dropped either using <code>View(MockJury2)</code> or by doing a quick summary of the <code>Attr</code> variable in the new tibble.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(MockJury2<span class="op">$</span>Attr)</code></pre></div>
<pre><code>##    Beautiful      Average Unattractive 
##            0           38           37</code></pre>
<p>It ends up that R remembers the <em>Beautiful</em> category even though there are 0 observations in it now and that can cause us some problems. When we remove a group of observations<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a>, we sometimes need to clean up categorical variables to just reflect the categories that are present. The <code>factor</code> function creates categorical variables based on the levels of the variables that are observed and is useful to run here to clean up <code>Attr</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury2<span class="op">$</span>Attr &lt;-<span class="st"> </span><span class="kw">factor</span>(MockJury2<span class="op">$</span>Attr) 
<span class="kw">summary</span>(MockJury2<span class="op">$</span>Attr)</code></pre></div>
<pre><code>##      Average Unattractive 
##           38           37</code></pre>
<p>Now if we remake the boxplots and beanplots, they only contain results for the two groups of interest here as seen in Figure <a href="chapter2.html#fig:Figure2-6">2.6</a>.</p>

<div class="figure"><span id="fig:Figure2-6"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-6-1.png" alt="Boxplot and beanplot of the Years responses on the reduced MockJury2 data set." width="552" />
<p class="caption">
Figure 2.6: Boxplot and beanplot of the <em>Years</em> responses on the reduced <code>MockJury2</code> data set.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2) 
<span class="kw">beanplot</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;bisque&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre></div>
<p>The two-sample mean techniques you learned in your previous course all start with comparing the means the two groups. We can obtain the two means using the <code>mean</code> function or directly obtain the difference in the means using the <code>diffmean</code> function (both require the <code>mosaic</code> package). The <code>diffmean</code> function provides <span class="math inline">\(\bar{x}_\text{Unattractive} - \bar{x}_\text{Average}\)</span> where <span class="math inline">\(\bar{x}\)</span> (read as “x-bar”) is the sample mean of observations in the subscripted group. Note that there are two directions that you could compare the means and this function chooses to take the mean from the second group name <em>alphabetically</em> and subtract the mean from the first alphabetical group name. It is always good to check the direction of this calculation as having a difference of <span class="math inline">\(-1.84\)</span> years versus <span class="math inline">\(1.84\)</span> years could be important.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre></div>
<pre><code>##      Average Unattractive 
##     3.973684     5.810811</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre></div>
<pre><code>## diffmean 
## 1.837127</code></pre>
</div>
<div id="section2-3" class="section level2">
<h2><span class="header-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</h2>
<p>There appears to be some evidence that the <em>Unattractive</em> group is getting higher average lengths of sentences from the prisoner “jurors” than the <em>Average</em> group, but we want to make sure that the difference is real – that there is evidence to reject the assumption that the means are the same “in the population”. First, a <strong><em>null hypothesis</em></strong><a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a> which defines a <strong><em>null model</em></strong><a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a> needs to be determined in terms of <strong><em>parameters</em></strong> (the true values in the population). The research question should help you determine the form of the hypotheses for the assumed population. In the 2 independent sample mean problem, the interest is in testing a null hypothesis of <span class="math inline">\(H_0: \mu_1 = \mu_2\)</span> versus the alternative hypothesis of <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span>, where <span class="math inline">\(\mu_1\)</span> is the parameter for the true mean of the first group and <span class="math inline">\(\mu_2\)</span> is the parameter for the true mean of the second group. The alternative hypothesis involves assuming a statistical model for the <span class="math inline">\(i^{th}\ (i=1,\ldots,n_j)\)</span> response from the <span class="math inline">\(j^{th}\ (j=1,2)\)</span> group, <span class="math inline">\(\boldsymbol{y}_{ij}\)</span>, that involves modeling it as <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span>, where we assume that <span class="math inline">\(\varepsilon_{ij} \sim N(0,\sigma^2)\)</span>. For the moment, focus on the models that either assume the means are the same (null) or different (alternative), which imply:</p>
<ul>
<li><p>Null Model: <span class="math inline">\(y_{ij} = \mu + \varepsilon_{ij}\)</span> There is <strong>no</strong> difference in <strong>true</strong> means for the two groups.</p></li>
<li><p>Alternative Model: <span class="math inline">\(y_{ij} = \mu_j + \varepsilon_{ij}\)</span> There is <strong>a</strong> difference in <strong>true</strong> means for the two groups.</p></li>
</ul>
<p>Suppose we are considering the alternative model for the 4<sup>th</sup> observation (<span class="math inline">\(i=4\)</span>) from the second group (<span class="math inline">\(j=2\)</span>), then the model for this observation is <span class="math inline">\(y_{42} = \mu_2 +\varepsilon_{42}\)</span>, that defines the response as coming from the true mean for the second group plus a random error term for that observation, <span class="math inline">\(\varepsilon_{42}\)</span>. For, say, the 5<sup>th</sup> observation from the first group (<span class="math inline">\(j=1\)</span>), the model is <span class="math inline">\(y_{51} = \mu_1 +\varepsilon_{51}\)</span>. If we were working with the null model, the mean is always the same (<span class="math inline">\(\mu\)</span>) – the group specified does not change the mean we use for that observation.</p>
<p>It can be helpful to think about the null and alternative models graphically. By assuming the null hypothesis is true (means are equal) and that the random errors around the mean follow a normal distribution, we assume that the truth is as displayed in the left panel of Figure <a href="chapter2.html#fig:Figure2-7">2.7</a> – two normal distributions with the same mean and variability. The alternative model allows the two groups to potentially have different means, such as those displayed in the right panel of Figure <a href="chapter2.html#fig:Figure2-7">2.7</a> where the second group has a larger mean. Note that in this scenario, we assume that the observations all came from the same distribution except that they had different means. Depending on the statistical procedure we are using, we basically are going to assume that the observations (<span class="math inline">\(y_{ij}\)</span>) either were generated as samples from the null or alternative model. You can imagine drawing observations at random from the pictured distributions. For hypothesis testing, the null model is assumed to be true and then the unusualness of the actual result is assessed relative to that assumption. In hypothesis testing, we have to decide if we have enough evidence to reject the assumption that the null model (or hypothesis) is true. If we reject the null hypothesis, then we would conclude that the other model considered (the alternative model) is more reasonable. The researchers obviously would have hoped to encounter some sort of noticeable difference in the sentences provided for the different pictures and have been able to find enough evidence to reject the null model where the groups “look the same”.</p>

<div class="figure"><span id="fig:Figure2-7"></span>
<img src="chapter2_files/image015.png" alt="Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis." width="480" />
<p class="caption">
Figure 2.7: Illustration of the assumed situations under the null (left) and a single possibility that could occur if the alternative were true (right) and the true means were different. There are an infinite number of ways to make a plot like the right panel that satisfies the alternative hypothesis.
</p>
</div>
<p>In statistical inference, null hypotheses (and their implied models) are set up as “straw men” with every interest in rejecting them even though we assume they are true to be able to assess the evidence <span class="math inline">\(\underline{\text{against them}}\)</span>. Consider the original study design here, the pictures were randomly assigned to the subjects. If the null hypothesis were true, then we would have no difference in the population means of the groups. And this would apply if we had done a different random assignment of the pictures to the subjects. So let’s try this: assume that the null hypothesis is true and randomly re-assign the treatments (pictures) to the observations that were obtained. In other words, keep the sentences (<em>Years</em>) the same and shuffle the group labels randomly. The technical term for this is doing a <strong><em>permutation</em></strong> (a random shuffling of a grouping variable relative to the observed responses). If the null is true and the means in the two groups are the same, then we should be able to re-shuffle the groups to the observed sentences (<em>Years</em>) and get results similar to those we actually observed. If the null is false and the means are really different in the two groups, then what we observed should differ from what we get under other random permutations. The differences between the two groups should be more noticeable in the observed data set than in (most) of the shuffled data sets. It helps to see an example of a permutation of the labels to understand what this means here.</p>
<p>In the <code>mosaic</code> package, the <code>shuffle</code> function allows us to easily perform a permutation<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a>. Just one time, we can explore what a permutation of the treatment labels could look like in the <code>PermutedAttr</code> variable below. Note that the <code>Years</code> are held in the same place while the group labels are shuffled.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Perm1 &lt;-<span class="st"> </span><span class="kw">with</span>(MockJury2, <span class="kw">tibble</span>(Years, Attr, <span class="dt">PermutedAttr=</span><span class="kw">shuffle</span>(Attr)))
<span class="co">#To force the tibble to print out all rows in data set - not used often</span>
<span class="kw">data.frame</span>(Perm1) </code></pre></div>
<pre><code>##    Years         Attr PermutedAttr
## 1      1 Unattractive Unattractive
## 2      4 Unattractive      Average
## 3      3 Unattractive      Average
## 4      2 Unattractive      Average
## 5      8 Unattractive      Average
## 6      8 Unattractive      Average
## 7      1 Unattractive Unattractive
## 8      1 Unattractive Unattractive
## 9      5 Unattractive      Average
## 10     7 Unattractive Unattractive
## 11     1 Unattractive      Average
## 12     5 Unattractive Unattractive
## 13     2 Unattractive Unattractive
## 14    12 Unattractive      Average
## 15    10 Unattractive      Average
## 16     1 Unattractive      Average
## 17     6 Unattractive Unattractive
## 18     2 Unattractive      Average
## 19     5 Unattractive Unattractive
## 20    12 Unattractive Unattractive
## 21     6 Unattractive      Average
## 22     3 Unattractive      Average
## 23     8 Unattractive      Average
## 24     4 Unattractive Unattractive
## 25    10 Unattractive Unattractive
## 26    10 Unattractive      Average
## 27    15 Unattractive Unattractive
## 28    15 Unattractive      Average
## 29     3 Unattractive      Average
## 30     3 Unattractive      Average
## 31     3 Unattractive Unattractive
## 32    11 Unattractive      Average
## 33    12 Unattractive      Average
## 34     2 Unattractive Unattractive
## 35     1 Unattractive Unattractive
## 36     1 Unattractive Unattractive
## 37    12 Unattractive      Average
## 38     5      Average Unattractive
## 39     5      Average Unattractive
## 40     4      Average Unattractive
## 41     3      Average Unattractive
## 42     6      Average      Average
## 43     4      Average      Average
## 44     9      Average      Average
## 45     8      Average Unattractive
## 46     3      Average      Average
## 47     2      Average Unattractive
## 48    10      Average      Average
## 49     1      Average Unattractive
## 50     1      Average Unattractive
## 51     3      Average Unattractive
## 52     1      Average      Average
## 53     3      Average      Average
## 54     5      Average      Average
## 55     8      Average Unattractive
## 56     3      Average      Average
## 57     1      Average      Average
## 58     1      Average Unattractive
## 59     1      Average      Average
## 60     2      Average      Average
## 61     2      Average Unattractive
## 62     1      Average      Average
## 63     1      Average Unattractive
## 64     2      Average Unattractive
## 65     3      Average Unattractive
## 66     4      Average Unattractive
## 67     5      Average      Average
## 68     3      Average Unattractive
## 69     3      Average      Average
## 70     3      Average      Average
## 71     2      Average Unattractive
## 72     7      Average Unattractive
## 73     6      Average Unattractive
## 74    12      Average Unattractive
## 75     8      Average      Average</code></pre>
<p>If you count up the number of subjects in each group by counting the number of times each label (Average, Unattractive) occurs, it is the same in both the <code>Attr</code> and <code>PermutedAttr</code> columns. Permutations involve randomly re-ordering the values of a variable – here the <code>Attr</code> group labels – without changing the content of the variable. This result can also be generated using what is called <strong><em>sampling without replacement</em></strong>: sequentially select <span class="math inline">\(n\)</span> labels from the original variable (<em>Attr</em>), removing each observed label and making sure that each of the original <code>Attr</code> labels is selected once and only once. The new, randomly selected order of selected labels provides the permuted labels. Stepping through the process helps to understand how it works: after the initial random sample of one label, there would <span class="math inline">\(n - 1\)</span> choices possible; on the <span class="math inline">\(n^{th}\)</span> selection, there would only be one label remaining to select. This makes sure that all original labels are re-used but that the order is random. Sampling without replacement is like picking names out of a hat, one-at-a-time, and not putting the names back in after they are selected. It is an exhaustive process for all the original observations. <strong><em>Sampling with replacement</em></strong>, in contrast, involves sampling from the specified list with each observation having an equal chance of selection for each sampled observation – in other words, observations can be selected more than once. This is like picking <span class="math inline">\(n\)</span> names out of a hat that contains <span class="math inline">\(n\)</span> names, except that every time a name is selected, it goes back into the hat – we’ll use this technique in Section <a href="chapter2.html#section2-8">2.8</a> to do what is called <strong><em>bootstrapping</em></strong>. Both sampling mechanisms can be used to generate inferences but each has particular situations where they are most useful. For hypothesis testing, we will use permutations (sampling without replacement) as its mechanism most closely matches the null hypotheses we will be testing.</p>
<p>The comparison of the beanplots for the real data set and permuted version of the labels is what is really interesting (Figure <a href="chapter2.html#fig:Figure2-8">2.8</a>). The original difference in the sample means of the two groups was 1.84 years (<em>Unattractive</em> minus <em>Average</em>). The sample means are the <strong><em>statistics</em></strong> that estimate the parameters for the true means of the two groups. In the permuted data set, the difference in the means is 1.15 years in the opposite direction (Average had a higher mean than Unattractive in the permuted data).</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(Years<span class="op">~</span>PermutedAttr, <span class="dt">data=</span>Perm1)</code></pre></div>
<pre><code>##      Average Unattractive 
##     5.447368     4.297297</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(Years<span class="op">~</span>PermutedAttr, <span class="dt">data=</span>Perm1)</code></pre></div>
<pre><code>##  diffmean 
## -1.150071</code></pre>

<div class="figure"><span id="fig:Figure2-8"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-8-1.png" alt="Boxplots of Years responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set." width="672" />
<p class="caption">
Figure 2.8: Boxplots of Years responses versus actual treatment groups and permuted groups. Note how the responses are the same but that they are shuffled between the two groups differently in the permuted data set.
</p>
</div>
<p>These results suggest that the observed difference was larger than what we got when we did a single permutation although it was only a little bit larger than a difference we could observe in permutations if we ignore the difference in directions. Conceptually, permuting observations between group labels is consistent with the null hypothesis – this is a technique to generate results that we might have gotten if the null hypothesis were true since the responses are the same in the two groups if the null is true. We just need to repeat the permutation process many times and track how unusual our observed result is relative to this distribution of potential responses if the null were true. If the observed differences are unusual relative to the results under permutations, then there is evidence against the null hypothesis, the null hypothesis should be rejected (Reject <span class="math inline">\(H_0\)</span>), and a conclusion should be made, in the direction of the alternative hypothesis, that there is evidence that the true means differ. If the observed differences are similar to (or at least not unusual relative to) what we get under random shuffling under the null model, we would have a tough time concluding that there is any real difference between the groups based on our observed data set.</p>
</div>
<div id="section2-4" class="section level2">
<h2><span class="header-section-number">2.4</span> Permutation testing for the two sample mean situation</h2>
<p>In any testing situation, you must define some function of the observations that gives us a single number that addresses our question of interest. This quantity is called a <strong><em>test statistic</em></strong>. These often take on complicated forms and have names like <span class="math inline">\(t\)</span> or <span class="math inline">\(z\)</span> statistics that relate to their parametric (named) distributions so we know where to look up <strong><em>p-values</em></strong><a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a>. In randomization settings, they can have simpler forms because we use the data set to find the distribution of the statistic and don’t need to rely on a named distribution. We will label our test statistic <strong><em>T</em></strong> (for <strong>T</strong>est statistic) unless the test statistic has a commonly used name. Since we are interested in comparing the means of the two groups, we can define</p>
<p><span class="math display">\[T=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average},\]</span></p>
<p>which coincidentally is what the <code>diffmean</code> function provided us previously. We label our <strong><em>observed test statistic</em></strong> (the one from the original data set) as</p>
<p><span class="math display">\[T_{obs}=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average},\]</span></p>
<p>which happened to be 1.84 years here. We will compare this result to the results for the test statistic that we obtain from permuting the group labels. To denote permuted results, we will add a * to the labels:</p>
<p><span class="math display">\[T^*=\bar{x}_{\text{Unattractive}^*}-\bar{x}_{\text{Average}^*}.\]</span></p>
<p>We then compare the <span class="math inline">\(T_{obs}=\bar{x}_\text{Unattractive}-\bar{x}_\text{Average} = 1.84\)</span> to the distribution of results that are possible for the permuted results (<span class="math inline">\(T^*\)</span>) which corresponds to assuming the null hypothesis is true.</p>
<p>We need to consider lots of permutations to do a permutation test. In contrast to your introductory statistics course where, if you did this, it was just a click away, we are going to learn what was going on under the hood. Specifically, we need a <strong><em>for loop</em></strong> in R to be able to repeatedly generate the permuted data sets and record <span class="math inline">\(T^*\)</span> for each one. Loops are a basic programming task that make randomization methods possible as well as potentially simplifying any repetitive computing task. To write a “for loop”, we need to choose how many times we want to do the loop (call that <code>B</code>) and decide on a counter to keep track of where we are at in the loops (call that <code>b</code>, which goes from 1 up to <code>B</code>). The simplest loop just involves printing out the index, <code>print(b)</code> at each step. This is our first use of curly braces, { and}, that are used to group the code we want to repeatedly run as we proceed through the loop. By typing the following code in the script window and then highlighting it all and hitting the run button, R will go through the loop 5 times, printing out the counter:</p>
<pre><code>B &lt;- 5
for (b in (1:B)){
  print(b)
}</code></pre>
<p>Note that when you highlight and run the code, it will look about the same with “+” printed after the first line to indicate that all the code is connected when it appears in the console, looking like this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="op">&gt;</span><span class="st"> </span><span class="cf">for</span>(b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
<span class="op">+</span><span class="st">   </span><span class="kw">print</span>(b)
<span class="op">+</span><span class="st"> </span>}</code></pre></div>
<p>When you run these three lines of code, the console will show you the following output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">[<span class="dv">1</span>] <span class="dv">1</span>
[<span class="dv">1</span>] <span class="dv">2</span>
[<span class="dv">1</span>] <span class="dv">3</span>
[<span class="dv">1</span>] <span class="dv">4</span>
[<span class="dv">1</span>] <span class="dv">5</span></code></pre></div>
<p>Instead of printing the counter, we want to use the loop to repeatedly compute our test statistic across B random permutations of the observations. The <code>shuffle</code> function performs permutations of the group labels relative to responses and the <code>diffmean</code> difference in the two group means in the permuted data set. For a single permutation, the combination of shuffling <code>Attr</code> and finding the difference in the means, storing it in a variable called <code>Ts</code> is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Ts &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
Ts</code></pre></div>
<pre><code>##  diffmean 
## -0.616643</code></pre>
<p>And putting this inside the <code>print</code> function allows us to find the test statistic under 5 different permutations easily:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">5</span>
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Ts &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
  <span class="kw">print</span>(Ts)
}</code></pre></div>
<pre><code>##   diffmean 
## -0.8300142 
##   diffmean 
## -0.1365576 
##    diffmean 
## -0.08321479 
##  diffmean 
## 0.5035562 
## diffmean 
## 1.677098</code></pre>
<p>Finally, we would like to store the values of the test statistic instead of just printing them out on each pass through the loop. To do this, we need to create a variable to store the results, let’s call it <code>Tstar</code>. We know that we need to store <code>B</code> results so will create a vector<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a> of length B, which contains B elements, full of missing values (NA) using the <code>matrix</code> function with the <code>nrow</code> option specifying the number of elements:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
Tstar</code></pre></div>
<pre><code>##      [,1]
## [1,]   NA
## [2,]   NA
## [3,]   NA
## [4,]   NA
## [5,]   NA</code></pre>
<p>Now we can run our loop B times and store the results in <code>Tstar</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years <span class="op">~</span><span class="st"> </span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
}
Tstar</code></pre></div>
<pre><code>##             [,1]
## [1,] -0.08321479
## [2,]  0.23684211
## [3,] -0.24324324
## [4,] -0.61664296
## [5,]  0.66358464</code></pre>
<p>Five permutations are still not enough to assess whether our <span class="math inline">\(T_{obs}\)</span> of 1.84 is unusual and we need to do many permutations to get an accurate assessment of the possibilities under the null hypothesis. It is common practice to consider something like 1,000 permutations. The <code>Tstar</code> vector when we set <em>B</em> to be large, say <code>B=1000</code>, contains the permutation distribution for the selected test statistic under<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a> the null hypothesis – what is called the <strong><em>null distribution</em></strong> of the statistic. The null distribution is the distribution of possible values of a statistic under the null hypothesis. We want to visualize this distribution and use it to assess how unusual our <span class="math inline">\(T_{obs}\)</span> result of 1.84 years was relative to all the possibilities under permutations (under the null hypothesis). So we repeat the loop, now with <span class="math inline">\(B=1000\)</span> and generate a histogram, density curve, and summary statistics of the results:</p>

<div class="figure"><span id="fig:Figure2-9"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-9-1.png" alt="Histogram (left, with counts in bars) and density curve (right) of values of test statistic for 1,000 permutations." width="960" />
<p class="caption">
Figure 2.9: Histogram (left, with counts in bars) and density curve (right) of values of test statistic for 1,000 permutations.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2)
}
<span class="kw">hist</span>(Tstar, <span class="dt">label=</span>T)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Tstar)</code></pre></div>
<pre><code>##        min         Q1     median        Q3      max       mean        sd
##  -2.910384 -0.5099573 0.07681366 0.6102418 2.530583 0.04694168 0.8497364
##     n missing
##  1000       0</code></pre>
<p>Figure <a href="chapter2.html#fig:Figure2-9">2.9</a> contains visualizations of <span class="math inline">\(T^*\)</span> and the <code>favstats</code> summary provides the related numerical summaries. Our observed <span class="math inline">\(T_{obs}\)</span> of 1.84 seems fairly unusual relative to these results with only 14 <span class="math inline">\(T^*\)</span> values over 2 based on the histogram. We need to make more specific comparisons of the permuted results versus our observed result to be able to clearly decide whether our observed result is really unusual.</p>
<p>To make the comparisons more concrete, first we can enhance the previous graphs by adding the value of the test statistic from the real data set, as shown in Figure <a href="chapter2.html#fig:Figure2-10">2.10</a>, using the <code>abline</code> function to draw a vertical line at our <span class="math inline">\(T_{obs}\)</span> value specified in the <code>v</code> (for vertical) option.</p>

<div class="figure"><span id="fig:Figure2-10"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-10-1.png" alt="Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic." width="960" />
<p class="caption">
Figure 2.10: Histogram (left) and density curve (right) of values of test statistic for 1,000 permutations with bold vertical line for value of observed test statistic.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="fl">1.837</span>
<span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p>Second, we can calculate the exact number of permuted results that were as large or larger than what we observed. To calculate the proportion of the 1,000 values that were as large or larger than what we observed, we will use the <code>pdata</code> function. To use this function, we need to provide the distribution of values to compare to the cut-off (<code>Tstar</code>), the cut-off point (<code>Tobs</code>), and whether we want calculate the proportion that are below (left of) or above (right of) the cut-off (<code>lower.tail=F</code> option provides the proportion of values above the cutoff of interest).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 0.02</code></pre>
<p>The proportion of 0.02 tells us that 20 of the 1,000 permuted results (2%) were as large or larger than what we observed. This type of work is how we can generate <strong><em>p-values</em></strong> using permutation distributions. P-values, as you should remember, are the probability of getting a result as extreme as or more extreme than what we observed, <span class="math inline">\(\underline{\text{given that the null is true}}\)</span>. Finding only 20 permutations of 1,000 that were larger than our observed result suggests that it is hard to find a result like what we observed if there really were no difference, although it is not impossible.</p>
<p>When testing hypotheses for two groups, there are two types of alternative hypotheses, one-sided or two-sided. <strong><em>One-sided tests</em></strong> involve only considering differences in one-direction (like <span class="math inline">\(\mu_1 &gt; \mu_2\)</span>) and are performed when researchers can decide <strong><em>a priori</em></strong><a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a> which group should have a larger mean if there is going to be any sort of difference. In this situation, we did not know enough about the potential impacts of the pictures to know which group should be larger than the other so should do a two-sided test. It is important to remember that you can’t look at the responses to decide on the hypotheses. It is often safer and more <strong><em>conservative</em></strong><a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a> to start with a <strong><em>two-sided alternative</em></strong> (<span class="math inline">\(\mathbf{H_A: \mu_1 \ne \mu_2}\)</span>). To do a 2-sided test, find the area larger than what we observed as above. We also need to add the area in the other tail (here the left tail) similar to what we observed in the right tail. Some people suggest doubling the area in one tail but we will collect information on the number that were more as or more extreme than the same value in the other tail. In other words, we count the proportion over 1.84 and below -1.84. So we need to also find how many of the permuted results were smaller than or equal to -1.84 years to add to our previous proportion. Using <code>pdata</code> with <code>-Tobs</code> as the cut-off and <code>lower.tail=T</code> provides this result:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(Tstar, <span class="op">-</span>Tobs, <span class="dt">lower.tail=</span>T)</code></pre></div>
<pre><code>## [1] 0.014</code></pre>
<p>So the p-value to test our null hypothesis of no difference in the true means between the groups is 0.02 + 0.014, providing a p-value of 0.034. Figure <a href="chapter2.html#fig:Figure2-11">2.11</a> shows both cut-offs on the histogram and density curve.</p>

<div class="figure"><span id="fig:Figure2-11"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-11-1.png" alt="Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (1.84) and its opposite value (-1.84) required for performing the two-sided test." width="960" />
<p class="caption">
Figure 2.11: Histogram and density curve of values of test statistic for 1,000 permutations with bold lines for value of observed test statistic (1.84) and its opposite value (-1.84) required for performing the two-sided test.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p>In general, the <strong><em>one-sided test p-value</em></strong> is the proportion of the permuted results that are as extreme or more extreme than observed in the direction of the <em>alternative</em> hypothesis (lower or upper tail, remembering that this also depends on the direction of the difference taken). For the 2-sided test, the p-value is the proportion of the permuted results that are <em>less than or equal to the negative version of the observed statistic and greater than or equal to the positive version of the observed statistic</em>. Using absolute values (| |), we can simplify this: the <strong><em>two-sided p-value</em></strong> is the <em>proportion of the |permuted statistics| that are as large or larger than |observed statistic|</em>. This will always work and finds areas in both tails regardless of whether the observed statistic is positive or negative. In R, the <code>abs</code> function provides the <strong><em>absolute value</em></strong> and we can again use <code>pdata</code> to find our p-value in one line of code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 0.034</code></pre>
<p>We will encourage you to think through what might constitute strong evidence against your null hypotheses and then discuss how strong you feel the evidence is against the null hypothesis in the p-value that you obtained. Basically, p-values present a measure of evidence against the null hypothesis, with smaller values presenting more evidence against the null. They range from 0 to 1 and you should interpret them on a graded scale from strong evidence (close to 0) to no evidence (close 1). We will discuss the use of a fixed <strong><em>significance level</em></strong> below as it is still commonly used in many fields and is necessary to think about the theory of hypothesis testing, but, for the moment, we can conclude that there is moderate evidence against the null hypothesis presented by having a p-value of 0.034 because our observed result is somewhat rare relative to what we would expect if the null hypothesis was true. And so we would likely <strong><em>reject the null hypothesis</em></strong> and conclude (in the direction of the alternative) that there is a difference in the population means in the two groups.</p>
<p>Before we move on, let’s note some interesting features of the permutation distribution of the difference in the sample means shown in Figure <a href="chapter2.html#fig:Figure2-11">2.11</a>.</p>
<ol style="list-style-type: decimal">
<li><p>It is basically centered at 0. Since we are performing permutations assuming the null model is true, we are assuming that <span class="math inline">\(\mu_1 = \mu_2\)</span> which implies that <span class="math inline">\(\mu_1 - \mu_2 = 0\)</span>. This also suggests that 0 should be the center of the permutation distribution and it was.</p></li>
<li><p>It is approximately normally distributed. This is due to the <strong><em>Central Limit Theorem</em></strong><a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a>, where the <strong><em>sampling distribution</em></strong> (distribution of all possible results for samples of this size) of the difference in sample means (<span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) becomes more normally distributed as the sample sizes increase. With 38 and 37 observations in the groups, we are likely to have a relatively normal looking distribution of the difference in the sample means. This result will allow us to use a parametric method to approximate this sampling distribution under the null model if some assumptions are met, as we’ll discuss below.</p></li>
<li><p>Our observed difference in the sample means (1.84 years) is a fairly unusual result relative to the rest of these results but there are some permuted data sets that produce more extreme differences in the sample means. When the observed differences are really large, we may not see any permuted results that are as extreme as what we observed. When <code>pdata</code> gives you 0, the p-value should be reported to be smaller than 0.0001 (<strong>not 0!</strong>) since it happened in less than 1 in 1,000 tries but does occur once – in the actual data set.</p></li>
<li><p>Since our null model is not specific about the direction of the difference, considering a result like ours but in the other direction (-1.84 years) needs to be included. The observed result seems to put about the same area in both tails of the distribution but it is not exactly the same. The small difference in the tails is a useful aspect of this approach compared to the parametric method discussed below as it accounts for potential asymmetry in the sampling distribution.</p></li>
</ol>
<p>Earlier, we decided that the p-value provided moderate evidence against the null hypothesis and that we should reject the null. In this course, you will often be allowed to use your own judgment about an appropriate significance level in a particular situation (in other words, if we forget to tell you an <span class="math inline">\(\alpha\)</span> -level, you can still make a decision based on how strong you feel the evidence was against the null based on the p-value). Remembering that the p-value is the probability you would observe a result like you did (or more extreme), assuming the null hypothesis is true; this tells you that the smaller the p-value is, the more evidence you have against the null. Figure <a href="chapter2.html#fig:FigurePValStr">2.12</a> provides a diagram of some suggestions for the graded p-value interpretation that you can use. The next section provides a more formal review of the hypothesis testing infrastructure, terminology, and some of things that can happen when testing hypotheses. P-values have been (validly) criticized for the inability of studies to be reproduced, for the bias in publications to only include studies that have small p-values, and for the lack of thought that often accompanies using a fixed significance level. To alleviate some of these criticisms, we recommend reporting the strength of evidence of the result based on the p-value and also reporting and discussing the size of the estimated results (with a measure of precision of the estimated difference).</p>

<div class="figure"><span id="fig:FigurePValStr"></span>
<img src="chapter2_files/pvalueStrengths.png" alt="Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values." width="800" />
<p class="caption">
Figure 2.12: Graphic suggesting potential interpretations of strength of evidence based on gradient of p-values.
</p>
</div>
</div>
<div id="section2-5" class="section level2">
<h2><span class="header-section-number">2.5</span> Hypothesis testing (general)</h2>
<p>In hypothesis testing (sometimes more explicitly called “Null Hypothesis Significance Testing” or NHST), it is formulated to answer a specific question about a population or true parameter(s) using a statistic based on a data set. In your previous statistics course, you (hopefully) considered one-sample hypotheses about population means and proportions and the two sample mean situation we are focused on here. Our hypotheses relate to trying to answer the question about whether the population mean sentences between the two groups are different, with an initial assumption of no difference.</p>
<p>NHST is much like a criminal trial with a jury where you are in the role of a jury member. Initially, the defendant is assumed innocent. In our situation, the true means are assumed to be equal between the groups. Then evidence is presented and, as a juror, you analyze it. In statistical hypothesis testing, data are collected and analyzed. Then you have to decide if we had “enough” evidence to reject the initial assumption (“innocence” that is initially assumed). To make this decision, you want to have thought about and decided on the standard of evidence required to reject the initial assumption. In criminal cases, “beyond a reasonable doubt” is used. Wikipedia’s definition (<a href="https://en.wikipedia.org/wiki/Reasonable_doubt" class="uri">https://en.wikipedia.org/wiki/Reasonable_doubt</a>) suggests that this standard is that “there can still be a doubt, but only to the extent that it would not affect a reasonable person’s belief regarding whether or not the defendant is guilty”. In civil trials, a lower standard called a “preponderance of evidence” is used. Based on that defined and pre-decided (<em>a priori</em>) measure, you decide that the defendant is guilty or not guilty. In statistics, we can compare our p-value to a significance level, <span class="math inline">\(\alpha\)</span>, which is most of the time selected to be 5%. If our p-value is less than <span class="math inline">\(\alpha\)</span>, we reject the null hypothesis. The choice of the significance level is like the variation in standards of evidence between criminal and civil trials – and in all situations everyone should know the standards required for rejecting the initial assumption before any information is “analyzed”. Once someone is found guilty, then there is the matter of sentencing which is related to the impacts (“size”) of the crime. In statistics, this is similar to the estimated size of differences and the related judgments about whether the differences are practically important or not. If the crime is proven beyond a reasonable doubt but it is a minor crime, then the sentence will be small. With the same level of evidence and a more serious crime, the sentence will be more dramatic.</p>
<p>There are some important aspects of the testing process to note that inform how we interpret statistical hypothesis test results. When someone is found “not guilty”, it does not mean “innocent”, it just means that there was not enough evidence to find the person guilty “beyond a reasonable doubt”. Not finding enough evidence to reject the null hypothesis does not imply that the true means are equal, just that there was not enough evidence to conclude that they were different. There are many potential reasons why we might fail to reject the null, but the most common one is that our sample size was too small (which is related to having too little evidence). Other reasons include simply the variation in taking a random sample from the population(s). This randomness in samples and the differences in the sample means also implies that p-values are random and can easily vary if the data set had been slightly different. This also relates to the suggestion of using a graded interpretation of p-values – if the p-value is an estimated quantity, is there really any difference between p-values of 0.049 and 0.051? We probably shouldn’t think there is a big difference in results for these two p-values even though the standard NHST reject/fail to reject the null approach considers these as completely different results. So where does that leave us? Interpret the p-values using strength of evidence against the null hypothesis, remembering that smaller (but not really small) p-values can still be interesting. And if you think the p-value is small enough, then you can reject the null hypothesis.</p>
<p>Throughout this material, we will continue to re-iterate the distinctions between parameters and statistics and want you to be clear about the distinctions between estimates based on the sample and inferences for the population or true values of the parameters of interest. Remember that statistics are summaries of the sample information and parameters are characteristics of populations (which we rarely know). In the two-sample mean situation, the sample means are always at least a little different – that is not an interesting conclusion. What is interesting is whether we have enough evidence to feel like we have proven that the population or true means differ “beyond a reasonable doubt”.</p>
<p>The scope of any inferences is constrained based on whether there is a <strong><em>random sample</em></strong> (RS) and/or <strong><em>random assignment</em></strong> (RA). Table <a href="chapter2.html#tab:Table2-2">2.2</a> contains the four possible combinations of these two characteristics of a given study. Random assignment of treatment levels to subjects allows for causal inferences for differences that are observed – the difference in treatment levels is said to cause differences in the mean responses. Random sampling (or at least some sort of representative sample) allows inferences to be made to the population of interest. If we do not have RA, then causal inferences cannot be made. If we do not have a representative sample, then our inferences are limited to the sampled subjects.</p>


<table>
<caption><span id="tab:Table2-2">Table 2.2: </span> Scope of inference summary.</caption>
<colgroup>
<col width="30%" />
<col width="34%" />
<col width="34%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Random<br />
Sampling/Random<br />
Assignment</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– Yes (controlled experiment)</strong></th>
<th align="left"><strong>Random Assignment (RA)<br />
– No (observational study)</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Random Sampling (RS)<br />
– Yes (or some method<br />
that results in a<br />
representative sample of<br />
population of<br />
interest)</strong></td>
<td align="left">Because we have RS, we can<br />
generalize inferences to the<br />
population the RS was taken<br />
from. Because we have<br />
RA we can assume the groups<br />
were equivalent on all aspects<br />
except for the treatment<br />
and can establish causal inference.<br />
</td>
<td align="left">Can generalize inference to<br />
population the RS was taken<br />
from but cannot establish<br />
causal inference (no RA<br />
– cannot isolate treatment<br />
variable as only difference<br />
among groups, could be<br />
confounding variables).</td>
</tr>
<tr class="even">
<td align="left"><strong>Random Sampling (RS)<br />
– No (usually a<br />
convenience sample)</strong></td>
<td align="left">Cannot generalize inference to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Can establish causal<br />
inference due to RA <span class="math inline">\(\rightarrow\)</span><br />
the inference from this type of<br />
study applies only to the sample.</td>
<td align="left">Cannot generalize inference to<br />
the population of interest<br />
because the sample was<br />
not random and could be<br />
biased – may not be<br />
“representative” of the<br />
population of interest.<br />
Cannot establish causal<br />
inference due to lack of RA of<br />
the treatment.</td>
</tr>
</tbody>
</table>

<p>A simple example helps to clarify how the scope of inference can change based on the study design. Suppose we are interested in studying the GPA of students. If we had taken a random sample from, say, the STAT 217 students in a given semester, our scope of inference would be the population of STAT 217 students in that semester. If we had taken a random sample from the entire MSU population, then the inferences would be to the entire MSU population in that semester. These are similar types of problems but the two populations are very different and the group you are trying to make conclusions about should be noted carefully in your results – it does matter! If we did not have a representative sample, say the students could choose to provide this information or not, then we can only make inferences to volunteers. These volunteers might differ in systematic ways from the entire population of STAT 217 students so we cannot safely extend our inferences beyond the group that volunteered.</p>
<p>To consider the impacts of RA versus results from purely observational studies, we need to be comparing groups. Suppose that we are interested in differences in the mean GPAs for different sections of STAT 217 and that we take a random sample of students from each section and compare the results and find evidence of some difference. In this scenario, we can conclude that there is some difference in the population of STAT 217 students but we can’t say that being in different sections caused the differences in the mean GPAs. Now suppose that we randomly assigned every STAT 217 student to get extra training in one of three different study techniques and found evidence of differences among the training methods. We could conclude that the training methods caused the differences in these students. These conclusions would only apply to STAT 217 students and could not be generalized to a larger population of students. If we took a random sample of STAT 217 students (say only 10 from each section) and then randomly assigned them to one of three training programs and found evidence of differences, then we can say that the training programs caused the differences. But we can also say that we have evidence that those differences pertain to the population of STAT 217 students. This seems similar to the scenario where all STAT 217 students participated in the training programs except that by using random sampling, only a fraction of the population needs to actually be studied to make inferences to the entire population of interest – saving time and money.</p>
<p>A quick summary of the terminology of hypothesis testing is useful at this point. The <strong><em>null hypothesis</em></strong> (<span class="math inline">\(H_0\)</span>) states that there is no difference or no relationship in the population. This is the statement of no effect or no difference and the claim that we are trying to find evidence against. In this chapter, <span class="math inline">\(H_0\)</span>: <span class="math inline">\(\mu_1=\mu_2\)</span>. When doing two-group problems, you always need to specify which group is 1 and which one is 2 because the order does matter. The <strong><em>alternative hypothesis</em></strong> (<span class="math inline">\(H_1\)</span> or <span class="math inline">\(H_A\)</span>) states a specific difference between parameters. This is the research hypothesis and the claim about the population that we hope to demonstrate is more reasonable to conclude than the null hypothesis. In the two-group situation, we can have <strong><em>one-sided alternatives</em></strong> <span class="math inline">\(H_A: \mu_1 &gt; \mu_2\)</span> (greater than) or <span class="math inline">\(H_A: \mu_1 &lt; \mu_2\)</span> (less than) or, the more common, <strong><em>two-sided alternative</em></strong> <span class="math inline">\(H_A: \mu_1 \ne \mu_2\)</span> (not equal to). We usually default to using two-sided tests because we often do not know enough to know the direction of a difference in advance, especially in more complicated situations. The <strong><em>sampling distribution under the null</em></strong> is the distribution of all possible values of a statistic under the assumption that <span class="math inline">\(H_0\)</span> is true. It is used to calculate the <strong><em>p-value</em></strong>, the probability of obtaining a result as extreme or more extreme than what we observed given that the null hypothesis is true. We will find sampling distributions using <strong><em>nonparametric</em></strong> approaches (like the permutation approach used previously) and <strong><em>parametric</em></strong> methods (using “named” distributions like the <span class="math inline">\(t\)</span>, F, and <span class="math inline">\(\chi^2\)</span>).</p>
<p>Small p-values are evidence against the null hypothesis because the observed result is unlikely due to chance if <span class="math inline">\(H_0\)</span> is true. Large p-values provide no evidence against <span class="math inline">\(H_0\)</span> but do not allow us to conclude that the null hypothesis is correct – just that we didn’t find enough evidence to think it was wrong. The <strong><em>level of significance</em></strong> is an <em>a priori</em> definition of how small the p-value needs to be to provide “enough” (sufficient) evidence against <span class="math inline">\(H_0\)</span>. This is most useful to prevent sliding the standards after the results are found. We can compare the p-value to the level of significance to decide if the p-value is small enough to constitute sufficient evidence to reject the null hypothesis. We use <span class="math inline">\(\alpha\)</span> to denote the level of significance and most typically use 0.05 which we refer to as the 5% significance level. We can compare the p-value to this level and make a decision, focusing our interpretation on the strength of evidence we found based on the p-value from very strong to none. If we are using the strict version of NHST, the two options for <em>decisions</em> are to either <em>reject the null hypothesis</em> if the p-value <span class="math inline">\(\le \alpha\)</span> or <em>fail to reject the null hypothesis</em> if the p-value <span class="math inline">\(&gt; \alpha\)</span>. When interpreting hypothesis testing results, remember that the p-value is a measure of how unlikely the observed outcome was, assuming that the null hypothesis is true. It is <strong>NOT</strong> the probability of the data or the probability of either hypothesis being true. The p-value, simply, is a measure of evidence against the null hypothesis.</p>
<p>Although we want to use graded evidence to interpret p-values, there is one situation where thinking about comparisons to fixed <span class="math inline">\(\alpha\)</span> levels is useful for understanding and studying statistical hypothesis testing. The specific definition of <span class="math inline">\(\alpha\)</span> is that it is the probability of rejecting <span class="math inline">\(H_0\)</span> when <span class="math inline">\(H_0\)</span> is true, the probability of what is called a <strong><em>Type I error</em></strong>. Type I errors are also called <strong><em>false rejections</em></strong> or <strong><em>false detections</em></strong>. In the two-group mean situation, a Type I error would be concluding that there is a difference in the true means between the groups when none really exists in the population. In the courtroom setting, this is like falsely finding someone guilty. We don’t want to do this very often, so we use small values of the significance level, allowing us to control the rate of Type I errors at <span class="math inline">\(\alpha\)</span>. We also have to worry about <strong>Type II errors</strong>, which are failing to reject the null hypothesis when it’s false. In a courtroom, this is the same as failing to convict a truly guilty person. This most often occurs due to a lack of evidence that could be due to a small sample size or merely just an unusual sample from the population. You can use the Table <a href="chapter2.html#tab:Table2-3">2.3</a> to help you remember all the possibilities.</p>

<table>
<caption><span id="tab:Table2-3">Table 2.3: </span> Table of decisions and truth scenarios in a hypothesis testing situation. But we never know the truth in a real situation.</caption>
<colgroup>
<col width="35%" />
<col width="32%" />
<col width="32%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"> </th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>True</strong></th>
<th align="left"><span class="math inline">\(\mathbf{H_0}\)</span> <strong>False</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>FTR</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Correct decision</td>
<td align="left">Type II error</td>
</tr>
<tr class="even">
<td align="left"><strong>Reject</strong> <span class="math inline">\(\mathbf{H_0}\)</span></td>
<td align="left">Type I error</td>
<td align="left">Correct decision</td>
</tr>
</tbody>
</table>
<p>In comparing different procedures or in planning studies, there is an interest in studying the rate or probability of Type I and II errors. The probability of a Type I error was defined previously as <span class="math inline">\(\alpha\)</span>, the significance level. The <strong><em>power</em></strong> of a procedure is the probability of rejecting the null hypothesis when it is false. Power is defined as</p>
<p><span class="math display">\[\text{Power} = 1 - \text{Probability(Type II error) } = 
\text{Probability(Reject } H_0 | H_0 \text{ is false),}\]</span></p>
<p>or, in words, the probability of detecting a difference when it actually exists. We want to use a statistical procedure that controls the Type I error rate at the pre-specified level and has high power to detect false null hypotheses. Increasing the sample size is one of the most commonly used methods for increasing the power in a given situation. Sometimes we can choose among different procedures and use the power of the procedures to help us make that selection. Note that there are many ways <span class="math inline">\(H_0\)</span> false and the power changes based on how false the null hypothesis actually is. To make this concrete, suppose that the true mean sentences differed by either 1 or 20 years in previous example. The chances of rejecting the null hypothesis are much larger when the groups actually differ by 20 years than if they differ by just 1 year, given the same sample size. For a given difference in the true means, the larger the sample, the higher the power of the study to actually find evidence of a difference in the groups.</p>
<p>After making a decision (was there enough evidence to reject the null or not), we want to make the conclusions specific to the problem of interest. If we reject <span class="math inline">\(H_0\)</span>, then we can conclude that there was sufficient evidence at the <span class="math inline">\(\alpha\)</span>-level that the null hypothesis is wrong (and the results point in the direction of the alternative). If we fail to reject <span class="math inline">\(H_0\)</span> (FTR <span class="math inline">\(H_0\)</span>), then we can conclude that there was insufficient evidence at the <span class="math inline">\(\alpha\)</span>-level to say that the null hypothesis is wrong. We are <strong>NOT</strong> saying that the null is correct and we <strong>NEVER</strong> accept the null hypothesis. We just failed to find enough evidence to say it’s wrong. If we find sufficient evidence to reject the null, then we need to revisit the method of data collection and design of the study to discuss scope of inference. Can we discuss causality (due to RA) and/or make inferences to a larger group than those in the sample (due to RS)?</p>
<p>To perform a hypothesis test, there are some steps to remember to complete to make sure you have thought through all the aspects of the results.</p>
<table>
<colgroup>
<col width="100%" />
</colgroup>
<tbody>
<tr class="odd">
<td align="left"><strong>Outline of 6+ steps to perform a Hypothesis Test</strong></td>
</tr>
<tr class="even">
<td align="left">Isolate the claim to be proved, method to use (define a test statistic T), and significance level.</td>
</tr>
<tr class="odd">
<td align="left">1. Write the null and alternative hypotheses,</td>
</tr>
<tr class="even">
<td align="left">2. Plot the data and assess the “Validity Conditions” for the procedure being used (discussed below),</td>
</tr>
<tr class="odd">
<td align="left">3. Find the value of the appropriate test statistic,</td>
</tr>
<tr class="even">
<td align="left">4. Find the p-value,</td>
</tr>
<tr class="odd">
<td align="left">5. Make a decision, and</td>
</tr>
<tr class="even">
<td align="left">6. Write a conclusion specific to the problem, including scope of inference discussion. Report and discuss an estimate of the differences.</td>
</tr>
</tbody>
</table>
</div>
<div id="section2-6" class="section level2">
<h2><span class="header-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</h2>
<p>In developing statistical inference techniques, we need to define the test statistic, <span class="math inline">\(T\)</span>, that measures the quantity of interest. To compare the means of two groups, a statistic is needed that measures their differences. In general, for comparing two groups, the choices are simple – a difference in the means often works well and is a natural choice. There are other options such as tracking the ratio of means or possibly the difference in medians. Instead of just using the difference in the means, we also could “standardize” the difference in the means by dividing by an appropriate quantity that reflects the variation in the difference in the means. All of these are valid and can sometimes provide similar results - it ends up that there are many possibilities for testing using the randomization (nonparametric) techniques introduced previously. Parametric statistical methods focus on means because the statistical theory surrounding means is quite a bit easier (not easy, just easier) than other options but there are just a couple of test statistics that you can use and end up with named distributions to use for generating inferences. Randomization techniques allow inference for other quantities but our focus here will be on using randomization for inferences on means to see the similarities with the more traditional parametric procedures.</p>
<p>In two-sample mean situations, instead of working just with the difference in the means, we often calculate a test statistic that is called the <strong><em>equal variance two-independent samples t-statistic</em></strong>. The test statistic is</p>
<p><span class="math display">\[t = \frac{\bar{x}_1 - \bar{x}_2}{s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}},\]</span></p>
<p>where <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> are the sample variances for the two groups, <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> are the sample sizes for the two groups, and the <strong><em>pooled sample standard deviation</em></strong>,</p>
<p><span class="math display">\[s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}.\]</span></p>
<p>The <span class="math inline">\(t\)</span>-statistic keeps the important comparison between the means in the numerator that we used before and standardizes (re-scales) that difference so that <span class="math inline">\(t\)</span> will follow a <span class="math inline">\(t\)</span>-distribution (a parametric “named” distribution) if certain assumptions are met. But first we should see if standardizing the difference in the means had an impact on our permutation test results. Instead of using the <code>diffmean</code> function, we will use the <code>t.test</code> function (see its full use below) and have it calculate the formula for <span class="math inline">\(t\)</span> for us. The R code “<code>$statistic</code>” is basically a way of extracting just the number we want to use for <span class="math inline">\(T\)</span> from a larger set of output the <code>t.test</code> function wants to provide you. We will see below that <code>t.test</code> switches the order of the difference (now it is <em>Average</em> - <em>Unattractive</em>) – always carefully check for the direction of the difference in the results. Since we are doing a two-sided test, the code resembles the permutation test code in Section <a href="chapter2.html#section2-4">2.4</a> with the new <span class="math inline">\(t\)</span>-statistic replacing the difference in the sample means that we used before.</p>
<p>The permutation distribution in Figure <a href="chapter2.html#fig:Figure2-12">2.13</a> looks similar to the previous results with slightly different x-axis scaling. The observed <span class="math inline">\(t\)</span>-statistic was <span class="math inline">\(-2.17\)</span> and the proportion of permuted results that were as or more extreme than the observed result was 0.031. This difference is due to a different set of random permutations being selected. If you run permutation code, you will often get slightly different results each time you run it. If you are uncomfortable with the variation in the results, you can run more than <span class="math inline">\(B=\)</span> 1,000 permutations (say 10,000) and the variability in the resulting p-values will be reduced further. Usually this uncertainty will not cause any substantive problems – but do not be surprised if your results vary from a colleagues if you are both analyzing the same data set or if you re-run your permutation code.</p>

<div class="figure"><span id="fig:Figure2-12"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-12-1.png" alt="Permutation distribution of the \(t\)-statistic." width="960" />
<p class="caption">
Figure 2.13: Permutation distribution of the <span class="math inline">\(t\)</span>-statistic.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">t.test</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
Tobs</code></pre></div>
<pre><code>##        t 
## -2.17023</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">t.test</span>(Years<span class="op">~</span><span class="kw">shuffle</span>(Attr), <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>##     t 
## 0.031</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p>The parametric version of these results is based on using what is called the <strong><em>two-independent sample t-test</em></strong>. There are actually two versions of this test, one that assumes that variances are equal in the groups and one that does not. There is a rule of thumb that if the <strong>ratio of the larger standard deviation over the smaller standard deviation is less than 2, the equal variance procedure is OK</strong>. It ends up that this assumption is less important if the sample sizes in the groups are approximately equal and more important if the groups contain different numbers of observations. In comparing the two potential test statistics, the procedure that assumes equal variances has a complicated denominator (see the formula above for <span class="math inline">\(t\)</span> involving <span class="math inline">\(s_p\)</span>) but a simple formula for <strong><em>degrees of freedom</em></strong> (<strong><em>df</em></strong>) for the <span class="math inline">\(t\)</span>-distribution (<span class="math inline">\(df=n_1+n_2-2\)</span>) that approximates the distribution of the test statistic, <span class="math inline">\(t\)</span>, under the null hypothesis. The procedure that assumes unequal variances has a simpler test statistic and a very complicated degrees of freedom formula. The equal variance procedure is most similar to the ANOVA methods we will consider in Chapters <a href="chapter3.html#chapter3">3</a> and <a href="chapter4.html#chapter4">4</a> so that will be our focus for the two group problem. Fortunately, both of these methods are readily available in the <code>t.test</code> function in R if needed.</p>

<div class="figure"><span id="fig:Figure2-13"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-13-1.png" alt="Plots of \(t\) distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the \(t\) distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the \(t\) distribution almost matches a standard normal curve." width="672" />
<p class="caption">
Figure 2.14: Plots of <span class="math inline">\(t\)</span> distributions with 2, 10, and 20 degrees of freedom and a normal distribution (dashed line). Note how the <span class="math inline">\(t\)</span> distributions get closer to the normal distribution as the degrees of freedom increase and at 20 degrees of freedom, the <span class="math inline">\(t\)</span> distribution <em>almost</em> matches a standard normal curve.
</p>
</div>
<p>If the assumptions for the equal variance <span class="math inline">\(t\)</span>-test are met and the null hypothesis is true, then the sampling distribution of the test statistic should follow a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n_1+n_2-2\)</span> degrees of freedom. The <strong><em>t-distribution</em></strong> is a bell-shaped curve that is more spread out for smaller values of degrees of freedom as shown in Figure <a href="chapter2.html#fig:Figure2-13">2.14</a>. The <span class="math inline">\(t\)</span>-distribution looks more and more like a <strong><em>standard normal distribution</em></strong> (<span class="math inline">\(N(0,1)\)</span>) as the degrees of freedom increase.</p>
<p>To get the p-value for the parametric <span class="math inline">\(t\)</span>-test, we need to calculate the test statistic and <span class="math inline">\(df\)</span>, then look up the areas in the tails of the <span class="math inline">\(t\)</span>-distribution relative to the observed <span class="math inline">\(t\)</span>-statistic. We’ll learn how to use R to do this below, but for now we will allow the <code>t.test</code> function to take care of this. The <code>t.test</code> function uses our formula notation (<code>Years~Attr</code>) and then <code>data=...</code> as we saw before for making plots. To get the equal-variance test result, the <code>var.equal=T</code> option needs to be turned on. Then <code>t.test</code> provides us with lots of useful output. The three results we’ve been discussing are available in the output below – the test statistic value (-2.17), <span class="math inline">\(df=73\)</span>, and the p-value, from the <span class="math inline">\(t\)</span>-distribution with 73 degrees of freedom, of 0.033.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(Years <span class="op">~</span><span class="st"> </span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811</code></pre>
<p>So the parametric <span class="math inline">\(t\)</span>-test gives a p-value of 0.033 from a test statistic of -2.1702. The negative sign on the test statistic occurred because the function took <em>Average</em> - <em>Unattractive</em> which is the opposite direction as <code>diffmean</code>. The p-value is very similar to the two permutation results found before. The reason for this similarity is that the permutation distribution looks an awful like a <span class="math inline">\(t\)</span>-distribution with 73 degrees of freedom. Figure <a href="chapter2.html#fig:Figure2-14">2.15</a> shows how similar the two distributions happened to be here.</p>

<div class="figure"><span id="fig:Figure2-14"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-14-1.png" alt="Plot of permutation and \(t\) distribution with \(df=73\). Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values." width="576" />
<p class="caption">
Figure 2.15: Plot of permutation and <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(df=73\)</span>. Note the close match in the two distributions, especially in the tails of the distributions where we are obtaining the p-values.
</p>
</div>
<p>In your previous statistics course, you might have used an applet or a table to find p-values such as what was provided in the previous R output. When not directly provided in the output of a function, R can be used to look up p-values<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a> from named distributions such as the <span class="math inline">\(t\)</span>-distribution. In this case, the distribution of the test statistic under the null hypothesis is a <span class="math inline">\(t(73)\)</span> or a <span class="math inline">\(t\)</span> with 73 degrees of freedom. The <code>pt</code> function is used to get p-values from the <span class="math inline">\(t\)</span>-distribution in the same manner that <code>pdata</code> could help us to find p-values from the permutation distribution. We need to provide the <code>df=...</code> and specify the tail of the distribution of interest using the <code>lower.tail</code> option along with the cutoff of interest. If we want the area to the left of -2.17:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.1702</span>, <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>T)</code></pre></div>
<pre><code>## [1] 0.01662286</code></pre>
<p>And we can double it to get the p-value that <code>t.test</code> provided earlier, because the <span class="math inline">\(t\)</span>-distribution is symmetric:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="op">-</span><span class="fl">2.1702</span>, <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>T)</code></pre></div>
<pre><code>## [1] 0.03324571</code></pre>
<p>More generally, we could always make the test statistic positive using the absolute value, find the area to the right of it, and then double that for a two-sided test p-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="kw">abs</span>(<span class="op">-</span><span class="fl">2.1702</span>), <span class="dt">df=</span><span class="dv">73</span>, <span class="dt">lower.tail=</span>T)</code></pre></div>
<pre><code>## [1] 1.966754</code></pre>
<p>Permutation distributions do not need to match the named parametric distribution to work correctly, although this happened in the previous example. The parametric approach, the <span class="math inline">\(t\)</span>-test, requires certain conditions to be met for the sampling distribution of the statistic to follow the named distribution and provide accurate p-values. The conditions for the equal variance t-test are:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independent observations</strong>: Each observation obtained is unrelated to all other observations. To assess this, consider whether anything in the data collection might lead to clustered or related observations that are un-related to the differences in the groups. For example, was the same person measured more than once?<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a></p></li>
<li><p><strong>Equal variances</strong> in the groups (because we used a procedure that assumes equal variances! – there is another procedure that allows you to relax this assumption if needed…). To assess this, compare the standard deviations and variability in the beanplots and see if they look noticeably different. Be particularly critical of this assessment if the sample sizes differ greatly between groups.</p></li>
<li><p><strong>Normal distributions</strong> of the observations in each group. We’ll learn more diagnostics later, but the boxplots and beanplots are a good place to start to help you look for skews or outliers, which were both present here. If you find skew and/or outliers, that would suggest a problem with the assumption of normality as normal distributions are symmetric and extreme observations occur very rarely.</p></li>
</ol>
<p>For the permutation test, we relax the third condition and replace it with:</p>
<ol start="3" style="list-style-type: decimal">
<li><strong><em>Similar distributions for the groups:</em></strong> The permutation approach allows valid inferences as long as the two groups have similar shapes and only possibly differ in their centers. In other words, the distributions need not look normal for the procedure to work well, but they do need to look similar.</li>
</ol>
<p>In the prisoner “juror” study, we can assume that the independent observation condition is met because there is no information suggesting that the same subjects were measured more than once or that some other type of grouping in the responses was present (like the subjects were divided in groups and placed in rooms to discuss their responses prior to submitting them). The equal variance condition might be violated. The variances need not be equal as the procedure can still provide reasonable results with some violation of this assumption. The standard deviations are 2.8 vs 4.4, so this difference is not “large” according to the rule of thumb noted above. It is, however, close to being considered problematic. It would be difficult to reasonably assume that the normality condition is met here (Figure <a href="chapter2.html#fig:Figure2-6">2.6</a>) with clear right skews in both groups and potential outliers which causes concerns for (3) for the parametric procedure. The shapes look similar for the two groups so there is less reason to be concerned with using the permutation approach based on its version of condition (3) above.</p>
<p>The permutation approach is resistant to impacts of violations of the normality assumption. It is not resistant to impacts of violations of any of the other assumptions. In fact, it can be quite sensitive to unequal variances as it will detect differences in the variances of the groups instead of differences in the means. Its scope of inference is the same as the parametric approach. It also can provide similarly inaccurate conclusions in the presence of non-independent observations as for the parametric approach. In this example, we discover that parametric and permutation approaches provide very similar inferences.</p>
</div>
<div id="section2-7" class="section level2">
<h2><span class="header-section-number">2.7</span> Second example of permutation tests</h2>
<p>In every chapter, the first example, used to motivate and explain the methods, is followed with a “worked” example where we focus just on the results. In a previous semester, some of the STAT 217 students (<strong><em>n</em></strong>=79) provided information on their <em>Sex</em>, <em>Age</em>, and current cumulative <em>GPA</em>. We might be interested in whether Males and Females had different average GPAs. First, we can take a look at the difference in the responses by groups based on the output and as displayed in Figure <a href="chapter2.html#fig:Figure2-15">2.16</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s217 &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/s217.csv&quot;</span>)
<span class="kw">require</span>(mosaic)
<span class="kw">require</span>(beanplot)
<span class="kw">mean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre></div>
<pre><code>##        F        M 
## 3.338378 3.088571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre></div>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>

<div class="figure"><span id="fig:Figure2-15"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-15-1.png" alt="Side-by-side boxplot and beanplot of GPAs of STAT 217 students by sex." width="480" />
<p class="caption">
Figure 2.16: Side-by-side boxplot and beanplot of GPAs of STAT 217 students by sex.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)
<span class="kw">beanplot</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;lightblue&quot;</span>, <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>)</code></pre></div>
<p>In these data, the distributions of the GPAs look to be left skewed but maybe not as dramatically as the responses were right-skewed in the MockJury example. The Female GPAs look to be slightly higher than for Males (0.25 GPA difference in the means) but is that a “real” difference? We need our inference tools to more fully assess these differences.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">diffmean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre></div>
<pre><code>##   diffmean 
## -0.2498069</code></pre>
<p>First, we can try the parametric approach:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>So the test statistic was observed to be <span class="math inline">\(t=2.69\)</span> and it hopefully follows a <span class="math inline">\(t(77)\)</span> distribution under the null hypothesis. This provides a p-value of 0.008713 that we can trust if all the conditions to use this procedure are met. Compare these results to the permutation approach, which relaxes that normality assumption, with the results that follow. In the permutation test, <span class="math inline">\(T=2.692\)</span> and the p-value is 0.005 which is a little smaller than the result provided by the parametric approach. The agreement of the two approaches, again, provides some re-assurance about the use of either approach.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">t.test</span>(GPA<span class="op">~</span><span class="kw">shuffle</span>(Sex), <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T)<span class="op">$</span>statistic
}
<span class="kw">pdata</span>(<span class="kw">abs</span>(Tstar), <span class="kw">abs</span>(Tobs), <span class="dt">lower.tail=</span>F)</code></pre></div>

<div class="figure"><span id="fig:Figure2-16"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-16-1.png" alt="Histogram and density curve of permutation distribution of test statistic for STAT 217 GPAs." width="960" />
<p class="caption">
Figure 2.17: Histogram and density curve of permutation distribution of test statistic for STAT 217 GPAs.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span>Tobs, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p>Here is a full write-up of the results using all 6+ hypothesis testing steps, using the permutation results for the grade data:</p>
<ol start="0" style="list-style-type: decimal">
<li><p><em>Isolate the claim to be proved and method to use (define a test statistic T)</em>. We want to test for a difference in the means between males and females and will use the equal variance two-sample t-test statistic to compare them, and we think that the 5% significance level seems reasonable in this situation.</p></li>
<li><p>Write the null and alternative hypotheses</p>
<ul>
<li><p><span class="math inline">\(H_0: \mu_\text{male} = \mu_\text{female}\)</span></p>
<ul>
<li>where <span class="math inline">\(\mu_\text{male}\)</span> is the true mean GPA for males and <span class="math inline">\(\mu_\text{female}\)</span> is true mean GPA for females.</li>
</ul></li>
<li><p><span class="math inline">\(H_A: \mu_\text{male} \ne \mu_\text{female}\)</span></p></li>
</ul></li>
<li><p>Check conditions for the procedure being used</p>
<ul>
<li><p><strong>Independent observations condition</strong>: It appears that this assumption is met because there is no reason to assume any clustering or grouping of responses that might create dependence in the observations. The only possible consideration is that the observations were taken from different sections and there could be some differences between the sections. However, for overall GPA there is not too much likelihood that the overall GPAs would vary greatly so this not likely to be a big issue. The only way this could create a violation here is if certain sections tended to attract students with different GPA levels (such as the 9 am section had the best/worst GPA students…).</p></li>
<li><p><strong>Equal variance condition</strong>: There is a small difference in the range of the observations in the two groups but the standard deviations are very similar so there is no evidence that this condition is violated.</p></li>
<li><p><strong>Similar distribution condition</strong>: Based on the side-by-side boxplots and beanplots, it appears that both groups have slightly left-skewed distributions, which could be problematic for the parametric approach, but there is no evidence that the permutation approach condition is violated since the distributions look to have fairly similar shapes.</p></li>
</ul></li>
<li><p>Find the value of the appropriate test statistic</p>
<ul>
<li><span class="math inline">\(T=2.69\)</span> from the previous R output.</li>
</ul></li>
</ol>

<ol start="4" style="list-style-type: decimal">
<li><p>Find the p-value</p>
<ul>
<li><p>p-value=0.005 from the permutation distribution results.</p></li>
<li><p>This means that there is about a 0.5% chance we would observe a difference in mean GPA (female-male or male-female) of 0.25 points or more if there in fact is no difference in true mean GPA between females and males in STAT 217 in a particular semester.</p></li>
</ul></li>
<li><p>Decision</p>
<ul>
<li>Since the p-value is “small”, we find strong evidence against the null hypothesis and can reject the null hypothesis.</li>
</ul></li>
<li><p>Conclusion and scope of inference, specific to the problem</p>
<ul>
<li><p>There is strong evidence against the null hypothesis of no difference in the true mean GPA between males and females for the STAT 217 students in this semester and so we conclude that there is evidence of a difference in the mean GPAs between males and females in STAT 217 students.</p></li>
<li><p>Because this was not a randomized experiment, we can’t say that the difference in sex causes the difference in mean GPA and because it was not a random sample from a larger population, our inferences only pertain the STAT 217 students that responded to the survey in that semester.</p></li>
<li><p>Females were estimated to have a higher mean GPA by 0.25 points. The next section discusses confidence intervals that we could add to this result to quantify the uncertainty in this estimate.</p></li>
</ul></li>
</ol>
</div>
<div id="section2-8" class="section level2">
<h2><span class="header-section-number">2.8</span> Confidence intervals and bootstrapping</h2>
<p>Randomly shuffling the treatments between the observations is like randomly sampling the treatments without replacement. In other words, we randomly sample one observation at a time from the treatments until we have <span class="math inline">\(n\)</span> observations. This provides us with a technique for testing hypotheses because it provides new splits of the observations into groups that are as interesting as what we observed if the null hypothesis is assumed true. In most situations, we also want to estimate parameters of interest and provide <strong><em>confidence intervals</em></strong> for those parameters (an interval where we are __% <strong><em>confident</em></strong> that the true parameter lies). As before, there are two options we will consider – a parametric and a nonparametric approach. The nonparametric approach will be using what is called <strong><em>bootstrapping</em></strong> and draws its name from “pull yourself up by your bootstraps” where you improve your situation based on your own efforts. In statistics, we make our situation or inferences better by re-using the observations we have by assuming that the sample represents the population. Since each observation represents other similar observations in the population that we didn’t get to measure, if we <strong><em>sample with replacement</em></strong> to generate a new data set of size <em>n</em> from our data set (also of size <em>n</em>) it mimics the process of taking repeated random samples of size <span class="math inline">\(n\)</span> from our population of interest. This process also ends up giving us useful sampling distributions of statistics even when our standard normality assumption is violated, similar to what we encountered in the permutation tests. Bootstrapping is especially useful in situations where we are interested in statistics other than the mean (say we want a confidence interval for a median or a standard deviation) or when we consider functions of more than one parameter and don’t want to derive the distribution of the statistic (say the difference in two medians). In this text, bootstrapping is used to provide more trustworthy inferences when some of our assumptions (especially normality) might be violated for our parametric procedure.</p>
<p>To perform bootstrapping, we will use the <code>resample</code> function from the <code>mosaic</code> package. We can apply this function to a data set and get a new version of the data set by sampling new observations <em>with replacement</em> from the original one. The new, bootstrapped version of the data set (called <code>MockJury_BTS</code> below) contains a new variable called <code>orig.id</code> which is the number of the subject from the original data set. By summarizing how often each of these id’s occurred in a bootstrapped data set, we can see how the re-sampling works. The <code>table</code> function will count up how many times each observation was used in the bootstrap sample, providing a row with the id followed by a row with the count<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a>. In the first bootstrap sample shown, the 1<sup>st</sup>, 4<sup>th</sup>, and 10<sup>th</sup> observations were sampled one time each, the 5<sup>th</sup> observation was sampled three times, and the 7<sup>th</sup>, 8<sup>th</sup>, 9<sup>th</sup>, and many others were not sampled at all. Bootstrap sampling thus picks some observations multiple times and to do that it has to ignore some<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a> observations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury_BTS &lt;-<span class="st"> </span><span class="kw">resample</span>(MockJury2)
<span class="kw">table</span>(<span class="kw">as.numeric</span>(MockJury_BTS<span class="op">$</span>orig.id))</code></pre></div>
<pre><code>## 
##  1  2  3  4  5  6 10 11 12 14 15 17 18 19 20 22 24 26 29 30 32 35 36 37 39 
##  1  2  2  1  3  2  1  2  1  1  3  1  2  1  2  1  2  1  2  2  2  2  1  2  2 
## 40 42 43 44 45 46 47 48 49 55 58 59 60 61 69 70 71 72 74 75 
##  2  1  1  4  2  2  1  2  1  2  1  1  2  2  2  2  2  1  1  1</code></pre>
<p>Like in permutations, one randomization isn’t enough. A second bootstrap sample is also provided to help you get a sense of what bootstrap data sets contain. It did not select subject 7 but did select 6, 14, and 21 more than once. You can see other variations in the resulting re-sampling of subjects with the most sampled subjects 6 and 50 sampled four times. With <span class="math inline">\(n=75\)</span>, the the chance of selecting any observation for any slot in the new data set is <span class="math inline">\(1/75\)</span> and the expected or mean number of appearances we expect to see for an observation is the number of random draws times the probably of selection on each so <span class="math inline">\(75*1/75=1\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">MockJury_BTS2 &lt;-<span class="st"> </span><span class="kw">resample</span>(MockJury2)
<span class="kw">table</span>(<span class="kw">as.numeric</span>(MockJury_BTS2<span class="op">$</span>orig.id))</code></pre></div>
<pre><code>## 
##  1  2  3  5  6  8 11 12 13 14 15 18 19 20 21 23 24 26 27 28 29 31 32 34 36 
##  1  1  1  1  4  1  1  1  1  3  1  1  1  1  3  2  2  1  1  1  2  1  2  1  2 
## 37 38 40 42 46 48 50 51 52 56 58 59 61 62 63 66 67 68 69 72 73 74 75 
##  1  2  1  1  1  2  4  1  1  1  3  2  1  1  1  1  1  1  2  3  1  4  2</code></pre>
<p>We can use the two results to get an idea of distribution of results in terms of number of times observations might be re-sampled when sampling with replacement and the variation in those results, as shown in Figure <a href="chapter2.html#fig:Figure2-17">2.18</a>. We could also derive the expected counts for each number of times of re-sampling when we start with all observations having an equal chance and sampling with replacement but this isn’t important for using bootstrapping methods.</p>

<div class="figure"><span id="fig:Figure2-17"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-17-1.png" alt="Counts of number of times of observation (or not observed for times re-sampled of 0) for two bootstrap samples." width="480" />
<p class="caption">
Figure 2.18: Counts of number of times of observation (or not observed for times re-sampled of 0) for two bootstrap samples.
</p>
</div>
<p>The main point of this exploration was to see that each run of the <code>resample</code> function provides a new version of the data set. Repeating this <span class="math inline">\(B\)</span> times using another <code>for</code> loop, we will track our quantity of interest, say <span class="math inline">\(T\)</span>, in all these new “data sets” and call those results <span class="math inline">\(T^*\)</span>. The distribution of the bootstrapped <span class="math inline">\(T^*\)</span> statistics will tell us about the range of results to expect for the statistic and the middle __% of the <span class="math inline">\(T^*\)</span>’s provides a <strong><em>bootstrap confidence interval</em></strong><a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a> for the true parameter – here the <em>difference in the two population means</em>.</p>
<p>To make this concrete, we can revisit our previous examples, starting with the <code>MockJury2</code> data created before and our interest in comparing the mean sentences for the <em>Average</em> and <em>Unattractive</em> picture groups. The bootstrapping code is very similar to the permutation code except that we apply the <code>resample</code> function to the entire data set as opposed to the <code>shuffle</code> function that was applied only to the explanatory variable.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2); Tobs</code></pre></div>
<pre><code>## diffmean 
## 1.837127</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">diffmean</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span><span class="kw">resample</span>(MockJury2))
  }
<span class="kw">favstats</span>(Tstar)</code></pre></div>
<pre><code>##         min       Q1   median       Q3      max     mean        sd    n
##  -0.3627312 1.305773 1.833091 2.385281 4.988756 1.854428 0.8438987 1000
##  missing
##        0</code></pre>

<div class="figure"><span id="fig:Figure2-18"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-18-1.png" alt="Histogram and density curve of bootstrap distributions of difference in sample mean Years with vertical line for the observed difference in the means of 1.84 years." width="960" />
<p class="caption">
Figure 2.19: Histogram and density curve of bootstrap distributions of difference in sample mean <code>Years</code> with vertical line for the observed difference in the means of 1.84 years.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre></div>
<p>In this situation, the observed difference in the mean sentences is 1.84 years (<em>Unattractive</em> - <em>Average</em>), which is the vertical line in Figure <a href="chapter2.html#fig:Figure2-18">2.19</a>. The bootstrap distribution shows the results for the difference in the sample means when fake data sets are re-constructed by sampling from the original data set with replacement. The bootstrap distribution is approximately centered at the observed value (difference in the sample means) and is relatively symmetric.</p>
<p>The permutation distribution in the same situation (Figure <a href="chapter2.html#fig:Figure2-12">2.13</a>) had a similar shape but was centered at 0. Permutations create sampling distributions based on assuming the null hypothesis is true, which is useful for hypothesis testing. Bootstrapping creates distributions centered at the observed result, which is the sampling distribution “under the alternative” or when no null hypothesis is assumed; bootstrap distributions are useful for generating confidence intervals for the true parameter values.</p>
<p>To create a 95% bootstrap confidence interval for the difference in the true mean sentences (<span class="math inline">\(\mu_\text{Unattr}-\mu_\text{Avg}\)</span>), select the middle 95% of results from the bootstrap distribution. Specifically, find the 2.5<sup>th</sup> percentile and the 97.5<sup>th</sup> percentile (values that put 2.5 and 97.5% of the results to the left) in the bootstrap distribution, which leaves 95% in the middle for the confidence interval. To find percentiles in a distribution in R, functions are of the form <code>q[Name of distribution]</code>, with the function <code>qt</code> extracting percentiles from a <span class="math inline">\(t\)</span>-distribution (examples below). From the bootstrap results, use the <code>qdata</code> function on the <code>Tstar</code> results that contain the bootstrap distribution of the statistic of interest.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.025</span>)</code></pre></div>
<pre><code>##         p  quantile 
## 0.0250000 0.2414232</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.975</span>)</code></pre></div>
<pre><code>##        p quantile 
## 0.975000 3.521528</code></pre>
<p>These results tell us that the 2.5<sup>th</sup> percentile of the bootstrap distribution is at 0.24 years and the 97.5<sup>th</sup> percentile is at 3.52 years. We can combine these results to provide a 95% confidence for <span class="math inline">\(\mu_\text{Unattr}-\mu_\text{Avg}\)</span> that is between 0.24 and 3.52 years. We can interpret this as with any confidence interval, that we are 95% confident that the difference in the true mean suggested sentences (<em>Unattractive</em> minus <em>Average</em> groups) is between 0.24 and 3.52 years. We can also obtain both percentiles in one line of code using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.025</span>,<span class="fl">0.975</span>))
quantiles</code></pre></div>
<pre><code>##        quantile     p
## 2.5%  0.2414232 0.025
## 97.5% 3.5215278 0.975</code></pre>
<p>Figure <a href="chapter2.html#fig:Figure2-19">2.20</a> displays those same percentiles on the bootstrap distribution residing in <code>Tstar</code>.</p>

<div class="figure"><span id="fig:Figure2-19"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-19-1.png" alt="Histogram and density curve of bootstrap distribution with 95% bootstrap confidence intervals displayed (vertical lines)." width="960" />
<p class="caption">
Figure 2.20: Histogram and density curve of bootstrap distribution with 95% bootstrap confidence intervals displayed (vertical lines).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<p>Although confidence intervals can exist without referencing hypotheses, we can revisit our previous hypotheses and see what this confidence interval tells us about the test of <span class="math inline">\(H_0: \mu_\text{Unattr} = \mu_\text{Avg}\)</span>. This null hypothesis is equivalent to testing <span class="math inline">\(H_0: \mu_\text{Unattr} - \mu_\text{Avg} = 0\)</span>, that the difference in the true means is equal to 0 years. And the difference in the means was the scale for our confidence interval, which did not contain 0 years. We will call 0 an interesting <strong><em>reference value</em></strong> for the confidence interval, because here it is the value where the true means are equal to each other (have a difference of 0 years). In general, if our confidence interval does not contain 0, then it is saying that 0 is not one of our likely values for the difference in the true means. This implies that we should reject a claim that they are equal. This provides the same inferences for the hypotheses that we considered previously using both a parametric and permutation approach.</p>
<p>The general summary is that we can use confidence intervals to test hypotheses by assessing whether the reference value under the null hypothesis is in the confidence interval (FTR <span class="math inline">\(H_0\)</span>) or outside the confidence interval (Reject <span class="math inline">\(H_0\)</span>). P-values are more informative about hypotheses (measure of evidence against the null hypothesis) but confidence intervals are more informative about the size of differences, so both offer useful information and, as shown here, can provide consistent conclusions about hypotheses.</p>
<p>As in the previous situation, we also want to consider the parametric approach for comparison purposes and to have that method available, especially to help us understand some methods where we will only consider parametric inferences in later chapters. The parametric confidence interval is called the <strong><em>equal variance, two-sample t confidence interval</em></strong> and additionally assumes that the populations being sampled from are normally distributed. It leads to using a <span class="math inline">\(t\)</span>-distribution to form the interval. The output from the <code>t.test</code> function provides the parametric 95% confidence interval calculated for you:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2, <span class="dt">var.equal=</span>T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  Years by Attr
## t = -2.1702, df = 73, p-value = 0.03324
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -3.5242237 -0.1500295
## sample estimates:
##      mean in group Average mean in group Unattractive 
##                   3.973684                   5.810811</code></pre>
<p>The <code>t.test</code> function again switched the order of the groups and provides slightly different end-points than our bootstrap confidence interval (both are made at the 95% confidence level though), which was slightly narrower. Both intervals have the same interpretation, only the methods for calculating the intervals and the assumptions differ. Specifically, the bootstrap interval can tolerate different distribution shapes other than normal and still provide intervals that work well<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a>. The other assumptions are all the same as for the hypothesis test, where we continue to assume that we have independent observations with equal variances for the two groups.</p>
<p>The formula that <code>t.test</code> is using to calculate the parametric <strong><em>equal variance two-sample t confidence interval</em></strong> is:</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\]</span></p>
<p>In this situation, the <em>df</em> is again <span class="math inline">\(n_1+n_2-2\)</span> and <span class="math inline">\(s_p = \sqrt{\frac{(n_1-1)s_1^2 + (n_2-1)s_2^2}{n_1+n_2-2}}\)</span>. The <span class="math inline">\(t^*_{df}\)</span> is a multiplier that comes from finding the percentile from the <span class="math inline">\(t\)</span>-distribution that puts <span class="math inline">\(C\)</span>% in the middle of the distribution with <span class="math inline">\(C\)</span> being the confidence level. It is important to note that this <span class="math inline">\(t^*\)</span> has nothing to do with the previous test statistic <span class="math inline">\(t\)</span>. It is confusing and many of you will, at some point, happily take the result from a test statistic calculation and use it for a multiplier in a <span class="math inline">\(t\)</span>-based confidence interval. Figure <a href="chapter2.html#fig:Figure2-20">2.21</a> shows the <span class="math inline">\(t\)</span>-distribution with 73 degrees of freedom and the cut-offs that put 95% of the area in the middle.</p>

<div class="figure"><span id="fig:Figure2-20"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-20-1.png" alt="Plot of \(t(73)\) with cut-offs for putting 95% of distribution in the middle." width="480" />
<p class="caption">
Figure 2.21: Plot of <span class="math inline">\(t(73)\)</span> with cut-offs for putting 95% of distribution in the middle.
</p>
</div>
<p>For 95% confidence intervals, the multiplier is going to be close to 2 and anything else is a sign of a mistake. We can use R to get the multipliers for confidence intervals using the <code>qt</code> function in a similar fashion to how <code>qdata</code> was used in the bootstrap results, except that this new value must be used in the previous confidence interval formula. This function produces values for requested percentiles, so if we want to put 95% in the middle, we place 2.5% in each tail of the distribution and need to request the 97.5<sup>th</sup> percentile. Because the <span class="math inline">\(t\)</span>-distribution is always symmetric around 0, we merely need to look up the value for the 97.5<sup>th</sup> percentile and know that the multiplier for the 2.5<sup>th</sup> percentile is just <span class="math inline">\(-t^*\)</span>. The <span class="math inline">\(t^*\)</span> multiplier to form the confidence interval is 1.993 for a 95% confidence interval when the <span class="math inline">\(df=73\)</span> based on the results from <code>qt</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)</code></pre></div>
<pre><code>## [1] 1.992997</code></pre>
<p>Note that the 2.5<sup>th</sup> percentile is just the negative of this value due to symmetry and the real source of the minus in the minus/plus in the formula for the confidence interval.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.025</span>, <span class="dt">df=</span><span class="dv">73</span>)</code></pre></div>
<pre><code>## [1] -1.992997</code></pre>
<p>We can also re-write the confidence interval formula into a slightly more general form as</p>
<p><span class="math display">\[\bar{x}_1 - \bar{x}_2 \mp t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\ \text{ OR }\ 
\bar{x}_1 - \bar{x}_2 \mp ME\]</span></p>
<p>where <span class="math inline">\(SE_{\bar{x}_1 - \bar{x}_2} = s_p\sqrt{\frac{1}{n_1}+\frac{1}{n_2}}\)</span> and <span class="math inline">\(ME = t^*_{df}SE_{\bar{x}_1 - \bar{x}_2}\)</span>. In some situations, researchers will report the <strong><em>standard error</em></strong> (SE) or <strong><em>margin of error</em></strong> (ME) as a method of quantifying the uncertainty in a statistic. The SE is an estimate of the standard deviation of the statistic (here <span class="math inline">\(\bar{x}_1 - \bar{x}_2\)</span>) and the ME is an estimate of the precision of a statistic that can be used to directly form a confidence interval. The ME depends on the choice of confidence level although 95% is almost always selected.</p>
<p>To finish this example, R can be used to help you do calculations much like a calculator except with much more power “under the hood”. You have to make sure you are careful with using <code>( )</code> to group items and remember that the asterisk (*) is used for multiplication in R. We need the pertinent information which is available from the <code>favstats</code> output repeated below to calculate the confidence interval “by hand”<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a> using R.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(Years<span class="op">~</span>Attr, <span class="dt">data=</span>MockJury2)</code></pre></div>
<pre><code>##           Attr min Q1 median Q3 max     mean       sd  n missing
## 1      Average   1  2      3  5  12 3.973684 2.823519 38       0
## 2 Unattractive   1  2      5 10  15 5.810811 4.364235 37       0</code></pre>
<p>Start with typing the following command to calculate <span class="math inline">\(s_p\)</span> and store it in a variable named <code>sp</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">38</span><span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="fl">2.8235</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">37</span><span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="fl">4.364</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">38</span><span class="op">+</span><span class="dv">37</span><span class="op">-</span><span class="dv">2</span>))
sp</code></pre></div>
<pre><code>## [1] 3.665036</code></pre>
<p>Then calculate the confidence interval that <code>t.test</code> provided using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">3.974</span><span class="op">-</span><span class="fl">5.811</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">38</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">37</span>)</code></pre></div>
<pre><code>## [1] -3.5240302 -0.1499698</code></pre>
<p>The previous code uses <code>c(-1, 1)</code> times the margin of error to subtract and add the ME to the difference in the sample means (<span class="math inline">\(3.974-5.811\)</span>), which generates the lower and then upper bounds of the confidence interval. If desired, we can also use just the last portion of the previous calculation to find the margin of error, which is 1.69 here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">73</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">38</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">37</span>)</code></pre></div>
<pre><code>## [1] 1.68703</code></pre>
</div>
<div id="section2-9" class="section level2">
<h2><span class="header-section-number">2.9</span> Bootstrap confidence intervals for difference in GPAs</h2>
<p>We can now apply the new confidence interval methods on the STAT 217 grade data. This time we start with the parametric 95% confidence interval “by hand” in R and then use <code>t.test</code> to verify our result. The <code>favstats</code> output provides us with the required information to calculate the confidence interval:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217)</code></pre></div>
<pre><code>##   Sex  min  Q1 median   Q3 max     mean        sd  n missing
## 1   F 2.50 3.1  3.400 3.70   4 3.338378 0.4074549 37       0
## 2   M 1.96 2.8  3.175 3.46   4 3.088571 0.4151789 42       0</code></pre>
<p>The <span class="math inline">\(df\)</span> are <span class="math inline">\(37+42-2 = 77\)</span>. Using the SDs from the two groups and their sample sizes, we can calculate <span class="math inline">\(s_p\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">sp &lt;-<span class="st"> </span><span class="kw">sqrt</span>(((<span class="dv">37</span><span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="fl">0.4075</span><span class="op">^</span><span class="dv">2</span>)<span class="op">+</span>(<span class="dv">42</span><span class="op">-</span><span class="dv">1</span>)<span class="op">*</span>(<span class="fl">0.41518</span><span class="op">^</span><span class="dv">2</span>))<span class="op">/</span>(<span class="dv">37</span><span class="op">+</span><span class="dv">42</span><span class="op">-</span><span class="dv">2</span>))
sp</code></pre></div>
<pre><code>## [1] 0.4116072</code></pre>
<p>The margin of error is:</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</code></pre></div>
<pre><code>## [1] 0.1847982</code></pre>
<p>All together, the 95% confidence interval is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">3.338</span><span class="op">-</span><span class="fl">3.0886</span><span class="op">+</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">77</span>)<span class="op">*</span>sp<span class="op">*</span><span class="kw">sqrt</span>(<span class="dv">1</span><span class="op">/</span><span class="dv">37</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">42</span>)</code></pre></div>
<pre><code>## [1] 0.0646018 0.4341982</code></pre>
<p>So we are 95% confident that the difference in the true mean GPAs between females and males (females minus males) is between 0.065 and 0.434 GPA points. We get a similar<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a> result from the <code>t.test</code> output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex,<span class="dt">data=</span>s217,<span class="dt">var.equal=</span>T)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  0.06501838 0.43459552
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>Note that we can easily switch to 90% or 99% confidence intervals by simply changing the percentile in <code>qt</code> or changing the <code>conf.level</code> option in the <code>t.test</code> function. In the following two lines of code, we added octothorpes<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a>(#) and then some text after function calls to explain what is being calculated. In computer code, octothorpes provide a way of adding comments that tell the software (here R) to ignore any text after a “#” on a given line. In the color version of the text, comments are also clearly distinguished.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.95</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co"># For 90% confidence and 77 df</span></code></pre></div>
<pre><code>## [1] 1.664885</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">77</span>) <span class="co">#For 99% confidence and 77 df</span></code></pre></div>
<pre><code>## [1] 2.641198</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T, <span class="dt">conf.level=</span><span class="fl">0.90</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 90 percent confidence interval:
##  0.09530553 0.40430837
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">t.test</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217, <span class="dt">var.equal=</span>T, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  GPA by Sex
## t = 2.6919, df = 77, p-value = 0.008713
## alternative hypothesis: true difference in means is not equal to 0
## 99 percent confidence interval:
##  0.004703598 0.494910301
## sample estimates:
## mean in group F mean in group M 
##        3.338378        3.088571</code></pre>
<p>As a review of some basic ideas with confidence intervals make sure you can answer the following questions:</p>
<ol style="list-style-type: decimal">
<li><p>What is the impact of increasing the confidence level in this situation?</p></li>
<li><p>What happens to the width of the confidence interval if the size of the SE increases or decreases?</p></li>
<li><p>What about increasing the sample size – should that increase or decrease the width of the interval?</p></li>
</ol>
<p>All the general results you learned before about impacts to widths of CIs hold in this situation whether we are considering the parametric or bootstrap methods…</p>
<p>To finish this example, we will generate the comparable bootstrap 90% confidence interval using the bootstrap distribution in Figure <a href="chapter2.html#fig:Figure2-21">2.22</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">diffmean</span>(GPA<span class="op">~</span>Sex, <span class="dt">data=</span>s217); Tobs</code></pre></div>
<pre><code>##   diffmean 
## -0.2498069</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b]&lt;-<span class="kw">diffmean</span>(GPA <span class="op">~</span><span class="st"> </span>Sex, <span class="dt">data=</span><span class="kw">resample</span>(s217))
  }
<span class="kw">qdata</span>(Tstar, <span class="fl">0.05</span>)</code></pre></div>
<pre><code>##          p   quantile 
##  0.0500000 -0.4032273</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qdata</span>(Tstar, <span class="fl">0.95</span>)</code></pre></div>
<pre><code>##           p    quantile 
##  0.95000000 -0.09521925</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">quantiles &lt;-<span class="st"> </span><span class="kw">qdata</span>(Tstar, <span class="kw">c</span>(<span class="fl">0.05</span>, <span class="fl">0.95</span>))
quantiles</code></pre></div>
<pre><code>##        quantile    p
## 5%  -0.40322729 0.05
## 95% -0.09521925 0.95</code></pre>
<p>The output tells us that the 90% confidence interval is from -0.393 to -0.094 GPA points. The bootstrap distribution with the observed difference in the sample means and these cut-offs is displayed in Figure <a href="chapter2.html#fig:Figure2-21">2.22</a> using this code:</p>

<div class="figure"><span id="fig:Figure2-21"></span>
<img src="02-reintroductionToStatistics_files/figure-html/Figure2-21-1.png" alt="Histogram and density curve of bootstrap distribution of difference in sample mean GPAs (male minus female) with observed difference (solid vertical line) and quantiles that delineate the 90% confidence intervals (dashed vertical lines)." width="960" />
<p class="caption">
Figure 2.22: Histogram and density curve of bootstrap distribution of difference in sample mean GPAs (male minus female) with observed difference (solid vertical line) and quantiles that delineate the 90% confidence intervals (dashed vertical lines).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(Tstar,<span class="dt">labels=</span>T)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar),<span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>quantiles<span class="op">$</span>quantile,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre></div>
<p>In the previous output, the parametric 90% confidence interval is from 0.095 to 0.404, suggesting similar results again from the two approaches once you account for the two different orders of differencing of the groups. Based on the bootstrap CI, we can say that we are 90% confident that the difference in the true mean GPAs for STAT 217 students is between -0.393 to -0.094 GPA points (male minus females). Because sex cannot be assigned to the subjects, we cannot infer that sex is causing this difference and because this was a voluntary response sample of STAT 217 students in a given semester, we cannot infer that a difference of this size would apply to all STAT 217 students or, certainly, students this semester.</p>
<p>Throughout the text, pay attention to the distinctions between parameters and statistics, focusing on the differences between estimates based on the sample and inferences for the population of interest in the form of the parameters of interest. Remember that statistics are summaries of the sample information and parameters are characteristics of populations (which we rarely know). And that our inferences are limited to the population that we randomly sampled from, if we randomly sampled.</p>
</div>
<div id="section2-10" class="section level2">
<h2><span class="header-section-number">2.10</span> Chapter summary</h2>
<p>In this chapter, we reviewed basic statistical inference methods in the context of a two-sample mean problem. You were introduced to using R to do permutation testing and generate bootstrap confidence intervals as well as obtaining parametric <span class="math inline">\(t\)</span>-test and confidence intervals in this same situation. You should have learned how to use a <code>for</code> loop for doing the nonparametric inferences and the <code>t.test</code> function for generating parametric inferences. In the two examples considered, the parametric and nonparametric methods provided similar results, suggesting that the assumptions were at least close to being met for the parametric procedures. When parametric and nonparametric approaches disagree, the nonparametric methods are likely to be more trustworthy since they have less restrictive assumptions but can still have problems.</p>
<p>When the noted conditions are not met in a hypothesis testing situation, the Type I error rates can be inflated, meaning that we reject the null hypothesis more often than we have allowed to occur by chance. Specifically, we could have a situation where our assumed 5% significance level test might actually reject the null when it is true 20% of the time. If this is occurring, we call a procedure <strong><em>liberal</em></strong> (it rejects too easily) and if the procedure is liberal, how could we trust a small p-value to be a “real” result and not just an artifact of violating the assumptions of the procedure? Likewise, for confidence intervals we hope that our 95% confidence level procedure, when repeated, will contain the true parameter 95% of the time. If our assumptions are violated, we might actually have an 80% confidence level procedure and it makes it hard to trust the reported results for our observed data set. Statistical inference relies on a belief in the methods underlying our inferences. If we don’t trust our assumptions, we shouldn’t trust the conclusions to perform the way we want them to. As sample sizes increase and/or violations of conditions lessen, then the procedures will perform better. In Chapter <a href="chapter3.html#chapter3">3</a>, some new tools for doing diagnostics are introduced to help us assess how much those validity conditions are violated.</p>
</div>
<div id="section2-11" class="section level2">
<h2><span class="header-section-number">2.11</span> Summary of important R code</h2>
<p>The main components of R code used in this chapter follow with components to modify in lighter and/or ALL CAPS text, remembering that any R packages mentioned need to be installed and loaded for this code to have a chance of working:</p>
<ul>
<li><p><strong>summary(<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of all variables in the data set.</li>
</ul></li>
<li><p><strong>t.test(<font color='red'>Y</font> ~ <font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, conf.level=0.95)</strong></p>
<ul>
<li>Provides two-sample t-test test statistic, df, p-value, and 95% confidence interval.</li>
</ul></li>
<li><p><strong>2<code>*</code>pt(abs(<font color='red'>Tobs</font>), df=<font color='red'>DF</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the two-sided test p-value for an observed 2-sample t-test statistic of <code>Tobs</code>.</li>
</ul></li>
<li><p><strong>hist(<font color='red'>DATASETNAME$Y</font>)</strong></p>
<ul>
<li>Makes a histogram of a variable named <code>Y</code> from the data set of interest.</li>
</ul></li>
<li><p><strong>boxplot(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Makes a boxplot of a variable named Y for groups in X from the data set.</li>
</ul></li>
<li><p><strong>beanplot(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, log=“”, col=“lightblue”, method=“jitter”)</strong></p>
<ul>
<li><p>Requires the <code>beanplot</code> package is loaded.</p></li>
<li><p>Makes a beanplot of a variable named Y for groups in X from the data set.</p></li>
</ul></li>
<li><p><strong>mean(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>); sd(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>This usage of <code>mean</code> and <code>sd</code> requires the <code>mosaic</code> package.</p></li>
<li><p>Provides the mean and sd of responses of Y for each group described in X.</p></li>
</ul></li>
<li><p><strong>favstats(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Provides numerical summaries of Y by groups described in X.</li>
</ul></li>
<li><p><strong>Tobs <code>&lt;-</code> t.test(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
Tstar[b] <code>&lt;-</code> t.test(<font color='red'>Y</font>~shuffle(<font color='red'>X</font>), data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 permuted versions of the test statistic using the <code>shuffle</code> function and keep track of the results in <code>Tstar</code></li>
</ul></li>
<li><p><strong>pdata(Tstar, abs(<font color='red'>Tobs</font>, lower.tail=F)</strong></p>
<ul>
<li>Finds the proportion of the permuted test statistics in Tstar that are less than -|Tobs| or greater than |Tobs|, useful for finding the two-sided test p-value.</li>
</ul></li>
<li><p><strong>Tobs <code>&lt;-</code> diffmean(<font color='red'>Y</font>~<font color='red'>X</font>, data=<font color='red'>DATASETNAME</font>, var.equal=T)$statistic; Tobs<br />
B <code>&lt;-</code> 1000<br />
Tstar <code>&lt;-</code> matrix(NA, nrow=B)<br />
for (b in (1:B)){<br />
Tstar[b] <code>&lt;-</code> diffmean(<font color='red'>Y</font>~<font color='red'>X</font>, data=resample(<font color='red'>DATASETNAME</font>))<br />
}</strong></p>
<ul>
<li>Code to run a <code>for</code> loop to generate 1000 bootstrapped versions of the data set using the <code>resample</code> function and keep track of the results of the statistic in <code>Tstar</code>.</li>
</ul></li>
<li><p><strong>qdata(Tstar, c(0.025, 0.975))</strong></p>
<ul>
<li>Provides the values that delineate the middle 95% of the results in the bootstrap distribution (<code>Tstar</code>).</li>
</ul></li>
</ul>

</div>
<div id="section2-12" class="section level2">
<h2><span class="header-section-number">2.12</span> Practice problems</h2>
<p>Load the <code>HELPrct</code> data set from the <code>mosaicData</code> package <span class="citation">(Pruim, Kaplan, and Horton <a href="#ref-R-mosaicData">2016</a>)</span> (you need to install the <code>mosaicData</code> package once to be able to load it). The HELP study was a clinical trial for adult inpatients recruited from a detoxification unit. Patients with no primary care physician were randomly assigned to receive a multidisciplinary assessment and a brief motivational intervention or usual care and various outcomes were observed. Two of the variables in the data set are <code>sex</code>, a factor with levels <em>male</em> and <em>female</em> and <code>daysanysub</code> which is the time (in days) to first use of any substance post-detox. We are interested in the difference in mean number of days to first use of any substance post-detox between males and females. There are some missing responses and the following code will produce <code>favstats</code> with the missing values and then provide a data set that by applying the <code>na.omit</code> function removes any observations with missing values.</p>
<pre><code>require(mosaicData)
data(HELPrct)
HELPrct2 &lt;- HELPrct[, c(&quot;daysanysub&quot;, &quot;sex&quot;)] #Just focus on two variables
HELPrct3 &lt;- na.omit(HELPrct2) #Removes subjects with missing values
favstats(daysanysub~sex, data=HELPrct2)
favstats(daysanysub~sex, data=HELPrct3)</code></pre>
<p>2.1. Based on the results provided, how many observations were missing for males and females? Missing values here likely mean that the subjects didn’t use any substances post-detox in the time of the study but might have at a later date – the study just didn’t run long enough. This is called <strong><em>censoring</em></strong>. What is the problem with the numerical summaries here if the missing responses were all something larger than the largest observation?</p>
<p>2.2. Make a beanplot and a boxplot of <code>daysanysub</code> ~ <code>sex</code> using the <code>HELPrct3</code> data set created above. Compare the distributions, recommending parametric or nonparametric inferences.</p>
<p>2.3. Generate the permutation results and write out the 6+ steps of the hypothesis test, making sure to note the numerical value of observed test statistic you are using and include a discussion of the scope of inference.</p>
<p>2.4. Interpret the p-value for these results.</p>
<p>2.5. Generate the parametric <code>t.test</code> results, reporting the test-statistic, its distribution under the null hypothesis, and compare the p-value to those observed using the permutation approach.</p>
<p>2.6. Make and interpret a 95% bootstrap confidence interval for the difference in the means.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Plaster1989">
<p>Plaster, M. E. 1989. “Inmates as Mock Jurors: The Effects of Physical Attractiveness Upon Juridic Decisions.” Master’s thesis, Greenville, NC: East Carolina University.</p>
</div>
<div id="ref-R-heplots">
<p>Fox, John, and Michael Friendly. 2017. <em>Heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</em>. <a href="https://CRAN.R-project.org/package=heplots" class="uri">https://CRAN.R-project.org/package=heplots</a>.</p>
</div>
<div id="ref-R-mosaic">
<p>Pruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2017. <em>Mosaic: Project Mosaic Statistics and Mathematics Teaching Utilities</em>. <a href="https://CRAN.R-project.org/package=mosaic" class="uri">https://CRAN.R-project.org/package=mosaic</a>.</p>
</div>
<div id="ref-Kampstra2008">
<p>Kampstra, Peter. 2008. “Beanplot: A Boxplot Alternative for Visual Comparison of Distributions.” <em>Journal of Statistical Software, Code Snippets</em> 28 (1): 1–9. <a href="http://www.jstatsoft.org/v28/c01/" class="uri">http://www.jstatsoft.org/v28/c01/</a>.</p>
</div>
<div id="ref-R-beanplot">
<p>Kampstra, Peter. 2014. <em>Beanplot: Visualization via Beanplots (Like Boxplot/Stripchart/Violin Plot)</em>. <a href="https://CRAN.R-project.org/package=beanplot" class="uri">https://CRAN.R-project.org/package=beanplot</a>.</p>
</div>
<div id="ref-R-mosaicData">
<p>Pruim, Randall, Daniel Kaplan, and Nicholas Horton. 2016. <em>MosaicData: Project Mosaic Data Sets</em>. <a href="https://CRAN.R-project.org/package=mosaicData" class="uri">https://CRAN.R-project.org/package=mosaicData</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="11">
<li id="fn11"><p>You will more typically hear “data is” but that more often refers to information, sometimes even statistical summaries of data sets, than to observations collected as part of a study, suggesting the confusion of this term in the general public. We will explore a data set in Chapter <a href="chapter5.html#chapter5">5</a> related to perceptions of this issue collected by researchers at <a href="http://fivethirtyeight.com/" class="uri">http://fivethirtyeight.com/</a>.<a href="chapter2.html#fnref11">↩</a></p></li>
<li id="fn12"><p>Either try to remember “data is a plural word” or replace “data” with “things” in your sentence and consider whether it sounds right.<a href="chapter2.html#fnref12">↩</a></p></li>
<li id="fn13"><p>As noted previously, we reserve the term “effect” for situations where random assignment allows us to consider causality as the reason for the differences in the response variable among levels of the explanatory variable, but this is only the case if we find evidence against the null hypothesis of no difference in the groups.<a href="chapter2.html#fnref13">↩</a></p></li>
<li id="fn14"><p>If you’ve taken calculus, you will know that the curve is being constructed so that the integral from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> is 1. If you don’t know calculus, think of a rectangle with area of 1 based on its height and width. These cover the same area but the top of the region wiggles.<a href="chapter2.html#fnref14">↩</a></p></li>
<li id="fn15"><p>Jittering typically involves adding random variability to each observation that is uniformly distributed in a range determined based on the spacing of the observation. This means that if you re-run the <code>jitter</code> function, the results will change. For more details, type <code>help(jitter)</code> in the console in RStudio.<a href="chapter2.html#fnref15">↩</a></p></li>
<li id="fn16"><p>Remember the bell-shaped curve you encountered in introductory statistics? If not, you can see some at <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a><a href="chapter2.html#fnref16">↩</a></p></li>
<li id="fn17"><p>Well, you can use other colors (try “lightblue” for example), but I think bisque looks nice in these plots.<a href="chapter2.html#fnref17">↩</a></p></li>
<li id="fn18"><p>It is also possible to use the <code>subset</code> function to eliminate observations that do not have a particular trait and <code>subset(MockJury,subset=NotBeautiful)</code> would provide the same result as <code>MockJury[MockJury$NotBeautiful,]</code> did here.<a href="chapter2.html#fnref18">↩</a></p></li>
<li id="fn19"><p>The hypothesis of no difference that is typically generated in the hopes of being rejected in favor of the alternative hypothesis, which contains the sort of difference that is of interest in the application.<a href="chapter2.html#fnref19">↩</a></p></li>
<li id="fn20"><p>The null model is the statistical model that is implied by the chosen null hypothesis. Here, a null hypothesis of no difference translates to having a model with the same mean for both groups.<a href="chapter2.html#fnref20">↩</a></p></li>
<li id="fn21"><p>We’ll see the <code>shuffle</code> function in a more common usage below; while the code to generate <code>Perm1</code> is provided, it isn’t something to worry about right now.<a href="chapter2.html#fnref21">↩</a></p></li>
<li id="fn22"><p>P-values are the probability of obtaining a result as extreme as or more extreme than we observed given that the null hypothesis is true.<a href="chapter2.html#fnref22">↩</a></p></li>
<li id="fn23"><p>In statistics, vectors are one dimensional lists of numeric elements – basically a column from a matrix or our tibble.<a href="chapter2.html#fnref23">↩</a></p></li>
<li id="fn24"><p>We often say “under” in statistics and we mean “given that the following is true”.<a href="chapter2.html#fnref24">↩</a></p></li>
<li id="fn25"><p>This is a fancy way of saying “in advance”, here in advance of seeing the observations.<a href="chapter2.html#fnref25">↩</a></p></li>
<li id="fn26"><p>Statistically, a conservative method is one that provides less chance of rejecting the null hypothesis in comparison to some other method or less than some pre-defined standard.<a href="chapter2.html#fnref26">↩</a></p></li>
<li id="fn27"><p>We’ll leave the discussion of the CLT to your previous statistics coursework or an internet search. For this material, just remember that it has something to do with distributions looking more normal as the sample size increases.<a href="chapter2.html#fnref27">↩</a></p></li>
<li id="fn28"><p>On exams, you will be asked to describe the area of interest, sketch a picture of the area of interest, and/or note the distribution you would use.<a href="chapter2.html#fnref28">↩</a></p></li>
<li id="fn29"><p>In some studies, the same subject might be measured in both conditions and this violates the assumptions of this procedure.<a href="chapter2.html#fnref29">↩</a></p></li>
<li id="fn30"><p>The <code>as.numeric</code> function is also used here. It really isn’t important but makes sure the output of <code>table</code> is sorted by observation number by first converting the <em>orig.id</em> variable into a numeric vector.<a href="chapter2.html#fnref30">↩</a></p></li>
<li id="fn31"><p>In any bootstrap sample, about 1/3 of the observations are not used at all.<a href="chapter2.html#fnref31">↩</a></p></li>
<li id="fn32"><p>There are actually many ways to use this information to make a confidence interval. We are using the simplest method that is called the “percentile” method.<a href="chapter2.html#fnref32">↩</a></p></li>
<li id="fn33"><p>When hypothesis tests “work well” they have high power to detect differences while having Type I error rates that are close to what we choose <em>a priori</em>. When confidence intervals “work well”, they contain the true parameter value in repeated random samples at around the selected confidence level.<a href="chapter2.html#fnref33">↩</a></p></li>
<li id="fn34"><p>We will often use this term to mean from the component summary information – not that you need to go back to the data set and calculate the means and standard deviations.<a href="chapter2.html#fnref34">↩</a></p></li>
<li id="fn35"><p>We rounded the means a little and that caused the small difference in results.<a href="chapter2.html#fnref35">↩</a></p></li>
<li id="fn36"><p>You can correctly call octothorpes <em>number</em> symbols or, in the twitter verse, <em>hashtags</em>. For more on this symbol, see “<a href="http://blog.dictionary.com/octothorpe/" class="uri">http://blog.dictionary.com/octothorpe/</a>”. Even after reading this, I call them number symbols.<a href="chapter2.html#fnref36">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf", "Greenwood_Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
