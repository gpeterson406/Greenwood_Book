<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="A Second Semester Statistics Course with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A Second Semester Statistics Course with R">

<title>A Second Semester Statistics Course with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Beanplots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Chapter summary</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Summary of important R code</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for Prisoner Rating data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and table plots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient (Optional section)</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomizing inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> What do didgeridoos really do about sleepiness?</a></li>
<li><a href="9-6-section9-6.html#section9-6"><span class="toc-section-number">9.6</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section7-2" class="section level2">
<h2><span class="header-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</h2>

<p>Our inference techniques will resemble previous material with an interest in forming confidence intervals and doing hypothesis testing, although the interpretation of confidence intervals for slope coefficients take some extra care. Remember that the general form of any parametric confidence interval is</p>
<p><span class="math display">\[\text{estimate} \mp t^*\text{SE}_{estimate},\]</span></p>
<p>so we need to obtain the appropriate standard error for regression model coefficients and the degrees of freedom to define the <span class="math inline">\(t\)</span>-distribution to look up <span class="math inline">\(t^*\)</span>. We will find the <span class="math inline">\(\text{SE}_{b_0}\)</span> and <span class="math inline">\(\text{SE}_{b_1}\)</span> in the model summary. The degrees of freedom for the <span class="math inline">\(t\)</span>-distribution in simple linear regression are <span class="math inline">\(\mathbf{df=n-2}\)</span>. Putting this together, the confidence interval for the true y-intercept, <span class="math inline">\(\beta_0\)</span>, is <span class="math inline">\(\mathbf{b_0 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_0}}\)</span> although this confidence interval is rarely of interest. The confidence interval that is almost always of interest is for the true slope coefficient, <span class="math inline">\(\beta_1\)</span>, that is <span class="math inline">\(\mathbf{b_1 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_1}}\)</span>. The slope confidence interval is used to do two things: (1) inference for the amount of change in the mean of <span class="math inline">\(y\)</span> for a unit change in <span class="math inline">\(x\)</span> in the population and (2) to potentially do hypothesis testing by checking whether 0 is in the CI or not. The sketch in Figure <a href="7-2-section7-2.html#fig:Figure7-3b">2.119</a> illustrates the roles of the CI for the slope in terms of determining where the population slope coefficient might be – centered at the sample slope coefficient – our best guess for the true slope. This sketch informs an <strong><em>interpretation of the slope coefficient confidence interval</em></strong>:</p>

<div class="figure"><span id="fig:Figure7-3b"></span>
<img src="chapter7_files/image045.png" alt="Graphic illustrating the confidence interval for a slope coefficient for a 1 unit increase in \(x\)." width="210" />
<p class="caption">
Figure 2.119: Graphic illustrating the confidence interval for a slope coefficient for a 1 unit increase in <span class="math inline">\(x\)</span>.
</p>
</div>
<blockquote>
<p>For a 1 <strong>[<em>units of X</em>]</strong> increase in <strong>X</strong>, we are ___ % confident that the <strong>true change in the mean of</strong> <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong> <strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p>In this interpretation, LL and UL are the calculated lower and upper limits of the confidence interval. This builds on our previous interpretation of the slope coefficient, adding in the information about pinning down the true change (population change) in the mean of the response variable. The interpretation of the y-intercept CI is:</p>
<blockquote>
<p>For an <strong><em>x</em></strong> of 0 <strong>[<em>units of X</em>]</strong>, we are 95% confident that the true mean of <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong> <strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p>This is really only interesting if the value of <span class="math inline">\(x=0\)</span> is interesting – we’ll see a method for generating CIs for the true mean at potentially more interesting values of <span class="math inline">\(x\)</span> in Section <a href="7-7-section7-7.html#section7-7">7.7</a>. To trust the results from these confidence intervals, all the regression validity conditions need to be met (or at least close to met).</p>
<p>The only hypothesis test of interest in this situation is for the slope coefficient. To develop the hypotheses of interest in SLR, note the effect of having <span class="math inline">\(\beta_1=0\)</span> in the mean of the regression equation, <span class="math inline">\(\mu_{y_i} = \beta_0 + \beta_1x_i = \beta_0 + 0x_i = \beta_0\)</span>. This is the “intercept-only” or “mean-only” model that suggests that the mean of <span class="math inline">\(y\)</span> does not vary with different values of <span class="math inline">\(x\)</span> as it is always <span class="math inline">\(\beta_0\)</span>. We saw this model in the ANOVA material as the reduced model when the null hypothesis of no difference in the true means across the groups was true. Here, this is the same as saying that there is no linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, or that <span class="math inline">\(x\)</span> is of no use in predicting <span class="math inline">\(y\)</span>, or that we make the same prediction for <span class="math inline">\(y\)</span> for every value of <span class="math inline">\(x\)</span>. Thus</p>
<p><span class="math display">\[\boldsymbol{H_0: \beta_1=0}\]</span></p>
<p>is a test for <strong>no linear relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>in the population</strong>. The alternative of <span class="math inline">\(\boldsymbol{H_A: \beta_1\ne 0}\)</span>, that there is <strong>some</strong> linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the population, is our main test of interest in these situations. It is also possible to test greater than or less than alternatives in certain situations.</p>
<p>Test statistics for regression coefficients are developed, if assumptions are met, using the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom. The <span class="math inline">\(t\)</span>-test statistic is generally</p>
<p><span class="math display">\[t=\frac{b_i}{\text{SE}_{b_i}}\]</span></p>
<p>with the main interest in the test for <span class="math inline">\(\beta_1\)</span> based on <span class="math inline">\(b_1\)</span> initially. The p-value would be calculated using the two-tailed area from the <span class="math inline">\(t_{n-2}\)</span> distribution calculated using the <code>pt</code> function. The p-value to test these hypotheses is also provided in the model summary as we will see below.</p>
<p>The greater than or less than alternatives can have interesting interpretations in certain situations. For example, the greater than alternative <span class="math inline">\(\left(\boldsymbol{H_A: \beta_1 &gt; 0}\right)\)</span> tests an alternative of a positive linear relationship, with the p-value extracted just from the right tail of the same <span class="math inline">\(t\)</span>-distribution. This could be used when a researcher would only find a result “interesting” if a positive relationship is detected, such as in the study of tree height and tree diameter where a researcher might be justified in deciding to test only for a positive linear relationship. Similarly, the left-tailed alternative is also possible, <span class="math inline">\(\boldsymbol{H_A: \beta_1 &lt; 0}\)</span>. To get one-tailed p-values from two-tailed results (the default), first check that the observed test statistic is in the direction of the alternative (<span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span> for <span class="math inline">\(H_A:\beta_1&lt;0\)</span>). <strong>If these conditions are met, then the p-value for the one-sided test from the two-sided version is found by dividing the reported p-value by 2</strong>. If <span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span> for <span class="math inline">\(H_A:\beta_1&lt;0\)</span> are not met, then the p-value would be greater than 0.5 and it would be easiest to look it up directly using <code>pt</code> in the direction of the alternative.</p>
<p>We can revisit a couple of examples for a last time with these ideas in hand to complete the analyses.</p>
<blockquote>
<p>For the <em>Beers, BAC</em> data, the 95% confidence for the true slope coefficient, <span class="math inline">\(\beta_1\)</span>, is</p>
</blockquote>
<p><span class="math display">\[\begin{array}{rl}
\boldsymbol{b_1 \mp t^*_{n-2}} \textbf{SE}_{\boldsymbol{b_1}}
&amp; \boldsymbol{= 0.01796 \mp 2.144787 * 0.002402} \\
&amp; \boldsymbol{= 0.01796 \mp 0.00515} \\
&amp; \boldsymbol{\rightarrow (0.0128, 0.0231).}
\end{array}\]</span></p>
<p>You can find the components of this calculation in the model summary and from <code>qt(0.975, df=n-2)</code> which was 2.145 for the <span class="math inline">\(t^*\)</span>-multiplier. Be careful not to use the <span class="math inline">\(t\)</span>-value of 7.48 in the model summary to make confidence intervals – that is the test statistic used below. The related calculations are shown at the bottom of the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(BAC<span class="op">~</span>Beers, <span class="dt">data=</span>BB)
<span class="kw">summary</span>(m1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>) <span class="co"># t* multiplier for 95% CI</span></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="fl">0.017964</span> <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="op">*</span><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></code></pre></div>
<pre><code>## [1] 0.01281222 0.02311578</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.975</span>, <span class="dt">df=</span><span class="dv">14</span>)<span class="op">*</span><span class="fl">0.002402</span></code></pre></div>
<pre><code>## [1] 0.005151778</code></pre>
<p>We can also get the confidence interval directly from the <code>confint</code> function run on our regression model, saving some calculation effort and providing both the CI for the y-intercept and the slope coefficient.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m1)</code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.03980535 0.01440414
## Beers        0.01281262 0.02311490</code></pre>
<p>We interpret the 95% CI for the slope coefficient as follows: For a 1 <strong>beer</strong> increase in number of beers consumed, we are 95% confident that the <strong>true</strong> change in the <strong>mean</strong> <em>BAC</em> will be between 0.0128 and 0.0231 g/dL. While the estimated slope is our best guess of the impacts of an extra beer consumed based on our sample, this CI provides information about the likely range of potential impacts on the mean in the population. It also could be used to test the two-sided hypothesis test and would suggest that we should reject the null hypothesis since the confidence interval does not contain 0.</p>
<p>The width of the CI, loosely the precision of the estimated slope, is impacted by the variability of the observations around the estimated regression line, the overall sample size, and the positioning of the x-observations. Basically all those aspects relate to how “clearly” known the regression line is and that determines the estimated precision in the slope. For example, the more variability around the line that is present, the more uncertainty there is about the correct line to use (Least Squares (LS) can still find an estimated line but there are other lines that might be “close” to its optimizing choice). Similarly, more observations help us get a better estimate of the mean – an idea that permeates all statistical methods. Finally, the location of x-values can impact the precision in a slope coefficient. We’ll revisit this in the context of <strong><em>multi-collinearity</em></strong> in the next chapter, and often we have no control of x-values, but just note that different patterns of x-values can lead to different precision of estimated slope coefficients<a href="#fn80" class="footnoteRef" id="fnref80"><sup>80</sup></a>.</p>
<p>For hypothesis testing, we will almost always stick with two-sided tests in regression modeling as it is a more conservative approach and does not require us to have an expectation of a direction for relationships <em>a priori</em>. In this example, the null hypothesis for the slope coefficient is that there is no linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The alternative hypothesis is that there is some linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The test statistic is <span class="math inline">\(t=0.01796/0.002402 =7.48\)</span> which, if model assumptions hold, follows a <span class="math inline">\(t(14)\)</span> distribution under the null hypothesis. The model summary provides the calculation of the test statistic and the two-sided test p-value of <span class="math inline">\(2.97\text{e-6} = 0.00000297\)</span>. So we would just report “p-value &lt; 0.0001”. This suggests that there is very strong evidence of a linear relationship between <em>Beers</em> and <em>BAC</em> in the population. Because of the random assignment, we can also say that drinking beers causes changes in BAC but, because the sample was made up of volunteers, we cannot infer that these results would hold in the general population of OSU students or more generally.</p>
<p>There are also results for the y-intercept in the output. The 95% CI is from -0.0398 to 0.0144, that the true mean <em>BAC</em> for a 0 beer consuming subject is between -0.0398 to 0.01445. This is really not a big surprise but possibly is comforting to know that these results would fail to reject the null hypothesis that the true mean <em>BAC</em> for 0 <em>Beers</em> is 0. Finding no evidence of a difference from 0 makes sense and makes the estimated y-intercept of -0.013 not so problematic. In other situations, the results for the y-intercept may be more illogical but this will often be because the y-intercept is extrapolating far beyond the scope of observations. The y-intercept’s main function in regression models is to be at the right level for the slope to “work” to make a line that describes the responses and thus is usually of lesser interest.</p>
<p>As a second example, we can revisit modeling the <em>Hematocrit</em> of female Australian athletes as a function of <em>body fat %</em>. The sample size is <span class="math inline">\(n=99\)</span> so the <em>df</em> are 97 in the analysis. In Chapter <a href="6-chapter6.html#chapter6"><strong>??</strong></a>, the relationship between <em>Hematocrit</em> and <em>body fat %</em> for females appeared to be a weak negative linear association. The 95% confidence interval for the slope is -0.187 to 0.0155. For a 1% increase in body fat %, we are 95% confident that the change in the true mean Hematocrit is between -0.187 and 0.0155% of blood. This suggests that we would fail to reject the null hypothesis of no linear relationship at the 5% significance level because this CI contains 0 – we can’t reject the null that the true slope is 0. In fact the p-value is 0.0965 which is larger than 0.05 which provides a consistent conclusion with using the 95% confidence interval to perform a hypothesis test. Either way, we would conclude that there is little evidence to conclude that there is some linear relationship between body fat and Hematocrit in the population of female Australian athletes. If your standards were different, say if you think p-values around 0.10 provide moderate evidence, you might have a different opinion about the evidence against the null hypothesis here. For this reason, we sometimes interpret this sort of marginal result as having some or marginal evidence against the null but certainly would never say that this presents strong evidence.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(alr3)
<span class="kw">data</span>(ais)
<span class="kw">require</span>(tibble)
ais &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(ais)
aisR2 &lt;-<span class="st"> </span>ais[<span class="op">-</span><span class="kw">c</span>(<span class="dv">56</span>,<span class="dv">166</span>), <span class="kw">c</span>(<span class="st">&quot;Ht&quot;</span>,<span class="st">&quot;Hc&quot;</span>,<span class="st">&quot;Bfat&quot;</span>,<span class="st">&quot;Sex&quot;</span>)]
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Hc<span class="op">~</span>Bfat, <span class="dt">data=</span>aisR2[aisR2<span class="op">$</span>Sex<span class="op">==</span><span class="dv">1</span>,]) <span class="co"># Results for Females </span>
<span class="kw">summary</span>(m2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = aisR2[aisR2$Sex == 1, ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m2)</code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 40.1626516 43.86490713
## Bfat        -0.1856071  0.01553165</code></pre>
<p>One more worked example is provided from the Montana fire data. In this example pay particular attention to how we are handling the units of the response variable, log-hectares, and to the changes to doing inferences with a 99% confidence level CI, and where you can find the needed results in the following output:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mtfires &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/climateR2.csv&quot;</span>)
mtfires<span class="op">$</span>loghectares &lt;-<span class="st"> </span><span class="kw">log</span>(mtfires<span class="op">$</span>hectares)
fire1 &lt;-<span class="st"> </span><span class="kw">lm</span>(loghectares<span class="op">~</span>Temperature, <span class="dt">data=</span>mtfires)
<span class="kw">summary</span>(fire1)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectares ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(fire1, <span class="dt">level=</span><span class="fl">0.99</span>)</code></pre></div>
<pre><code>##                    0.5 %     99.5 %
## (Intercept) -104.6477287 -34.921286
## Temperature    0.7753784   2.001499</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qt</span>(<span class="fl">0.995</span>, <span class="dt">df=</span><span class="dv">21</span>)</code></pre></div>
<pre><code>## [1] 2.83136</code></pre>
<ul>
<li><p>Based on the estimated regression model, we can say that if the average temperature is 0, we expect that, on average, the log-area burned would be -69.8 log-hectares.</p></li>
<li><p>From the regression model summary, <span class="math inline">\(b_1=1.39\)</span> with <span class="math inline">\(\text{SE}_{b_1}=0.2165\)</span> and <span class="math inline">\(\mathbf{t=6.41}\)</span></p></li>
<li><p>There were <span class="math inline">\(n=23\)</span> measurements taken, so <span class="math inline">\(\mathbf{df=n-2=23-3=21}\)</span></p></li>
<li><p>Suppose that we want to test for a linear relationship between temperature and log-hectares burned:</p>
<p><span class="math display">\[H_0: \beta_1=0\]</span></p>
<ul>
<li>In words, the true slope coefficient between <em>Temperature</em> and <em>log-area burned</em> is 0 OR there is no linear relationship between <em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul>
<p><span class="math display">\[H_A: \beta_1\ne 0\]</span></p>
<ul>
<li>In words, the alternative states that the true slope coefficient between <em>Temperature</em> and <em>log-area burned</em> is not 0 OR there is a linear relationship between <em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul></li>
</ul>
<p>Test statistic: <span class="math inline">\(t = 1.39/0.217 = 6.41\)</span></p>
<ul>
<li>Assuming the null hypothesis to be true (no linear relationship), the <span class="math inline">\(t\)</span>-statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2 = 23-2=21\)</span> degrees of freedom (or simply <span class="math inline">\(t_{21}\)</span>).</li>
</ul>
<p>p-value:</p>
<ul>
<li><p>From the model summary, the <strong>p-value is</strong> <span class="math inline">\(\mathbf{2.35*10^{-6}}\)</span></p>
<ul>
<li>Interpretation: There is less than a 0.01% chance that we would observe slope coefficient like we did or something more extreme (greater than 1.39 log(hectares)/<span class="math inline">\(^\circ F\)</span>) if there were in fact no linear relationship between temperature (<span class="math inline">\(^\circ F\)</span>) and log-area burned (log-hectares) in the population.</li>
</ul></li>
</ul>
<p>Decision: The p-value is very small so presents very strong evidence against <span class="math inline">\(H_0\)</span>.</p>
<p>Conclusion: There is very strong evidence against the null hypothesis of no linear relationship, so we would conclude that there is, in fact, a linear relationship between Temperature and log(Hectares) burned. Since we have a time series of results, our inferences pertain to the results we could have observed for these years but not for years we did not observe – so just for the true slope for this sample of years. Because we can’t randomly assign the amount of area burned, we cannot make causal inferences – there are many reasons why both the average temperature and area burned would vary together that would not involve a direct connection between them.</p>
<p><span class="math display">\[\text{99}\% \text{ CI for } \beta_1: \boldsymbol{b_1 \mp
t^*_{n-2}}\textbf{SE}_{\boldsymbol{b_1}} \rightarrow 1.39 \mp 2.831\bullet 0.217
\rightarrow (0.78, 2.00)\]</span></p>
<p>Interpretation of 99% CI for slope coefficient:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the change in the true mean log-area burned is between 0.78 and 2.00 log(Hectares).</li>
</ul>
<p>Another way to interpret this is:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the mean Area Burned will change by between 0.78 and 2.00 log(Hectares) <strong>in the population</strong>.</li>
</ul>
<p>Also <span class="math inline">\(R^2\)</span> is 66.2%, which tells us that <em>Temperature</em> explains 66.2% of the variation in <em>log(Hectares) burned</em>. Or that the linear regression model built using <em>Temperature</em> explains 66.2% of the variation in yearly <em>log(Hectares) burned</em> so this model explains quite a bit but not all the variation in the responses.</p>
</div>
<div class="footnotes">
<hr />
<ol start="80">
<li id="fn80"><p>There is an area of statistical research on how to optimally choose x-values to get the most precise estimate of a slope coefficient. In observational studies we have to deal with whatever pattern of <span class="math inline">\(x\text{&#39;s}\)</span> we ended up with. If you can choose, generate an even spread of <span class="math inline">\(x\text{&#39;s}\)</span> over some range of interest similar to what was used in the <em>Beers</em> vs <em>BAC</em> study to provide the best distribution of values to discover the relationship across the selected range of x-values.<a href="7-2-section7-2.html#fnref80">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="7-1-section7-1.html"><button class="btn btn-default">Previous</button></a>
<a href="7-3-section7-3.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
