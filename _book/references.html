<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>References | Intermediate Statistics with R</title>
  <meta name="description" content="References | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="References | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="References | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter9.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="references" class="section level1 unnumbered">
<h1>References</h1>

<div id="refs" class="references">
<div>
<p>Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> 19: 716–23.</p>
</div>
<div>
<p>Allaire, JJ. 2014. <em>Manipulate: Interactive Plots for Rstudio</em>. <a href="https://CRAN.R-project.org/package=manipulate">https://CRAN.R-project.org/package=manipulate</a>.</p>
</div>
<div>
<p>Azzalini, Adelchi, and Adrian W. Bowman. 1990. “A Look at Some Data on the Old Faithful Geyser.” <em>Applied Statistics</em> 39: 357–65.</p>
</div>
<div>
<p>Barton, Kamil. 2019. <em>MuMIn: Multi-Model Inference</em>. <a href="https://CRAN.R-project.org/package=MuMIn">https://CRAN.R-project.org/package=MuMIn</a>.</p>
</div>
<div>
<p>Benson, Roger B. J., and Philip D. Mannion. 2012. “Multi-Variate Models Are Essential for Understanding Vertebrate Diversification in Deep Time.” <em>Biology Letters</em> 8: 127–30. <a href="https://doi.org/10.1098/rsbl.2011.0460">https://doi.org/10.1098/rsbl.2011.0460</a>.</p>
</div>
<div>
<p>Bland, J Martin, and Douglas G Altman. 1995. “Multiple Significance Tests: The Bonferroni Method.” <em>BMJ</em> 310 (6973): 170. <a href="https://doi.org/10.1136/bmj.310.6973.170">https://doi.org/10.1136/bmj.310.6973.170</a>.</p>
</div>
<div>
<p>Burnham, Kenneth P., and David R. Anderson. 2002. <em>Model Selection and Multimodel Inference</em>. NY: Springer.</p>
</div>
<div>
<p>Clevenger, Anthony P, and Nigel Waltho. 2005. “Performance Indices to Identify Attributes of Highway Crossing Structures Facilitating Movement of Large Mammals.” <em>Biological Conservation</em> 121 (3): 453–64.</p>
</div>
<div>
<p>Crampton, E. 1947. “The Growth of the Odontoblast of the Incisor Teeth as a Criterion of Vitamin c Intake of the Guinea Pig.” <em>The Journal of Nutrition</em> 33 (5): 491–504. <a href="http://jn.nutrition.org/content/33/5/491.full.pdf">http://jn.nutrition.org/content/33/5/491.full.pdf</a>.</p>
</div>
<div>
<p>Dayton, C. Mitchell. 1998. <em>Latent Class Scaling Analysis</em>. Thousand Oaks, CA: SAGE Publications.</p>
</div>
<div>
<p>De Veaux, Richard D., Paul F. Velleman, and David E. Bock. 2011. <em>Stats: Data and Models, 3rd Edition</em>. Pearson.</p>
</div>
<div>
<p>Dieser, Markus, Mark C. Greenwood, and Christine M. Foreman. 2010. “Carotenoid Pigmentation in Antarctic Heterotrophic Bacteria as a Strategy to Withstand Environmental Stresses.” <em>Arctic, Antarctic, and Alpine Research</em> 42(4): 396–405. <a href="https://doi.org/10.1657/1938-4246-42.4.396">https://doi.org/10.1657/1938-4246-42.4.396</a>.</p>
</div>
<div>
<p>Diez, David M, Christopher D Barr, and Mine Cetinkaya-Rundel. 2017. <em>Openintro: Data Sets and Supplemental Functions from ’Openintro’ Textbooks</em>. <a href="https://CRAN.R-project.org/package=openintro">https://CRAN.R-project.org/package=openintro</a>.</p>
</div>
<div>
<p>Doolittle, Alan E., and Catherine Welch. 1989. “Gender Differences in Performance on a College-Level Acheivement Test.” <em>ACT Research Report</em>, 89–90.</p>
</div>
<div>
<p>Faraway, Julian. 2016. <em>Faraway: Functions and Datasets for Books by Julian Faraway</em>. <a href="https://CRAN.R-project.org/package=faraway">https://CRAN.R-project.org/package=faraway</a>.</p>
</div>
<div>
<p>F. L. Ramsey, Original by, D. W. Schafer; modifications by Daniel W. Schafer, Jeannie Sifneos, Berwin A. Turlach; vignettes contributed by Nicholas Horton, Kate Aloisio, Ruobing Zhang, and with corrections by Randall Pruim. 2019. <em>Sleuth2: Data Sets from Ramsey and Schafer’s "Statistical Sleuth (2nd Ed)"</em>. <a href="https://CRAN.R-project.org/package=Sleuth2">https://CRAN.R-project.org/package=Sleuth2</a>.</p>
</div>
<div>
<p>Fox, John. 2003. “Effect Displays in R for Generalised Linear Models.” <em>Journal of Statistical Software</em> 8 (15): 1–27. <a href="http://www.jstatsoft.org/v08/i15/">http://www.jstatsoft.org/v08/i15/</a>.</p>
</div>
<div>
<p>Fox, John, and Michael Friendly. 2018. <em>Heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</em>. <a href="https://CRAN.R-project.org/package=heplots">https://CRAN.R-project.org/package=heplots</a>.</p>
</div>
<div>
<p>Fox, John, and Sanford Weisberg. 2011. <em>An R-Companion to Applied Regression, Second Edition</em>. Thousand Oaks, CA: SAGE Publications. <a href="http://socserv.socsci.mcmaster.ca/jfox/Books/Companion">http://socserv.socsci.mcmaster.ca/jfox/Books/Companion</a>.</p>
</div>
<div>
<p>Fox, John, Sanford Weisberg, and Brad Price. 2018. <em>CarData: Companion to Applied Regression Data Sets</em>. <a href="https://CRAN.R-project.org/package=carData">https://CRAN.R-project.org/package=carData</a>.</p>
</div>
<div>
<p>———. 2019. <em>Car: Companion to Applied Regression</em>. <a href="https://CRAN.R-project.org/package=car">https://CRAN.R-project.org/package=car</a>.</p>
</div>
<div>
<p>Fox, John, Sanford Weisberg, Brad Price, Michael Friendly, and Jangman Hong. 2019. <em>Effects: Effect Displays for Linear, Generalized Linear, and Other Models</em>. <a href="https://CRAN.R-project.org/package=effects">https://CRAN.R-project.org/package=effects</a>.</p>
</div>
<div>
<p>Gandrud, Christopher. 2015. <em>Reproducible Research with R and R Studio, Second Edition</em>. Chapman Hall, CRC.</p>
</div>
<div>
<p>Garnier, Simon. 2018. <em>Viridis: Default Color Maps from ’Matplotlib’</em>. <a href="https://CRAN.R-project.org/package=viridis">https://CRAN.R-project.org/package=viridis</a>.</p>
</div>
<div>
<p>Greenwood, Mark C., Joel Harper, and Johnnie Moore. 2011. “An Application of Statistics in Climate Change: Detection of Nonlinear Changes in a Streamflow Timing Measure in the Columbia and Missouri Headwaters.” In <em>Handbook of the Philosophy of Science, Vol. 7: Statistics</em>, edited by P. S. Bandyopadhyay and M. Forster, 1117–42. Elsevier.</p>
</div>
<div>
<p>Greenwood, Mark C., and N. F. Humphrey. 2002. “Glaciated Valley Profiles: An Application of Nonlinear Regression.” <em>Computing Science and Statistics</em> 34: 452–60.</p>
</div>
<div>
<p>Gude, Patricia H., J. Anthony Cookson, Mark C. Greenwood, and Mark Haggerty. 2009. “Homes in Wildfire-Prone Areas: An Empirical Analysis of Wildfire Suppression Costs and Climate Change.” <a href="www.headwaterseconomics.org">www.headwaterseconomics.org</a>.</p>
</div>
<div>
<p>Gundale, Michael J., Lisbet H. Bach, and Annika Nordin. 2013. “The Impact of Simulated Chronic Nitrogen Deposition on the Biomass and N2-Fixation Activity of Two Boreal Feather Moss–Cyanobacteria Associations.” <em>Biology Letters</em> 9 (6). <a href="https://doi.org/10.1098/rsbl.2013.0797">https://doi.org/10.1098/rsbl.2013.0797</a>.</p>
</div>
<div>
<p>Hothorn, Torsten, Frank Bretz, and Peter Westfall. 2008. “Simultaneous Inference in General Parametric Models.” <em>Biometrical Journal</em> 50 (3): 346–63.</p>
</div>
<div>
<p>———. 2019. <em>Multcomp: Simultaneous Inference in General Parametric Models</em>. <a href="https://CRAN.R-project.org/package=multcomp">https://CRAN.R-project.org/package=multcomp</a>.</p>
</div>
<div>
<p>Hurlbert, Stuart H. 1984. “Pseudoreplication and the Design of Ecological Field Experiments.” <em>Ecological Monographs</em> 54 (2): 187–211. <a href="http://www.jstor.org/stable/1942661">http://www.jstor.org/stable/1942661</a>.</p>
</div>
<div>
<p>Jones, Owen, Robert Maillardet, Andrew Robinson, Olga Borovkova, and Steven Carnie. 2018. <em>SpuRs: Functions and Datasets for "Introduction to Scientific Programming and Simulation Using R"</em>. <a href="https://CRAN.R-project.org/package=spuRs">https://CRAN.R-project.org/package=spuRs</a>.</p>
</div>
<div>
<p>Kampstra, Peter. 2008. “Beanplot: A Boxplot Alternative for Visual Comparison of Distributions.” <em>Journal of Statistical Software, Code Snippets</em> 28 (1): 1–9. <a href="http://www.jstatsoft.org/v28/c01/">http://www.jstatsoft.org/v28/c01/</a>.</p>
</div>
<div>
<p>Lea, Stephen E. G., Paul Webley, and Catherine M. Walker. 1995. “Psychological Factors in Consumer Debt: Money Management, Economic Socialization, and Credit Use.” <em>Journal of Economic Psychology</em> 16 (4): 681–701.</p>
</div>
<div>
<p>Liao, Xiyue, and Mary C. Meyer. 2014. “Coneproj: An R Package for the Primal or Dual Cone Projections with Routines for Constrained Regression.” <em>Journal of Statistical Software</em> 61 (12): 1–22. <a href="http://www.jstatsoft.org/v61/i12/">http://www.jstatsoft.org/v61/i12/</a>.</p>
</div>
<div>
<p>Likert, Rensis. 1932. “A Technique for the Measurement of Attitudes.” <em>Archives of Psychology</em> 140: 1–55.</p>
</div>
<div>
<p>Linzer, Drew, and Jeffrey Lewis. 2014. <em>PoLCA: Polytomous Variable Latent Class Analysis</em>. <a href="https://CRAN.R-project.org/package=poLCA">https://CRAN.R-project.org/package=poLCA</a>.</p>
</div>
<div>
<p>Linzer, Drew, and Jeffrey Lewis. 2011. “PoLCA: An R Package for Polytomous Variable Latent Class Analysis.” <em>Journal of Statistical Software</em> 42 (10): 1–29.</p>
</div>
<div>
<p>Lumley, Thomas. 2019. <em>Survey: Analysis of Complex Survey Samples</em>. <a href="https://CRAN.R-project.org/package=survey">https://CRAN.R-project.org/package=survey</a>.</p>
</div>
<div>
<p>Merkle, Ed, and Michael Smithson. 2018. <em>Smdata: Data to Accompany Smithson &amp; Merkle, 2013</em>. <a href="https://CRAN.R-project.org/package=smdata">https://CRAN.R-project.org/package=smdata</a>.</p>
</div>
<div>
<p>Meyer, David, Achim Zeileis, and Kurt Hornik. 2017. <em>Vcd: Visualizing Categorical Data</em>. <a href="https://CRAN.R-project.org/package=vcd">https://CRAN.R-project.org/package=vcd</a>.</p>
</div>
<div>
<p>Meyer, Mary C., and Xiyue Liao. 2018. <em>Coneproj: Primal or Dual Cone Projections with Routines for Constrained Regression</em>. <a href="https://CRAN.R-project.org/package=coneproj">https://CRAN.R-project.org/package=coneproj</a>.</p>
</div>
<div>
<p>Moore, Johnnie N., Joel T. Harper, and Mark C. Greenwood. 2007. “Significance of Trends Toward Earlier Snowmelt Runoff, Columbia and Missouri Basin Headwaters, Western United States.” <em>Geophysical Research Letters</em> 34 (16). <a href="https://doi.org/10.1029/2007GL031022">https://doi.org/10.1029/2007GL031022</a>.</p>
</div>
<div>
<p>Phillips, Nathaniel. 2017a. <em>Yarrr: A Companion to the E-Book "Yarrr!: The Pirate’s Guide to R"</em>. <a href="https://CRAN.R-project.org/package=yarrr">https://CRAN.R-project.org/package=yarrr</a>.</p>
</div>
<div>
<p>———. 2017b. <em>Yarrr: A Companion to the E-Book "Yarrr!: The Pirate’s Guide to R"</em>. <a href="https://CRAN.R-project.org/package=yarrr">https://CRAN.R-project.org/package=yarrr</a>.</p>
</div>
<div>
<p>Piepho, Hans-Peter. 2004. “An Algorithm for a Letter-Based Representation of All-Pairwise Comparisons.” <em>Journal of Computational and Graphical Statistics</em> 13 (2): 456–66.</p>
</div>
<div>
<p>Pruim, Randall, Daniel Kaplan, and Nicholas Horton. 2018. <em>MosaicData: Project Mosaic Data Sets</em>. <a href="https://CRAN.R-project.org/package=mosaicData">https://CRAN.R-project.org/package=mosaicData</a>.</p>
</div>
<div>
<p>Pruim, Randall, Daniel T. Kaplan, and Nicholas J. Horton. 2019. <em>Mosaic: Project Mosaic Statistics and Mathematics Teaching Utilities</em>. <a href="https://CRAN.R-project.org/package=mosaic">https://CRAN.R-project.org/package=mosaic</a>.</p>
</div>
<div>
<p>Puhan, Milo A, Alex Suarez, Christian Lo Cascio, Alfred Zahn, Markus Heitz, and Otto Braendli. 2006. “Didgeridoo Playing as Alternative Treatment for Obstructive Sleep Apnoea Syndrome: Randomised Controlled Trial.” <em>BMJ</em> 332 (7536): 266–70. <a href="https://doi.org/10.1136/bmj.38705.470590.55">https://doi.org/10.1136/bmj.38705.470590.55</a>.</p>
</div>
<div>
<p>Ramsey, Fred, and Daniel Schafer. 2012. <em>The Statistical Sleuth: A Course in Methods of Data Analysis</em>. Cengage Learning. <a href="https://books.google.com/books?id=eSlLjA9TwkUC">https://books.google.com/books?id=eSlLjA9TwkUC</a>.</p>
</div>
<div>
<p>R Core Team. 2019. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div>
<p>Revelle, William. 2019. <em>Psych: Procedures for Psychological, Psychometric, and Personality Research</em>. <a href="https://CRAN.R-project.org/package=psych">https://CRAN.R-project.org/package=psych</a>.</p>
</div>
<div>
<p>Ripley, Brian. 2019. <em>MASS: Support Functions and Datasets for Venables and Ripley’s Mass</em>. <a href="https://CRAN.R-project.org/package=MASS">https://CRAN.R-project.org/package=MASS</a>.</p>
</div>
<div>
<p>Robinson, Rebekah, and Homer White. 2016. <em>Tigerstats: R Functions for Elementary Statistics</em>. <a href="https://CRAN.R-project.org/package=tigerstats">https://CRAN.R-project.org/package=tigerstats</a>.</p>
</div>
<div>
<p>RStudio Team. 2018. <em>RStudio: Integrated Development Environment for R</em>. Boston, MA: RStudio, Inc. <a href="http://www.rstudio.com/">http://www.rstudio.com/</a>.</p>
</div>
<div>
<p>Santibáñez, Pamela A., Olivia J. Maselli, Mark C. Greenwood, Mackenzie M. Grieman, Eric S. Saltzman, Joseph R. McConnell, and John C. Priscu. 2018. “Prokaryotes in the Wais Divide Ice Core Reflect Source and Transport Changes Between Last Glacial Maximum and the Early Holocene.” <em>Global Change Biology</em> 24 (5): 2182–97. <a href="https://doi.org/10.1111/gcb.14042">https://doi.org/10.1111/gcb.14042</a>.</p>
</div>
<div>
<p>Sasaki, Takao, and Stephen C. Pratt. 2013. “Ants Learn to Rely on More Informative Attributes During Decision-Making.” <em>Biology Letters</em> 9 (6). <a href="https://doi.org/10.1098/rsbl.2013.0667">https://doi.org/10.1098/rsbl.2013.0667</a>.</p>
</div>
<div>
<p>Schneck, Andreas. 2017. “Examining Publication Bias—a Simulation-Based Evaluation of Statistical Tests on Publication Bias.” <em>PeerJ</em> 5 (November): e4115. <a href="https://doi.org/10.7717/peerj.4115">https://doi.org/10.7717/peerj.4115</a>.</p>
</div>
<div>
<p>Smith, Michael L. 2014. “Honey Bee Sting Pain Index by Body Location.” <em>PeerJ</em> 2 (April): e338. <a href="https://doi.org/10.7717/peerj.338">https://doi.org/10.7717/peerj.338</a>.</p>
</div>
<div>
<p>Tennekes, Martijn, and Edwin de Jonge. 2019. <em>Tabplot: Tableplot, a Visualization of Large Datasets</em>. <a href="https://CRAN.R-project.org/package=tabplot">https://CRAN.R-project.org/package=tabplot</a>.</p>
</div>
<div>
<p>Vsevolozhskaya, Olga A., Dmitri V. Zaykin, Mark C. Greenwood, Changshuai Wei, and Qing Lu. 2014. “Functional Analysis of Variance for Association Studies.” <em>PLOS ONE</em> 9 (9): 13. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0105074">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0105074</a>.</p>
</div>
<div>
<p>Walker, Ian, Ian Garrard, and Felicity Jowitt. 2014. “The Influence of a Bicycle Commuter’s Appearance on Drivers’ Overtaking Proximities: An on-Road Test of Bicyclist Stereotypes, High-Visibility Clothing and Safety Aids in the United Kingdom.” <em>Accident Analysis U+0026 Prevention</em> 64: 69–77. <a href="https://doi.org/https://doi.org/10.1016/j.aap.2013.11.007">https://doi.org/https://doi.org/10.1016/j.aap.2013.11.007</a>.</p>
</div>
<div>
<p>Wasserstein, Ronald L., and Nicole A. Lazar. 2016. “The Asa Statement on P-Values: Context, Process, and Purpose.” <em>The American Statistician</em> 70 (2): 129–33. <a href="https://doi.org/10.1080/00031305.2016.1154108">https://doi.org/10.1080/00031305.2016.1154108</a>.</p>
</div>
<div>
<p>Wei, Taiyun, and Viliam Simko. 2017. <em>Corrplot: Visualization of a Correlation Matrix</em>. <a href="https://CRAN.R-project.org/package=corrplot">https://CRAN.R-project.org/package=corrplot</a>.</p>
</div>
<div>
<p>Weisberg, Sanford. 2005. <em>Applied Linear Regression, Third Edition</em>. Hoboken, NJ: Wiley.</p>
</div>
<div>
<p>———. 2018. <em>Alr3: Data to Accompany Applied Linear Regression 3rd Edition</em>. <a href="https://CRAN.R-project.org/package=alr3">https://CRAN.R-project.org/package=alr3</a>.</p>
</div>
<div>
<p>Westfall, Peter H., and S. Stanley Young. 1993. <em>Resampling-Based Multiple Testing: Examples and Methods for P-Value Adjustment</em>. New York: Wiley.</p>
</div>
<div>
<p>Wickham, Hadley, Winston Chang, Lionel Henry, Thomas Lin Pedersen, Kohske Takahashi, Claus Wilke, Kara Woo, and Hiroaki Yutani. 2019. <em>Ggplot2: Create Elegant Data Visualisations Using the Grammar of Graphics</em>. <a href="https://CRAN.R-project.org/package=ggplot2">https://CRAN.R-project.org/package=ggplot2</a>.</p>
</div>
<div>
<p>Wickham, Hadley, Jim Hester, and Romain Francois. 2018. <em>Readr: Read Rectangular Text Data</em>. <a href="https://CRAN.R-project.org/package=readr">https://CRAN.R-project.org/package=readr</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>A <strong><em>placebo</em></strong> is a treatment level designed to
mimic the potentially efficacious level(s) but that can have no actual effect. The
<strong><em>placebo effect</em></strong> is the effect that thinking that an effective treatment was
received has on subjects. There are other related issues in performing experiments
like the <strong><em>Hawthorne</em></strong> or <strong><em>observer effect</em></strong> where subjects modify behavior
because they are being observed.<a href="chapter1.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>We will reserve the term “effect” for situations where we could
potentially infer causal impacts on the response of the explanatory variable which
occurs in situations where the levels of the explanatory variable are randomly
assigned to the subjects.<a href="chapter1.html#fnref2" class="footnote-back">↩︎</a></p></li>
<li id="fn3"><p>There is a cloud version of R Studio available at <a href="https://rstudio.cloud/" class="uri">https://rstudio.cloud/</a> if you want to avoid these steps. We still recommend following the steps to be able to work locally but try this option if you have issues with the installation process.<a href="chapter1.html#fnref3" class="footnote-back">↩︎</a></p></li>
<li id="fn4"><p>I recorded a video that walks through getting R and RStudio installed on a PC available <a href="https://montana.techsmithrelay.com/ojg6"><strong>in a recorded video</strong></a>. If you want to see them installed on a mac, you can try <a href="https://www.youtube.com/watch?v=GFImMj1lMRI"><strong>this video on youtube</strong></a>. Or for either version, try searching YouTube for “How to install R and RStudio”.<a href="chapter1.html#fnref4" class="footnote-back">↩︎</a></p></li>
<li id="fn5"><p>The need to keep the code up-to-date as R continues to evolve is one reason that this book is locally published and that this is the 6<sup>th</sup> time it has been revised in
six years…<a href="chapter1.html#fnref5" class="footnote-back">↩︎</a></p></li>
<li id="fn6"><p>There are ways to read “.xls” and “.xlsx” files
directly into R that we will explore later so you can also use that format if you prefer.<a href="chapter1.html#fnref6" class="footnote-back">↩︎</a></p></li>
<li id="fn7"><p>If
you are having trouble getting the file converted and read into R, copy and
run the following code:
<code>treadmill &lt;- read_csv("http://www.math.montana.edu/courses/s217/documents/treadmill.csv")</code>.<a href="chapter1.html#fnref7" class="footnote-back">↩︎</a></p></li>
<li id="fn8"><p>You can also use Ctrl+Enter if you like hot keys.<a href="chapter1.html#fnref8" class="footnote-back">↩︎</a></p></li>
<li id="fn9"><p>Tibbles are R objects that can contain both
categorical and quantitative variables on your <span class="math inline">\(n\)</span> subjects with a name for each
variable that is also the name of each column in a matrix.  Each subject is a
row of the data set. The name (supposedly) is due to the way <em>table</em> sounds in the accent of a particularly influential developer at RStudio who is from New Zealand.<a href="chapter1.html#fnref9" class="footnote-back">↩︎</a></p></li>
<li id="fn10"><p>The median, quartiles and whiskers sometimes occur at the same
values when there are many tied observations. If you can’t see all the
components of the boxplot, produce the numerical summary to help you understand
what happened.<a href="chapter1.html#fnref10" class="footnote-back">↩︎</a></p></li>
<li id="fn11"><p>You will more typically hear “data is” but that more often refers to
information, sometimes even statistical summaries of data sets, than to
observations made on subjects collected as part of a study, suggesting the confusion of this
term in the general public. We will explore a data set in Chapter <a href="chapter5.html#chapter5">5</a>
related to perceptions of this issue collected by researchers at
<a href="http://fivethirtyeight.com/" class="uri">http://fivethirtyeight.com/</a>.<a href="chapter2.html#fnref11" class="footnote-back">↩︎</a></p></li>
<li id="fn12"><p>Either try to remember “data is a plural word” or replace “data” with “things” in your sentence and consider whether it sounds right.<a href="chapter2.html#fnref12" class="footnote-back">↩︎</a></p></li>
<li id="fn13"><p>Of particular interest to the bicycle rider might be the “close” passes and we will revisit this as a categorical response with “close” and “not close” as its two categories later.<a href="chapter2.html#fnref13" class="footnote-back">↩︎</a></p></li>
<li id="fn14"><p>Thanks to Ian Walker for allowing me to use and post these data.<a href="chapter2.html#fnref14" class="footnote-back">↩︎</a></p></li>
<li id="fn15"><p>As noted previously, we reserve the term “effect” for situations where random assignment  allows us to consider causality as the reason for the differences in the response variable among levels of the explanatory variable, but this is only the case if we find evidence against the null hypothesis of no difference
in the groups.<a href="chapter2.html#fnref15" class="footnote-back">↩︎</a></p></li>
<li id="fn16"><p>If you’ve taken calculus, you will
know that the
curve is being constructed so that the integral from <span class="math inline">\(-\infty\)</span> to
<span class="math inline">\(\infty\)</span> is 1. If you don’t know calculus, think of a rectangle with area
of 1 based on its height and width. These cover the same area but the top of the
region wiggles.<a href="chapter2.html#fnref16" class="footnote-back">↩︎</a></p></li>
<li id="fn17"><p>Jittering typically
involves adding random variability to each observation that
is uniformly distributed in a range determined based on the spacing of the
observation. The idea is to jitter just enough to see all the points but not too much. This means that if you re-run the <code>jitter</code> function, the results will change if you do not set the random number seed using <code>set.seed</code> that is discussed more below.
For more details, type <code>help(jitter)</code> in the console in RStudio.<a href="chapter2.html#fnref17" class="footnote-back">↩︎</a></p></li>
<li id="fn18"><p>If you want to type this character in Rmarkdown, try <code>$\sim$</code> outside of codechunks.<a href="chapter2.html#fnref18" class="footnote-back">↩︎</a></p></li>
<li id="fn19"><p>Remember the
bell-shaped curve you encountered in introductory statistics? If not, you can
see some at <a href="https://en.wikipedia.org/wiki/Normal_distribution" class="uri">https://en.wikipedia.org/wiki/Normal_distribution</a>.<a href="chapter2.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>The package and function are intentionally amusingly titled but are based on ideas in the beanplot in <span class="citation">Kampstra (<a href="#ref-Kampstra2008" role="doc-biblioref">2008</a>)</span> and provide what they call an <strong><em>RDI graphic</em></strong> - <strong><em>R</em></strong>aw data, <strong><em>D</em></strong>escriptive, and <strong><em>I</em></strong>nferential statistic in the same display.<a href="chapter2.html#fnref20" class="footnote-back">↩︎</a></p></li>
<li id="fn21"><p>The
hypothesis of no difference that is typically generated in the hopes of
being rejected in favor of the alternative hypothesis, which contains the sort
of difference that is of interest in the application.<a href="chapter2.html#fnref21" class="footnote-back">↩︎</a></p></li>
<li id="fn22"><p>The null model is the statistical model that
is implied by the chosen null hypothesis. Here, a null hypothesis of no
difference translates to having a model with the same mean for both groups.<a href="chapter2.html#fnref22" class="footnote-back">↩︎</a></p></li>
<li id="fn23"><p>While note required, we often set our random number seed using the <code>set.seed</code> function so that when we re-run code with randomization in it we get the same results. <a href="chapter2.html#fnref23" class="footnote-back">↩︎</a></p></li>
<li id="fn24"><p>We’ll see the <code>shuffle</code> function in a more common usage below;
while the code to generate <code>Perm1</code> is provided, it isn’t something to worry
about right now.<a href="chapter2.html#fnref24" class="footnote-back">↩︎</a></p></li>
<li id="fn25"><p>This is a bit like getting a new convertible sports car and driving it to the grocery store - there might be better ways to get groceries, but we want to drive our new car as soon as we get it.<a href="chapter2.html#fnref25" class="footnote-back">↩︎</a></p></li>
<li id="fn26"><p>This will be formalized and explained more in the next chapter when we encounter more than two groups in these same models. For now, it is recommended to start with the sample means from <code>favstats</code> for the two groups and then use that to sort out which direction the differencing was done in the <code>lm</code> output.<a href="chapter2.html#fnref26" class="footnote-back">↩︎</a></p></li>
<li id="fn27"><p>P-values  are the
probability of obtaining a result as extreme as or more extreme than we observed
given that the null hypothesis is true.<a href="chapter2.html#fnref27" class="footnote-back">↩︎</a></p></li>
<li id="fn28"><p>In statistics, vectors are one dimensional lists of numeric elements – basically a column from a matrix of our tibble.<a href="chapter2.html#fnref28" class="footnote-back">↩︎</a></p></li>
<li id="fn29"><p>We often say
“under” in statistics and we mean “given that the following is true”.<a href="chapter2.html#fnref29" class="footnote-back">↩︎</a></p></li>
<li id="fn30"><p>This is a fancy way of saying “in advance”,
here in advance of seeing the observations.<a href="chapter2.html#fnref30" class="footnote-back">↩︎</a></p></li>
<li id="fn31"><p>Statistically, a conservative method is
one that provides less chance of rejecting the null hypothesis in comparison to
some other method or less than some pre-defined standard. A liberal method provides higher rates of false rejections.<a href="chapter2.html#fnref31" class="footnote-back">↩︎</a></p></li>
<li id="fn32"><p>Both approaches are reasonable. By using both tails of the distribution we can incorporate potential differences in shape in both tails of the permutation distribution.<a href="chapter2.html#fnref32" class="footnote-back">↩︎</a></p></li>
<li id="fn33"><p>We’ll leave the discussion of the CLT to your previous statistics coursework
or an internet search. For this material, just remember that it has something to do with distributions of statistics
looking more normal as the sample size increases.<a href="chapter2.html#fnref33" class="footnote-back">↩︎</a></p></li>
<li id="fn34"><p>The <code>t.test</code> function with the <code>var.equal=T</code> option is the more direct route to calculating this statistic (here that would be <code>t.test(Distance~Condition, data=dsamp, var.equal=T)</code>), but since we can get the result of interest by fitting a linear model, we will use that approach.<a href="chapter2.html#fnref34" class="footnote-back">↩︎</a></p></li>
<li id="fn35"><p>On exams, you might be asked to describe the area of interest, sketch a
picture of the area of interest, and/or note the distribution you would use. Make sure you think about what you are trying to do here as much as learning the mechanics of how to get p-values from R.<a href="chapter2.html#fnref35" class="footnote-back">↩︎</a></p></li>
<li id="fn36"><p>In some studies, the same subject is measured in both conditions and
this violates the assumptions of this procedure.<a href="chapter2.html#fnref36" class="footnote-back">↩︎</a></p></li>
<li id="fn37"><p>At this level, it is critical to learn the tools and learn where they might provide inaccurate inferences. If you explore more advanced statistical resources, you will encounter methods that can allow you to obtain valid inferences in even more scenarios.<a href="chapter2.html#fnref37" class="footnote-back">↩︎</a></p></li>
<li id="fn38"><p>Only male and female were provided as options on the survey. These data were collected as part of a project to study learning of material using online versus paper versions of the book but we focus just on the gender differences in GPA here.<a href="chapter2.html#fnref38" class="footnote-back">↩︎</a></p></li>
<li id="fn39"><p>The data are provided and briefly discussed in the Practice Problems for Chapter <a href="chapter3.html#chapter3">3</a>.<a href="chapter2.html#fnref39" class="footnote-back">↩︎</a></p></li>
<li id="fn40"><p>You can correctly call octothorpes <em>number</em> symbols or, in the
twitter verse, <em>hashtags</em>. For more on this symbol, see
“<a href="http://blog.dictionary.com/octothorpe/" class="uri">http://blog.dictionary.com/octothorpe/</a>”. Even after reading this, I call them
number symbols.<a href="chapter2.html#fnref40" class="footnote-back">↩︎</a></p></li>
<li id="fn41"><p>An unbiased estimator  is a statistic that is on average equal to the population parameter.<a href="chapter2.html#fnref41" class="footnote-back">↩︎</a></p></li>
<li id="fn42"><p>Some perform bootstrap sampling in this situation by re-sampling within each of the groups. We will discuss using this technique in situations without clearly defined groups, so prefer to sample with replacement from the entire data set. It also directly corresponds to situations where the data came from one large sample and then the grouping variable of interest was measured on the <span class="math inline">\(n\)</span> subjects.<a href="chapter2.html#fnref42" class="footnote-back">↩︎</a></p></li>
<li id="fn43"><p>The <code>as.numeric</code> function is also used here. It really isn’t important
but makes sure the output of <code>table</code> is sorted by observation number by first
converting the <em>orig.id</em> variable into a numeric vector.<a href="chapter2.html#fnref43" class="footnote-back">↩︎</a></p></li>
<li id="fn44"><p>In any bootstrap sample, about 1/3 of the observations are not used at all.<a href="chapter2.html#fnref44" class="footnote-back">↩︎</a></p></li>
<li id="fn45"><p>There are actually many ways to use this
information to make a confidence interval. We are using the simplest method
that is called the “percentile” method.<a href="chapter2.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>When hypothesis tests “work well” they have high
power  to detect differences while having Type I error rates  that are close
to what we choose <em>a priori</em>. When confidence intervals “work well”, they contain
the true parameter value in repeated random samples at around the selected
confidence level, which is called the <strong><em>coverage rate</em></strong>. <a href="chapter2.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>We will often use this term to
indicate perform a calculation using the <code>favstats</code> results – not that you need to go back to
the data set and calculate the means and standard deviations yourself.<a href="chapter2.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>In Chapter <a href="chapter4.html#chapter4">4</a>, methods are discussed for when there are two
categorical explanatory variables that is called the Two-Way ANOVA and related
ANOVA tests are used in Chapter <a href="chapter8.html#chapter8">8</a> for working with extensions of
these models.<a href="chapter3.html#fnref48" class="footnote-back">↩︎</a></p></li>
<li id="fn49"><p>In Chapter <a href="chapter2.html#chapter2">2</a>, we used <code>lm</code> to get these estimates and focused on the estimate of the difference between the second group and the baseline - that was and still is the difference in the sample means. Now there are potentially more than two groups and we need to formalize notation to handle this more complex situation.<a href="chapter3.html#fnref49" class="footnote-back">↩︎</a></p></li>
<li id="fn50"><p>
If you look closely in the code for the rest of the book, any model for a
quantitative response will use this function, suggesting a common thread in
the most commonly used statistical models.<a href="chapter3.html#fnref50" class="footnote-back">↩︎</a></p></li>
<li id="fn51"><p>We can and will select the order of the levels of categorical variables as it can make plots easier to interpret.<a href="chapter3.html#fnref51" class="footnote-back">↩︎</a></p></li>
<li id="fn52"><p>Suppose we were doing environmental monitoring
and were studying asbestos levels in soils. We might be hoping that the mean-only
model were reasonable to use if the groups being compared were in remediated areas
and in areas known to have never been contaminated.<a href="chapter3.html#fnref52" class="footnote-back">↩︎</a></p></li>
<li id="fn53"><p>Make sure you can work
from left to right and up and down to fill in the ANOVA table given just the
necessary information to determine the other components or from a study description to complete the <em>DF</em> part of the table – there are always questions like these on exams…<a href="chapter3.html#fnref53" class="footnote-back">↩︎</a></p></li>
<li id="fn54"><p>Any further claimed precision is an exaggeration and eventually we might see p-values that approach the precision of the computer at 2.2e-16 and anything below 0.0001 should just be reported as being below 0.0001. Also note the way that R represents small or extremely large numbers using scientific notation such as <code>3e-4</code> which is <span class="math inline">\(3 \cdot 10^{-4} = 0.0003\)</span>.<a href="chapter3.html#fnref54" class="footnote-back">↩︎</a></p></li>
<li id="fn55"><p>This would be another type of publication bias – where researchers search across groups and only report their biggest differences and fail to report the other pairs that they compared. As discussed before, this biases the
results to detecting results more than they should be and then when
other researchers try to repeat the same studies and compare just, say, two groups, they likely will fail to find
similar results unless they also search across many different possible comparisons and only report the most extreme. The better approach is to do the ANOVA <span class="math inline">\(F\)</span>-test first and then Tukey’s comparisons and report all these results, as discussed below.<a href="chapter3.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>We have been using this function quite a bit to make multi-panel
graphs but did not show you that line of code. But you need to use this command
for linear model diagnostics or you won’t get the plots we want from the model.
And you really just need <code>plot(lm2)</code> but the <code>pch=16</code> option makes it easier
to see some of the points in the plots.<a href="chapter3.html#fnref56" class="footnote-back">↩︎</a></p></li>
<li id="fn57"><p>Along with multiple names,
there is variation of what is plotted on the x and y axes, the scaling of
the values plotted, and even the way the line is chosen to represent the 1-1 relationship, increasing the challenge of interpreting QQ-plots. We are
consistent about the x and y axis choices throughout this book and how the line is drawn but different versions of
these plots do vary in what is presented, so be careful with using QQ-plots.<a href="chapter3.html#fnref57" class="footnote-back">↩︎</a></p></li>
<li id="fn58"><p>Here this means re-scaled so that they should have similar
scaling to a standard normal with mean 0 and standard deviation 1. This does
not change the shape of the distribution but can make outlier identification
simpler – having a standardized residual more extreme than 5 or -5 would
suggest a deviation from normality since we rarely see values that many
standard deviations from the mean in a normal distribution. But mainly focus
on the pattern in points in the QQ-plot and whether it matches the 1-1 line that is being plotted.<a href="chapter3.html#fnref58" class="footnote-back">↩︎</a></p></li>
<li id="fn59"><p>A resistant
procedure is one that is not severely impacted by a particular violation of an
assumption. For example, the median is resistant to the impact of
an outlier. But the mean is not a resistant measure as changing the value
of a single point changes the mean.<a href="chapter3.html#fnref59" class="footnote-back">↩︎</a></p></li>
<li id="fn60"><p>A violation of the independence
assumption could have easily been created if they measured cells in

two locations on each guinea pig or took measurements over time on each subject.<a href="chapter3.html#fnref60" class="footnote-back">↩︎</a></p></li>
<li id="fn61"><p>Note that to see all the group labels in the plot
when making the figure, you have to widen the plot window before copying the
figure out of R. You can resize the plot window using the small vertical and
horizontal “=” signs in the grey bars that separate the different panels in
RStudio.<a href="chapter3.html#fnref61" class="footnote-back">↩︎</a></p></li>
<li id="fn62"><p>In working with researchers on hundreds of projects, my
experience has been that many conversations are often required to discover
all the potential sources of issues in data sets, especially related to
assessing independence of the observations. Discussing how the data were collected is sometimes the only way to understand whether violations of independence are present or not.<a href="chapter3.html#fnref62" class="footnote-back">↩︎</a></p></li>
<li id="fn63"><p>When this procedure is used with unequal group sizes it is also sometimes
called Tukey-Kramer’s method.<a href="chapter3.html#fnref63" class="footnote-back">↩︎</a></p></li>
<li id="fn64"><p>We often use “spurious” to describe falsely rejected null hypotheses, but
they are also called false detections.<a href="chapter3.html#fnref64" class="footnote-back">↩︎</a></p></li>
<li id="fn65"><p>In more complex models, this code can be used to create pair-wise comparisons on one of many explanatory variables.<a href="chapter3.html#fnref65" class="footnote-back">↩︎</a></p></li>
<li id="fn66"><p>The plot of results usually contains all the labels of groups but if the labels are long or there many groups, sometimes the row labels are hard to see even with re-sizing the plot to make it taller in RStudio. The numerical output is
useful as a guide to help you read the plot in those situations.<a href="chapter3.html#fnref66" class="footnote-back">↩︎</a></p></li>
<li id="fn67"><p>Note that this method is implemented slightly differently than explained here in some software packages so if you see this in a journal article, read the discussion carefully.<a href="chapter3.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>There is a warning message produced by the default Tukey’s code here related to the algorithms used to generate approximate p-values and then the CLD, but the results seem reasonable and just a few p-values seem to vary in the second or third decimal points.<a href="chapter3.html#fnref68" class="footnote-back">↩︎</a></p></li>
<li id="fn69"><p>We would not suggest throwing away observations to get balanced designs. Plan in advance to try to have a balanced design but analyze the responses you get.<a href="chapter4.html#fnref69" class="footnote-back">↩︎</a></p></li>
<li id="fn70"><p>Copy and include this code in a codechunk any time you want to use the <code>intplot</code> or <code>inplotarray</code> functions.<a href="chapter4.html#fnref70" class="footnote-back">↩︎</a></p></li>
<li id="fn71"><p>We will use “main effects” to refer to the two
explanatory variables in the additive model even if they are not randomly
assigned to contrast with having those variables interacting in the model.
It is the one place in the book where we use “effects” without worrying about the causal connotation of that word.<a href="chapter4.html#fnref71" class="footnote-back">↩︎</a></p></li>
<li id="fn72"><p>In the standard
ANOVA table,
<span class="math inline">\(\text{SS}_A + \text{SS}_B + \text{SS}_{AB} + \text{SS}_E = \text{SS}_{\text{Total}}\)</span>.
However, to get the tests we really desire when our designs are not balanced, a
slight modification of the SS is used, using what are called Type II sums of
squares and this result doesn’t hold in the output you will see for additive
models. This is discussed further below.<a href="chapter4.html#fnref72" class="footnote-back">↩︎</a></p></li>
<li id="fn73"><p>The <code>anova</code> results are not wrong, just not what we want in all situations.<a href="chapter4.html#fnref73" class="footnote-back">↩︎</a></p></li>
<li id="fn74"><p>Actually, the tests are only conditional
on other main effects if Type II Sums of Squares are used for an interaction
model, but we rarely focus on the main effect tests when the interaction is present.<a href="chapter4.html#fnref74" class="footnote-back">↩︎</a></p></li>
<li id="fn75"><p>In Multiple Linear Regression models in
Chapter <a href="chapter8.html#chapter8">8</a>, the reasons for this wording will (hopefully)
become clearer.<a href="chapter4.html#fnref75" class="footnote-back">↩︎</a></p></li>
<li id="fn76"><p>Just so you don’t think
that perfect R code should occur on the first try, I
have made similarly serious coding mistakes even after accumulating more
than decade of experience with R. It is finding those mistakes (in time) that matters.<a href="chapter4.html#fnref76" class="footnote-back">↩︎</a></p></li>
<li id="fn77"><p>To get <code>dosef</code> on the x-axis in the plot, the <code>x.var="dosef"</code> option was employed to force the <em>Dose</em> to be the variable on the x-axis.<a href="chapter4.html#fnref77" class="footnote-back">↩︎</a></p></li>
<li id="fn78"><p>We switched back to the <code>anova</code> function here as the <code>Anova</code> function only reports <code>Error in Anova.lm(lm(responses ~ dropsf * brand, data = ptR)) :    residual df = 0</code>, which is fine but not as useful for understanding this issue as what <code>anova</code> provides.<a href="chapter4.html#fnref78" class="footnote-back">↩︎</a></p></li>
<li id="fn79"><p>In larger data sets,
multiple subjects are displayed in each row as proportions of
the rows in each category.<a href="chapter4.html#fnref79" class="footnote-back">↩︎</a></p></li>
<li id="fn80"><p>Quantitative variables are displayed with
boxplot-like bounds to describe the variability in the variable for that row of
responses for larger data sets.<a href="chapter4.html#fnref80" class="footnote-back">↩︎</a></p></li>
<li id="fn81"><p>While randomization is typically useful in trying to “equalize” the composition of groups, a possible randomization of subjects to the groups is to put all the males into the treatment group. Sometimes we add additional constraints to randomization of subjects to treatments to guarantee that we don’t get stuck with an unusual and highly unlikely assignment like that. It is important at least to check the demographics of different treatment groups to see if anything odd occurred.<a href="chapter5.html#fnref81" class="footnote-back">↩︎</a></p></li>
<li id="fn82"><p>The vertical line, “<code>|</code>”, in <code>~ y|x</code>
is available on most keyboards on the same key as “<code>\</code>”. It is the mathematical
symbol that means “conditional on” whatever follows.<a href="chapter5.html#fnref82" class="footnote-back">↩︎</a></p></li>
<li id="fn83"><p>Standardizing involves dividing by the standard deviation
of a quantity so it has a standard deviation 1 regardless of its original
variability and that is what is happening here even though it doesn’t
look like the standardization you are used to with continuous variables.<a href="chapter5.html#fnref83" class="footnote-back">↩︎</a></p></li>
<li id="fn84"><p>Note that in smaller data sets to get results as discussed here, use the <code>correct=F</code> option. If you get output that contains “<code>...with Yate's continuity correction</code>”, a slightly modified version of this test is being used.<a href="chapter5.html#fnref84" class="footnote-back">↩︎</a></p></li>
<li id="fn85"><p>Here it allows us to relax
a requirement that all the expected cell counts are larger than 5 for the
parametric test (Section <a href="chapter5.html#section5-6">5.6</a>).<a href="chapter5.html#fnref85" class="footnote-back">↩︎</a></p></li>
<li id="fn86"><p>Doctors are faced with this exact dilemma – with little more training than you have now in statistics, they read a result like this in a paper and used to be encouraged to focus on the p-value to decide about treatment recommendations. Would you recommend the treatment here just based on the small p-value? Would having Figure <a href="chapter5.html#fig:Figure5-11">5.11</a> to go with the small p-value help you make a more educated decision? Now the recommendations are starting to move past just focusing on the p-values and thinking about the practical importance and size of the differences. The potential benefits of a treatment need to be balanced with risks of complications too, but that takes us back into discussing having multiple analyses in the same study (treatment improvement, complications/not, etc.).<a href="chapter5.html#fnref86" class="footnote-back">↩︎</a></p></li>
<li id="fn87"><p>Independence tests can’t
be causal by their construction. Homogeneity tests could be causal or just
associational, depending on how the subjects ended up in the groups.<a href="chapter5.html#fnref87" class="footnote-back">↩︎</a></p></li>
<li id="fn88"><p>A stratified random sample
involves taking a simple random sample from each group or strata of the
population.

It is useful to make sure that each group is represented at a
chosen level (for example the sample proportion of the total
size). If a simple random sample of all schools had been taken, it is possible
that a level could have no schools selected.<a href="chapter5.html#fnref88" class="footnote-back">↩︎</a></p></li>
<li id="fn89"><p>There are measures
of correlation between categorical variables but when
statisticians say correlation they mean correlation of quantitative variables.
If they are discussing correlations of other types, they will make that clear.<a href="chapter6.html#fnref89" class="footnote-back">↩︎</a></p></li>
<li id="fn90"><p>Some of the details of this study have been lost,
so we will assume that the
subjects were randomly assigned and that a beer means a regular sized can of beer and that
the beer was of regular strength. We don’t know if any of that is actually true.
It would be nice to repeat this study to know more details and possibly have a larger sample size but I doubt if our institutional review board would allow students to drink as much as 9 beers.<a href="chapter6.html#fnref90" class="footnote-back">↩︎</a></p></li>
<li id="fn91"><p>I added
<code>pch=16, col=30</code> to fill in the default circles and make the points in something
other than black, an entirely unnecessary addition here.<a href="chapter6.html#fnref91" class="footnote-back">↩︎</a></p></li>
<li id="fn92"><p>This
interface with the <code>cor</code> function only works after you load the
<code>mosaic</code> package.<a href="chapter6.html#fnref92" class="footnote-back">↩︎</a></p></li>
<li id="fn93"><p>The
natural log (<span class="math inline">\(\log_e\)</span> or <span class="math inline">\(\ln\)</span>) is used in statistics so much that the
function in R <code>log</code> actually takes the natural log and if you want a
<span class="math inline">\(\log_{10}\)</span> you have to use the function <code>log10</code>. When statisticians
say log we mean natural log.<a href="chapter6.html#fnref93" class="footnote-back">↩︎</a></p></li>
<li id="fn94"><p>This is related to what is called Simpson’s paradox, where the overall analysis (ignoring a grouping variable) leads to a conclusion of a relationship in one direction, but when the relationship is broken down into subgroups it is in the opposite direction in each group.  This emphasizes the importance of checking and accounting for differences in groups and the more complex models we are setting the stage to consider in the coming chapters.<a href="chapter6.html#fnref94" class="footnote-back">↩︎</a></p></li>
<li id="fn95"><p>The interval is “far” from the reference value under the null (0) so this provides at least strong evidence. With using confidence intervals for tests, we really don’t know much about the strength of evidence against the null hypothesis but the hypothesis test here is a bit more complicated to construct and understand and we will have to tolerate just having crude information about the p-value to assess strength of evidence.<a href="chapter6.html#fnref95" class="footnote-back">↩︎</a></p></li>
<li id="fn96"><p>Observations at the edge of the <span class="math inline">\(x\text{&#39;s}\)</span> will be called high
leverage points in Section <a href="chapter6.html#section6-9">6.9</a>; this point is a low leverage point
because it is close to mean of the <span class="math inline">\(x\text{&#39;s}\)</span>.<a href="chapter6.html#fnref96" class="footnote-back">↩︎</a></p></li>
<li id="fn97"><p>Even with clear scientific logic, we sometimes make choices
to flip the model directions to facilitate different types of analyses. In
<span class="citation">Vsevolozhskaya et al. (<a href="#ref-Vsevol2014" role="doc-biblioref">2014</a>)</span> we looked at genomic differences based on obesity groups, even
though we were really interested in exploring how gene-level differences
explained differences in obesity.<a href="chapter6.html#fnref97" class="footnote-back">↩︎</a></p></li>
<li id="fn98"><p>The
residuals from these methods and ANOVA are the same because they all come
from linear models but are completely different from the standardized residuals
used in the Chi-square material in Chapter <a href="chapter5.html#chapter5">5</a>.<a href="chapter6.html#fnref98" class="footnote-back">↩︎</a></p></li>
<li id="fn99"><p>We can also write this as
<span class="math inline">\(E(y_i|x_i) = \mu\{y_i|x_i\} = \beta_0 + \beta_1x_i\)</span>, which is the notation you
will see in books like the <em>Statistical Sleuth</em> <span class="citation">(Ramsey and Schafer <a href="#ref-Ramsey2012" role="doc-biblioref">2012</a>)</span>. We will
use notation that is consistent with how we originally introduced the methods.<a href="chapter7.html#fnref99" class="footnote-back">↩︎</a></p></li>
<li id="fn100"><p>There is an area of statistical research on how to
optimally choose <span class="math inline">\(x\)</span>-values to
get the most precise estimate of a slope coefficient. In observational studies
we have to deal with whatever pattern of <span class="math inline">\(x\text{&#39;s}\)</span> we ended up with. If you can
choose, generate an even spread of <span class="math inline">\(x\text{&#39;s}\)</span> over some range of
interest similar to what was used in the <em>Beers</em> vs <em>BAC</em> study to provide
the best distribution of values to discover the relationship across the
selected range of <span class="math inline">\(x\)</span>-values.<a href="chapter7.html#fnref100" class="footnote-back">↩︎</a></p></li>
<li id="fn101"><p>See
<a href="http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/" class="uri">http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/</a>
for an interesting discussion of
weather variability where Great Falls, MT had a very high rating on
“unpredictability”.<a href="chapter7.html#fnref101" class="footnote-back">↩︎</a></p></li>
<li id="fn102"><p>It is actually pretty
amazing that there are hundreds of locations with nearly complete daily
records for over 100 years.<a href="chapter7.html#fnref102" class="footnote-back">↩︎</a></p></li>
<li id="fn103"><p>All joking aside, if researchers can find evidence of climate
change using <strong><em>conservative</em></strong> methods (methods that reject the null
hypothesis when it is true less often than stated), then their results are
even harder to ignore.<a href="chapter7.html#fnref103" class="footnote-back">↩︎</a></p></li>
<li id="fn104"><p>It took many permutations
to get competitor plots this close to the real data set and they really
aren’t that close.<a href="chapter7.html#fnref104" class="footnote-back">↩︎</a></p></li>
<li id="fn105"><p>If
the removal is of a point that is extreme in <span class="math inline">\(x\)</span>-values, then it is appropriate
to note that the results only apply to the restricted range of <span class="math inline">\(x\)</span>-values that
were actually analyzed in the scope of inference discussion. Our results only ever apply to the range of <span class="math inline">\(x\)</span>-values we
had available so this is a relatively minor change.<a href="chapter7.html#fnref105" class="footnote-back">↩︎</a></p></li>
<li id="fn106"><p>Note <code>exp(x)</code> is the same as <span class="math inline">\(e^{(x)}\)</span> but easier
to read in-line and <code>exp()</code> is the R function name to execute this calculation.<a href="chapter7.html#fnref106" class="footnote-back">↩︎</a></p></li>
<li id="fn107"><p>You can read my dissertation if you want my take on modeling U and V-shaped valley elevation profiles that included some discussion of these models and some related discussions in <span class="citation">Greenwood and Humphrey (<a href="#ref-Greenwood2002" role="doc-biblioref">2002</a>)</span>.<a href="chapter7.html#fnref107" class="footnote-back">↩︎</a></p></li>
<li id="fn108"><p>This transformation could not be applied directly to the education growth score data in Chapter <a href="chapter5.html#chapter5">5</a> because there were negative “growth” scores.<a href="chapter7.html#fnref108" class="footnote-back">↩︎</a></p></li>
<li id="fn109"><p>This silly
nomenclature was inspired by <span class="citation">De Veaux, Velleman, and Bock (<a href="#ref-DeVeaux2011" role="doc-biblioref">2011</a>)</span> <em>Stats: Data and
Models</em> text. If you find this too cheesy, you can just call it x-vee.<a href="chapter7.html#fnref109" class="footnote-back">↩︎</a></p></li>
<li id="fn110"><p>I have suppressed
some of the code for making plots in this and the next chapter to make
“pretty” pictures - which you probably are happy to not see it until you want
to make a pretty plot on your own. All the code used is available upon
request.<a href="chapter7.html#fnref110" class="footnote-back">↩︎</a></p></li>
<li id="fn111"><p>I have really enjoyed writing this book but kind of hope to at least have updated this data set before 2050.<a href="chapter7.html#fnref111" class="footnote-back">↩︎</a></p></li>
<li id="fn112"><p>If you take advanced applied mathematics courses, you can learn
more about the algorithms being used by <code>lm</code>.
Everyone else only cares
about the algorithms when they don’t work – which is usually due to the
user’s inputs in these models not the algorithm itself.<a href="chapter8.html#fnref112" class="footnote-back">↩︎</a></p></li>
<li id="fn113"><p>Sometimes the <code>effects</code> plots
ignores the edge explanatory observations with the
default display. Always check the original variable summaries when considering
the range of observed values. By turning on the the “partial residuals” with SLR models, the plots show the original observations along with the fitted values and 95% confidence interval band. In more complex models, these displays with residuals are more complicated but can be used to assess linearity with each predictor in the model after accounting for other variables.<a href="chapter8.html#fnref113" class="footnote-back">↩︎</a></p></li>
<li id="fn114"><p>We used this same notation in the fitting the additive Two-Way
ANOVA and this is also additive in terms of these variables. Interaction
models are discussed later in the chapter.<a href="chapter8.html#fnref114" class="footnote-back">↩︎</a></p></li>
<li id="fn115"><p>I have not given you a formula for calculating partial residuals. We will leave that for more advanced material.<a href="chapter8.html#fnref115" class="footnote-back">↩︎</a></p></li>
<li id="fn116"><p>Imagine
showing up to a ski area expecting a 40 inch base and there only being
11 inches. I’m sure ski areas are always more accurate than this model in their
reporting of amounts of snow on the ground…<a href="chapter8.html#fnref116" class="footnote-back">↩︎</a></p></li>
<li id="fn117"><p>The site name is redacted to protect
the innocence of the reader. More information on this site, located in
Beaverhead County, is available at
<a href="http://www.wcc.nrcs.usda.gov/nwcc/site?sitenum=355&amp;state=mt" class="uri">http://www.wcc.nrcs.usda.gov/nwcc/site?sitenum=355&amp;state=mt</a>.<a href="chapter8.html#fnref117" class="footnote-back">↩︎</a></p></li>
<li id="fn118"><p>They hold factor variables at their modal category but we don’t have factor variables in the MLR models, yet.<a href="chapter8.html#fnref118" class="footnote-back">↩︎</a></p></li>
<li id="fn119"><p>The <code>seq</code> function has syntax of
<code>seq(from=startingpoint, to=endingpoint, length.out=#ofvalues_between_start_and_end)</code> and the <code>rep</code> function has
syntax of <code>rep(numbertorepeat, #oftimes).</code><a href="chapter8.html#fnref119" class="footnote-back">↩︎</a></p></li>
<li id="fn120"><p>Also
see Section <a href="chapter8.html#section8-13">8.13</a> for another method of picking among different
models.<a href="chapter8.html#fnref120" class="footnote-back">↩︎</a></p></li>
<li id="fn121"><p>This section was inspired by a
similar section from <span class="citation">De Veaux, Velleman, and Bock (<a href="#ref-DeVeaux2011" role="doc-biblioref">2011</a>)</span>.<a href="chapter8.html#fnref121" class="footnote-back">↩︎</a></p></li>
<li id="fn122"><p>There are some social
science models where the model is fit with the mean subtracted from each
predictor so all have mean 0 and the precision of the <span class="math inline">\(y\)</span>-intercept is
interesting. In some cases both the response and predictor variables are “standardized” to have means of 0 and standard deviations of 1. The interpretations of coefficients then relates to changes in standard deviations around the means. These coefficients are called “standardized betas”.  But even in these models where the <span class="math inline">\(x\)</span>-values of 0 are of interest, the test for the <span class="math inline">\(y\)</span>-intercept being 0 is
rarely of interest.<a href="chapter8.html#fnref122" class="footnote-back">↩︎</a></p></li>
<li id="fn123"><p>Either
someone had a weighted GPA with bonus points, or more likely here, there
was a coding error in the data set since only one observation was over 4.0 in
the GPA data. Either way, we could remove it and note that our inferences for
HSGPA do not extend above 4.0.<a href="chapter8.html#fnref123" class="footnote-back">↩︎</a></p></li>
<li id="fn124"><p>When there
are just two predictors, the VIFs have to be the same since the
proportion of information shared is the same in both directions. With
more than two predictors, each variable can have a different VIF
value.<a href="chapter8.html#fnref124" class="footnote-back">↩︎</a></p></li>
<li id="fn125"><p>We
are actually making an educated guess about what these codes mean. Other
similar data sets used 1 for males but the documentation on these data is a bit
sparse. We proceed with a small potential that the conclusions regarding
differences in gender are in the wrong direction.<a href="chapter8.html#fnref125" class="footnote-back">↩︎</a></p></li>
<li id="fn126"><p>Some people also call them <strong><em>dummy variables</em></strong> to reflect that they are stand-ins for dealing with the categorical information. But it seems like a harsh anthropomorphism so I prefer “indicators”.<a href="chapter8.html#fnref126" class="footnote-back">↩︎</a></p></li>
<li id="fn127"><p>This is true for
additive uses of indicator variables. In Section <a href="chapter8.html#section8-11">8.11</a>, we
consider interactions between quantitative and categorical variables which has
the effect of changing slopes and intercepts. The simplification ideas to
produce estimated equations for each group are used there but we have to account
for changing slopes by group too.<a href="chapter8.html#fnref127" class="footnote-back">↩︎</a></p></li>
<li id="fn128"><p>Models like this with a categorical variable and quantitative variable are often called <em>ANCOVA</em> or <em>analysis of covariance</em> models but really are just versions of our linear models we’ve been using throughout this material.<a href="chapter8.html#fnref128" class="footnote-back">↩︎</a></p></li>
<li id="fn129"><p>Note that we employed some specific options in the <code>legend</code> option to get the legend to fit on this scatterplot better. Usually you can avoid this but the <code>coords</code> option defined a location and the <code>columns</code> option made it a two column legend. The <code>viridis(4)</code> code makes the plot in a suite of four colors from the <code>viridis</code> package <span class="citation">(Garnier <a href="#ref-R-viridis" role="doc-biblioref">2018</a>)</span>.<a href="chapter8.html#fnref129" class="footnote-back">↩︎</a></p></li>
<li id="fn130"><p>The strength of this recommendation drops when you have many predictors as you can’t do this for every variable, but the concern remains about an assumption of no interaction whenever you fit models without them. In more complex situations, think about variables that are most likely to interact in their impacts on the response based on the situation being studied and try to explore those.<a href="chapter8.html#fnref130" class="footnote-back">↩︎</a></p></li>
<li id="fn131"><p>Standardizing quantitative predictor variables is popular in social sciences, often where the response variable is also standardized. In those situations, they generate what are called “standardized betas” (<a href="https://en.wikipedia.org/wiki/Standardized_coefficient" class="uri">https://en.wikipedia.org/wiki/Standardized_coefficient</a>) that estimate the change in SDs in the response for a 1 SD increase in the explanatory variable.<a href="chapter8.html#fnref131" class="footnote-back">↩︎</a></p></li>
<li id="fn132"><p>There is a way to test for a difference in the two lines at a particular <span class="math inline">\(x\)</span> value but it is beyond the scope of this material.<a href="chapter8.html#fnref132" class="footnote-back">↩︎</a></p></li>
<li id="fn133"><p>This is an example of what is called “step down” testing for model
refinement which is a commonly used technique for arriving at a final model to
describe response variables. Note that each step in the process should be
reported, not just the final model that only has variables with small p-values
remaining in it.<a href="chapter8.html#fnref133" class="footnote-back">↩︎</a></p></li>
<li id="fn134"><p>We could also use the <code>anova</code> function to do this but using <code>Anova</code>
throughout this material provides the answers we want in the additive model and
it has no impact for the only test of interest in the interaction model since
the interaction is the last component in the model.<a href="chapter8.html#fnref134" class="footnote-back">↩︎</a></p></li>
<li id="fn135"><p>In most situations, it would be crazy
to assume that the true model for a
process has been obtained so we can never pick the “correct” model. In fact, we
won’t even know if we are picking a “good” model, but just the best from a set
of the candidate models on a criterion. But we can study the general performance of methods
using simulations where we know the true model and the AIC has some useful
properties in identifying the correct model when it is in the candidate set of
models. No such similar theory exists for the adjusted <strong><em>R</em></strong><sup>2</sup>.<a href="chapter8.html#fnref135" class="footnote-back">↩︎</a></p></li>
<li id="fn136"><p>Most people now call this
Akaike’s (pronounced <strong>ah-kah-ee-kay</strong>) Information Criterion, but he used the
AIC nomenclature to mean An Information Criterion – he was not so vain as to
name the method after himself in the original paper that proposed it. But it is now common to use “A” for his last name.<a href="chapter8.html#fnref136" class="footnote-back">↩︎</a></p></li>
<li id="fn137"><p>More details on these components of the methods will be
left for more advanced material - we will focus on an introduction to using the AIC measure here.<a href="chapter8.html#fnref137" class="footnote-back">↩︎</a></p></li>
<li id="fn138"><p>Although sometimes excluded, the count of
parameters should include counting the residual variance as a parameter.<a href="chapter8.html#fnref138" class="footnote-back">↩︎</a></p></li>
<li id="fn139"><p>We put quotes on “full” or sometimes
call it the “fullish” model because we could always add more to the model, like
interactions or other explanatory variables. So we rarely have a completely full model but
we do have our “most complicated that we are considering” model.<a href="chapter8.html#fnref139" class="footnote-back">↩︎</a></p></li>
<li id="fn140"><p>The options in <code>extra=...</code> are to get extra information displayed that you do not necessarily need. You can simply run <code>dredge(m6,rank="AIC")</code> to get just the AIC results.<a href="chapter8.html#fnref140" class="footnote-back">↩︎</a></p></li>
<li id="fn141"><p>The researchers did not
do this analysis so never directly addressed this research question although
they did discuss it in general ways.<a href="chapter9.html#fnref141" class="footnote-back">↩︎</a></p></li>
<li id="fn142"><p>Instructors often get asked what a problem with
non-constant variance actually looks like – this is a perfect example of it!<a href="chapter9.html#fnref142" class="footnote-back">↩︎</a></p></li>
<li id="fn143"><p>This was not even close to their top AIC model so they made an odd
choice.<a href="chapter9.html#fnref143" class="footnote-back">↩︎</a></p></li>
<li id="fn144"><p>I had students read this
paper in a class and one decided that this was a reasonable way to report small
p-values – it is WRONG. We are interested in how small a p-value might be and
saying it is over a value is never useful, especially if you say it is larger than a tiny number.<a href="chapter9.html#fnref144" class="footnote-back">↩︎</a></p></li>
<li id="fn145"><p>All too often, I read journal articles that have under-utilized,
under-reported, mis-applied, or mis-interpreted statistical methods and
results. One of the reasons that I wanted to write this book was to help more
people move from basic statistical knowledge to correct use of intermediate
statistical methods and beginning to see the potential in more advanced
statistical methods. It took me many years of being a statistician (after getting a PhD) just to
feel armed for battle when confronted with new applications and two stat
courses are not enough to get you there, but you have to start somewhere. You
are only maybe two or three hundred hours into your 10,000 hours required for mastery.
This book is intended get you some solid fundamentals to build on or a
few intermediate tools to use if this is your last statistics training
experience.<a href="chapter9.html#fnref145" class="footnote-back">↩︎</a></p></li>
<li id="fn146"><p>They also had an error in their AIC
results that is difficult to explain here but was due to an un-careful usage of
the results from the more advanced models that account for autocorrelation,
which seems to provide the proper ranking of models (<em>that they ignored</em>) but
did not provide the correct differences among models.<a href="chapter9.html#fnref146" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-DeVeaux2011">
<p>De Veaux, Richard D., Paul F. Velleman, and David E. Bock. 2011. <em>Stats: Data and Models, 3rd Edition</em>. Pearson.</p>
</div>
<div id="ref-R-viridis">
<p>Garnier, Simon. 2018. <em>Viridis: Default Color Maps from ’Matplotlib’</em>. <a href="https://CRAN.R-project.org/package=viridis">https://CRAN.R-project.org/package=viridis</a>.</p>
</div>
<div id="ref-Greenwood2002">
<p>Greenwood, Mark C., and N. F. Humphrey. 2002. “Glaciated Valley Profiles: An Application of Nonlinear Regression.” <em>Computing Science and Statistics</em> 34: 452–60.</p>
</div>
<div id="ref-Kampstra2008">
<p>Kampstra, Peter. 2008. “Beanplot: A Boxplot Alternative for Visual Comparison of Distributions.” <em>Journal of Statistical Software, Code Snippets</em> 28 (1): 1–9. <a href="http://www.jstatsoft.org/v28/c01/">http://www.jstatsoft.org/v28/c01/</a>.</p>
</div>
<div id="ref-Ramsey2012">
<p>Ramsey, Fred, and Daniel Schafer. 2012. <em>The Statistical Sleuth: A Course in Methods of Data Analysis</em>. Cengage Learning. <a href="https://books.google.com/books?id=eSlLjA9TwkUC">https://books.google.com/books?id=eSlLjA9TwkUC</a>.</p>
</div>
<div id="ref-Vsevol2014">
<p>Vsevolozhskaya, Olga A., Dmitri V. Zaykin, Mark C. Greenwood, Changshuai Wei, and Qing Lu. 2014. “Functional Analysis of Variance for Association Studies.” <em>PLOS ONE</em> 9 (9): 13. <a href="http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0105074">http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0105074</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter9.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
