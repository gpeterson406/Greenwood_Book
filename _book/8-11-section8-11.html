<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta property="og:title" content="A Second Semester Statistics Course with R" />
<meta property="og:type" content="book" />



<meta name="github-repo" content="gpeterson406/Greenwood_Book" />

<meta name="author" content="Mark C Greenwood" />


<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>

<meta name="description" content="A Second Semester Statistics Course with R">

<title>A Second Semester Statistics Course with R</title>

<script src="libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="libs/navigation-1.1/tabsets.js"></script>


<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link rel="stylesheet" href="toc.css" type="text/css" />

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
</style>
</head>

<body>

<div class="container-fluid main-container">


<div class="row">
<div class="col-sm-12">
<div id="TOC">
<ul>
<li><a href="index.html#cover">Cover</a></li>
<li><a href="acknowledgments.html#acknowledgments">Acknowledgments</a></li>
<li class="has-sub"><a href="1-chapter1.html#chapter1"><span class="toc-section-number">1</span> Preface</a><ul>
<li><a href="1-1-section1-1.html#section1-1"><span class="toc-section-number">1.1</span> Overview of methods</a></li>
<li><a href="1-2-section1-2.html#section1-2"><span class="toc-section-number">1.2</span> Getting started in R</a></li>
<li><a href="1-3-section1-3.html#section1-3"><span class="toc-section-number">1.3</span> Basic summary statistics, histograms, and boxplots using R</a></li>
<li><a href="1-4-section1-4.html#section1-4"><span class="toc-section-number">1.4</span> Chapter summary</a></li>
<li><a href="1-5-section1-5.html#section1-5"><span class="toc-section-number">1.5</span> Summary of important R code</a></li>
<li><a href="1-6-section1-6.html#section1-6"><span class="toc-section-number">1.6</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="2-chapter2.html#chapter2"><span class="toc-section-number">2</span> (R)e-Introduction to statistics</a><ul>
<li><a href="2-1-section2-1.html#section2-1"><span class="toc-section-number">2.1</span> Histograms, boxplots, and density curves</a></li>
<li><a href="2-2-section2-2.html#section2-2"><span class="toc-section-number">2.2</span> Beanplots</a></li>
<li><a href="2-3-section2-3.html#section2-3"><span class="toc-section-number">2.3</span> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li><a href="2-4-section2-4.html#section2-4"><span class="toc-section-number">2.4</span> Permutation testing for the two sample mean situation</a></li>
<li><a href="2-5-section2-5.html#section2-5"><span class="toc-section-number">2.5</span> Hypothesis testing (general)</a></li>
<li><a href="2-6-section2-6.html#section2-6"><span class="toc-section-number">2.6</span> Connecting randomization (nonparametric) and parametric tests</a></li>
<li><a href="2-7-section2-7.html#section2-7"><span class="toc-section-number">2.7</span> Second example of permutation tests</a></li>
<li><a href="2-8-section2-8.html#section2-8"><span class="toc-section-number">2.8</span> Confidence intervals and bootstrapping</a></li>
<li><a href="2-9-section2-9.html#section2-9"><span class="toc-section-number">2.9</span> Bootstrap confidence intervals for difference in GPAs</a></li>
<li><a href="2-10-section2-10.html#section2-10"><span class="toc-section-number">2.10</span> Chapter summary</a></li>
<li><a href="2-11-section2-11.html#section2-11"><span class="toc-section-number">2.11</span> Summary of important R code</a></li>
<li><a href="2-12-section2-12.html#section2-12"><span class="toc-section-number">2.12</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="3-chapter3.html#chapter3"><span class="toc-section-number">3</span> One-Way ANOVA</a><ul>
<li><a href="3-1-section3-1.html#section3-1"><span class="toc-section-number">3.1</span> Situation</a></li>
<li><a href="3-2-section3-2.html#section3-2"><span class="toc-section-number">3.2</span> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li><a href="3-3-section3-3.html#section3-3"><span class="toc-section-number">3.3</span> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li><a href="3-4-section3-4.html#section3-4"><span class="toc-section-number">3.4</span> ANOVA model diagnostics including QQ-plots</a></li>
<li><a href="3-5-section3-5.html#section3-5"><span class="toc-section-number">3.5</span> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li><a href="3-6-section3-6.html#section3-6"><span class="toc-section-number">3.6</span> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li><a href="3-7-section3-7.html#section3-7"><span class="toc-section-number">3.7</span> Pair-wise comparisons for Prisoner Rating data</a></li>
<li><a href="3-8-section3-8.html#section3-8"><span class="toc-section-number">3.8</span> Chapter summary</a></li>
<li><a href="3-9-section3-9.html#section3-9"><span class="toc-section-number">3.9</span> Summary of important R code</a></li>
<li><a href="3-10-section3-10.html#section3-10"><span class="toc-section-number">3.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="4-chapter4.html#chapter4"><span class="toc-section-number">4</span> Two-Way ANOVA</a><ul>
<li><a href="4-1-section4-1.html#section4-1"><span class="toc-section-number">4.1</span> Situation</a></li>
<li><a href="4-2-section4-2.html#section4-2"><span class="toc-section-number">4.2</span> Designing a two-way experiment and visualizing results</a></li>
<li><a href="4-3-section4-3.html#section4-3"><span class="toc-section-number">4.3</span> Two-Way ANOVA models and hypothesis tests</a></li>
<li><a href="4-4-section4-4.html#section4-4"><span class="toc-section-number">4.4</span> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li><a href="4-5-section4-5.html#section4-5"><span class="toc-section-number">4.5</span> Observational study example: The Psychology of Debt</a></li>
<li><a href="4-6-section4-6.html#section4-6"><span class="toc-section-number">4.6</span> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li><a href="4-7-section4-7.html#section4-7"><span class="toc-section-number">4.7</span> Chapter summary</a></li>
<li><a href="4-8-section4-8.html#section4-8"><span class="toc-section-number">4.8</span> Summary of important R code</a></li>
<li><a href="4-9-section4-9.html#section4-9"><span class="toc-section-number">4.9</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="5-chapter5.html#chapter5"><span class="toc-section-number">5</span> Chi-square tests</a><ul>
<li><a href="5-1-section5-1.html#section5-1"><span class="toc-section-number">5.1</span> Situation, contingency tables, and table plots</a></li>
<li><a href="5-2-section5-2.html#section5-2"><span class="toc-section-number">5.2</span> Homogeneity test hypotheses</a></li>
<li><a href="5-3-section5-3.html#section5-3"><span class="toc-section-number">5.3</span> Independence test hypotheses</a></li>
<li><a href="5-4-section5-4.html#section5-4"><span class="toc-section-number">5.4</span> Models for R by C tables</a></li>
<li><a href="5-5-section5-5.html#section5-5"><span class="toc-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-6-section5-6.html#section5-6"><span class="toc-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li><a href="5-7-section5-7.html#section5-7"><span class="toc-section-number">5.7</span> Examining residuals for the source of differences</a></li>
<li><a href="5-8-section5-8.html#section5-8"><span class="toc-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li><a href="5-9-section5-9.html#section5-9"><span class="toc-section-number">5.9</span> Political party and voting results: Complete analysis</a></li>
<li><a href="5-10-section5-10.html#section5-10"><span class="toc-section-number">5.10</span> Is cheating and lying related in students?</a></li>
<li><a href="5-11-section5-11.html#section5-11"><span class="toc-section-number">5.11</span> Analyzing a stratified random sample of California schools</a></li>
<li><a href="5-12-section5-12.html#section5-12"><span class="toc-section-number">5.12</span> Chapter summary</a></li>
<li><a href="5-13-section5-13.html#section5-13"><span class="toc-section-number">5.13</span> Summary of important R commands</a></li>
<li><a href="5-14-section5-14.html#section5-14"><span class="toc-section-number">5.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="6-chapter6.html#chapter6"><span class="toc-section-number">6</span> Correlation and Simple Linear Regression</a><ul>
<li><a href="6-1-section6-1.html#section6-1"><span class="toc-section-number">6.1</span> Relationships between two quantitative variables</a></li>
<li><a href="6-2-section6-2.html#section6-2"><span class="toc-section-number">6.2</span> Estimating the correlation coefficient</a></li>
<li><a href="6-3-section6-3.html#section6-3"><span class="toc-section-number">6.3</span> Relationships between variables by groups</a></li>
<li><a href="6-4-section6-4.html#section6-4"><span class="toc-section-number">6.4</span> Inference for the correlation coefficient (Optional section)</a></li>
<li><a href="6-5-section6-5.html#section6-5"><span class="toc-section-number">6.5</span> Are tree diameters related to tree heights?</a></li>
<li><a href="6-6-section6-6.html#section6-6"><span class="toc-section-number">6.6</span> Describing relationships with a regression model</a></li>
<li><a href="6-7-section6-7.html#section6-7"><span class="toc-section-number">6.7</span> Least Squares Estimation</a></li>
<li><a href="6-8-section6-8.html#section6-8"><span class="toc-section-number">6.8</span> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li><a href="6-9-section6-9.html#section6-9"><span class="toc-section-number">6.9</span> Outliers: leverage and influence</a></li>
<li><a href="6-10-section6-10.html#section6-10"><span class="toc-section-number">6.10</span> Residual diagnostics – setting the stage for inference</a></li>
<li><a href="6-11-section6-11.html#section6-11"><span class="toc-section-number">6.11</span> Old Faithful discharge and waiting times</a></li>
<li><a href="6-12-section6-12.html#section6-12"><span class="toc-section-number">6.12</span> Chapter summary</a></li>
<li><a href="6-13-section6-13.html#section6-13"><span class="toc-section-number">6.13</span> Summary of important R code</a></li>
<li><a href="6-14-section6-14.html#section6-14"><span class="toc-section-number">6.14</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="7-chapter7.html#chapter7"><span class="toc-section-number">7</span> Simple linear regression inference</a><ul>
<li><a href="7-1-section7-1.html#section7-1"><span class="toc-section-number">7.1</span> Model</a></li>
<li><a href="7-2-section7-2.html#section7-2"><span class="toc-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li><a href="7-3-section7-3.html#section7-3"><span class="toc-section-number">7.3</span> Bozeman temperature trend</a></li>
<li><a href="7-4-section7-4.html#section7-4"><span class="toc-section-number">7.4</span> Randomizing inferences for the slope coefficient</a></li>
<li><a href="7-5-section7-5.html#section7-5"><span class="toc-section-number">7.5</span> Transformations part I: Linearizing relationships</a></li>
<li><a href="7-6-section7-6.html#section7-6"><span class="toc-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li><a href="7-7-section7-7.html#section7-7"><span class="toc-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li><a href="7-8-section7-8.html#section7-8"><span class="toc-section-number">7.8</span> Chapter summary</a></li>
<li><a href="7-9-section7-9.html#section7-9"><span class="toc-section-number">7.9</span> Summary of important R code</a></li>
<li><a href="7-10-section7-10.html#section7-10"><span class="toc-section-number">7.10</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="8-chapter8.html#chapter8"><span class="toc-section-number">8</span> Multiple linear regression</a><ul>
<li><a href="8-1-section8-1.html#section8-1"><span class="toc-section-number">8.1</span> Going from SLR to MLR</a></li>
<li><a href="8-2-section8-2.html#section8-2"><span class="toc-section-number">8.2</span> Validity conditions in MLR</a></li>
<li><a href="8-3-section8-3.html#section8-3"><span class="toc-section-number">8.3</span> Interpretation of MLR terms</a></li>
<li><a href="8-4-section8-4.html#section8-4"><span class="toc-section-number">8.4</span> Comparing multiple regression models</a></li>
<li><a href="8-5-section8-5.html#section8-5"><span class="toc-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</a></li>
<li><a href="8-6-section8-6.html#section8-6"><span class="toc-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</a></li>
<li><a href="8-7-section8-7.html#section8-7"><span class="toc-section-number">8.7</span> Overall F-test in multiple linear regression</a></li>
<li><a href="8-8-section8-8.html#section8-8"><span class="toc-section-number">8.8</span> Case study: First year college GPA and SATs</a></li>
<li><a href="8-9-section8-9.html#section8-9"><span class="toc-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</a></li>
<li><a href="8-10-section8-10.html#section8-10"><span class="toc-section-number">8.10</span> Additive MLR with more than two groups: Headache example</a></li>
<li><a href="8-11-section8-11.html#section8-11"><span class="toc-section-number">8.11</span> Different slopes and different intercepts</a></li>
<li><a href="8-12-section8-12.html#section8-12"><span class="toc-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li><a href="8-13-section8-13.html#section8-13"><span class="toc-section-number">8.13</span> AICs for model selection</a></li>
<li><a href="8-14-section8-14.html#section8-14"><span class="toc-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</a></li>
<li><a href="8-15-section8-15.html#section8-15"><span class="toc-section-number">8.15</span> Chapter summary</a></li>
<li><a href="8-16-section8-16.html#section8-16"><span class="toc-section-number">8.16</span> Summary of important R code</a></li>
<li><a href="8-17-section8-17.html#section8-17"><span class="toc-section-number">8.17</span> Practice problems</a></li>
</ul></li>
<li class="has-sub"><a href="9-chapter9.html#chapter9"><span class="toc-section-number">9</span> Case studies</a><ul>
<li><a href="9-1-section9-1.html#section9-1"><span class="toc-section-number">9.1</span> Overview of material covered</a></li>
<li><a href="9-2-section9-2.html#section9-2"><span class="toc-section-number">9.2</span> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li><a href="9-3-section9-3.html#section9-3"><span class="toc-section-number">9.3</span> Ants learn to rely on more informative attributes during decision-making</a></li>
<li><a href="9-4-section9-4.html#section9-4"><span class="toc-section-number">9.4</span> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li><a href="9-5-section9-5.html#section9-5"><span class="toc-section-number">9.5</span> General summary</a></li>
</ul></li>
<li><a href="references.html#references">References</a></li>
</ul>
</div>
</div>
</div>
<div class="row">
<div class="col-sm-12">
<div id="section8-11" class="section level2">
<h2><span class="header-section-number">8.11</span> Different slopes and different intercepts</h2>
<p>Sometimes researchers are specifically interested in whether the slopes vary across groups or the regression lines in the scatterplot for the different groups may not look parallel or it may just be hard to tell visually if there really is a difference in the slopes. Unless you are <strong>very sure</strong> that there is not an interaction between the grouping variable and the quantitative predictor, you should start by fitting a model containing an interaction and then see if you can drop it. It may be the case that you end up with the simpler additive model from the previous sections, but you don’t want to assume the same slope across groups unless you are absolutely sure that is the case. This should remind you a bit of the discussions of the additive and interaction models in the Two-way ANOVA material. The models, concerns, and techniques are very similar, but with the quantitative variable replacing one of the two categorical variables. As always, the scatterplot is a good first step to understanding whether we need the extra complexity that these models require.</p>
<p>A new example provides motivation for the consideration of different slopes and intercepts. A study was performed to address whether the relationship between nonverbal IQs and reading accuracy differs between dyslexic and non-dyslexic students. Two groups of students were identified, one group of <em>dyslexic</em> students was identified first (19 students) and then a group of gender and age similar student matches were identified (25 students) for a total sample size of <span class="math inline">\(n=44\)</span>, provided in the <code>dyslexic3</code> data set from the <code>smdata</code> package <span class="citation">(Merkle and Smithson <a href="#ref-R-smdata">2013</a>)</span>. This type of study design is an attempt to “balance” the data from the two groups on some important characteristics to make the comparisons of the groups as fair as possible. The researchers attempted to balance the characteristics of the subjects in the two groups so that if they found different results for the two groups, they could attribute it to the main difference they used to create the groups – dyslexia or not. This design, <strong><em>case-control</em></strong> or <strong>case-comparison</strong> where each subject with a trait is matched to one or more subjects in the “control” group would hopefully reduce confounding from other factors and then allow stronger conclusions in situations where it is impossible to randomly assign treatments to subjects. We still would avoid using “causal” language but this design is about as good as you can get when you are unable to randomly assign levels to subjects.</p>
<p>Using these data, we can explore the relationship between nonverbal IQ scores and reading accuracy, with reading accuracy measured as a proportion correct. The fact that there is an upper limit to the response variable attained by many students will cause complications below, but we can still learn something from our attempts to analyze these data using an MLR model. The scatterplot in Figure <a href="8-11-section8-11.html#fig:Figure8-25">2.164</a> seems to indicate some clear differences in the <em>IQ</em> vs <em>reading score</em> relationship between the <em>dys</em>=0 (non-dyslexic) and <em>dys</em>=1 (dyslexic) students. Note that the IQ is standardized to have mean 0 and standard deviation of 1 which means that a 1 unit change in IQ score is a 1 SD change and that the <em>y</em>-intercept (for <span class="math inline">\(x=0\)</span>) is right in the center of the plot and actually interesting<a href="#fn108" class="footnoteRef" id="fnref108"><sup>108</sup></a>.</p>

<div class="figure"><span id="fig:Figure8-25"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-25-1.png" alt="Scatterplot for reading score versus nonverbal IQ by dyslexia group." width="576" />
<p class="caption">
Figure 2.164: Scatterplot for reading score versus nonverbal IQ by dyslexia group.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(smdata)
<span class="kw">data</span>(<span class="st">&quot;dyslexic3&quot;</span>)
dyslexic3 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(dyslexic3)
<span class="kw">scatterplot</span>(score<span class="op">~</span>ziq<span class="op">|</span>dys, <span class="dt">xlab=</span><span class="st">&quot;Standardized nonverbal IQ scores&quot;</span>,
            <span class="dt">ylab=</span><span class="st">&quot;Reading score&quot;</span>, <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Plot of IQ vs Reading by dyslexia status&quot;</span>)</code></pre></div>
<p>To allow for both different y-intercepts and slope coefficients on the quantitative predictor, we need to include a “modification” of the slope coefficient. This is performed using an <strong><em>interaction</em></strong> between the two predictor variables where we allow the impacts of one variable (slopes) to change based on the levels of another variable (grouping variable). The formula notation is <code>y~x*group</code>, remembering that this also includes the <strong><em>main effects</em></strong> (the additive variable components) as well as the interaction coefficients as we discussed in the Two-Way ANOVA interaction model. We can start with the general model for a two-level categorical variable with an interaction, which is</p>
<p><span class="math display">\[y_i=\beta_0 + \beta_1x_i +\beta_2I_{\text{CatName},i} +
{\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i,\]</span></p>
<p>where the new component involves both the indicator and the quantitative predictor variable. The <span class="math inline">\(\color{red}{\boldsymbol{\beta_3}}\)</span> coefficient will be found in a row of output with <strong>both</strong> variable names in it (with the indicator level name) with a colon between them (something like <code>x:grouplevel</code>). As always, the best way to understand any model involving indicators is to plug in 0s or 1s for the indicator variable(s) and simplify the equations.</p>
<ul>
<li><p>For any observation in the baseline group <span class="math inline">\(I_{\text{CatName},i}=0\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
{\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>simplifies quickly to:</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\ .\]</span></p>
<ul>
<li>So the baseline group’s model involves the initial intercept and quantitative slope coefficient.</li>
</ul></li>
<li><p>For any observation in the second category <span class="math inline">\(I_{\text{CatName},i}=1\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
{\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>is</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2*1+
{\color{red}{\boldsymbol{\beta_3*1*x_i}}}+\varepsilon_i\]</span></p>
<p>which “simplifies” to</p>
<p><span class="math display">\[y_i = (\beta_0+\beta_2) + (\beta_1+{\color{red}{\boldsymbol{\beta_3}}})x_i
+\varepsilon_i,\]</span></p>
<p>by combining like terms.</p>
<ul>
<li>For the second category, the model contains a modified y-intercept, now <span class="math inline">\(\beta_0+\beta_2\)</span>, <strong>and</strong> a modified slope coefficient, now <span class="math inline">\(\beta_1+\color{red}{\boldsymbol{\beta_3}}\)</span>.</li>
</ul></li>
</ul>
<p>We can make this more concrete by applying this to the dyslexia data with <code>dys</code> as a categorical variable for dyslexia status of subjects (levels of 0 and 1) and <code>ziq</code> the standardized IQ. The estimated model is:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Because dys was just numerically coded - makes it a factor</span>
dyslexic3<span class="op">$</span>dys &lt;-<span class="st"> </span><span class="kw">factor</span>(dyslexic3<span class="op">$</span>dys) 
dys_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">*</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_model)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq * dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26362 -0.04152  0.01682  0.06790  0.17740 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.87586    0.02391  36.628  &lt; 2e-16
## ziq          0.05827    0.02535   2.299   0.0268
## dys1        -0.27951    0.03827  -7.304 7.11e-09
## ziq:dys1    -0.07285    0.03821  -1.907   0.0638
## 
## Residual standard error: 0.1017 on 40 degrees of freedom
## Multiple R-squared:  0.712,  Adjusted R-squared:  0.6904 
## F-statistic: 32.96 on 3 and 40 DF,  p-value: 6.743e-11</code></pre>
<p>The estimated model can be written as</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i - 0.280I_{\text{Level }1,i}
-{\color{red}{\boldsymbol{0.073}}}I_{\text{Level }1,i}\cdot\text{ZIQ}_i\]</span></p>
<p>and simplified for the two groups as:</p>
<ul>
<li><p>For the baseline (non-dyslexic, <span class="math inline">\(I_{\text{Level}1,i}=0\)</span>) students:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>For the deviation (dyslexic, <span class="math inline">\(I_{\text{Level1},i}=1\)</span>) students:</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{Score}}_i&amp;=0.876+0.058\cdot\text{ZIQ}_i - 0.280*1-
0.073*1\cdot\text{ZIQ}_i \\
&amp;=(0.876- 0.280) + (0.058-0.073)\cdot\text{ZIQ}_i, \\
\end{array}\]</span></p>
<p>which simplifies finally to:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.596-0.015\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>So the slope switched from 0.058 in the non-dyslexic students to -0.015 in the dyslexic students. The interpretations of these coefficients are outlined below:</p>
<ul>
<li><p>For the non-dyslexic students: For a 1 SD increase in verbal IQ score, we expect, on average, for the mean reading score to go up by 0.058 “points”.</p></li>
<li><p>For the dyslexic students: For a 1 SD increase in verbal IQ score, we expect, on average, for the mean reading score to change by -0.015 “points”.</p></li>
</ul></li>
</ul>
<p>So, an expected pattern of results emerges for the non-dyslexic students. Those with higher IQs tend to have higher reading accuracy; this does not mean higher IQ’s cause more accurate reading because random assignment of IQ is not possible. However, for the dyslexic students, the relationship is not what one would might expect. It is slightly negative, showing that higher IQ’s are related to lower reading accuracy. What we conclude from this is that we should not expect higher IQ’s to show higher performance on a test like this.</p>
<p>Checking the assumptions is always recommended before getting focused on the inferences in the model. When fitting models with multiple groups, it is possible to see “groups” in the fitted values (x-axis in Residuals vs Fitted and Scale-Location plots) and that is not a problem – it is a feature of these models. You should look for issues in the residuals for each group but the residuals should overall still be normally distributed and have the same variability everywhere. It is a bit hard to see issues in Figure <a href="8-11-section8-11.html#fig:Figure8-26">2.165</a> because of the group differences, but note the line of residuals for the higher fitted values. This is an artifact of the upper threshold in the reading accuracy test used. As in the first year of college GPA, these observations were <strong><em>censored</em></strong> – their true score was outside the range of values we could observe – and so we did not really get a measure of how good these students were since a lot of their abilities were higher than the test could detect and they all binned up at the same value of getting all the questions correct. The relationship in this group might be even stronger if we could really observe differences in the highest level readers. We should treat the results for the non-dyslexic group with caution even though they are clearly scoring on average higher and have a different slope than the results for the dyslexic students. The normality and influence diagnostics do not suggest any major issues other than this.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(dys_model,
     <span class="dt">sub.caption=</span><span class="st">&quot;Plot of diagnostics for Dyslexia Interaction model&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure8-26"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-26-1.png" alt="Diagnostic plots for interaction model for reading scores." width="960" />
<p class="caption">
Figure 2.165: Diagnostic plots for interaction model for reading scores.
</p>
</div>
<p>For these models, we have relaxed an earlier assumption that data were collected from only one group. In fact, we are doing specific research that is focused on questions about the differences between groups. However, these models still make assumptions that, within a specific group, the linearity assumption is met. They also assume that the variability in the residuals is the same for all observations. Sometimes it can be difficult to check the assumptions by looking at the overall diagnostic plots and it may be easier to go back to the original scatterplot or plot the residuals vs fitted values by group to fully assess the results. For example, Figure <a href="8-11-section8-11.html#fig:Figure8-27">2.166</a> shows a scatterplot of the residuals vs the quantitative explanatory variable by the groups. The variability in the residuals is a bit larger in the non-dyslexic group, possibly suggesting that variability in the reading test is higher for higher scoring individuals even though we couldn’t observe all of that variability because there were so many perfect scores in this group.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">scatterplot</span>(<span class="kw">residuals</span>(dys_model)<span class="op">~</span><span class="kw">fitted</span>(dys_model)<span class="op">|</span>dys,
            <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F)</code></pre></div>
<div class="figure"><span id="fig:Figure8-27"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-27-1.png" alt="Plot of Residuals vs Fitted from interaction dyslexia data model with groups indicated." width="576" />
<p class="caption">
Figure 2.166: Plot of Residuals vs Fitted from interaction dyslexia data model with groups indicated.
</p>
</div>
<p>If we feel comfortable enough with the assumptions to trust the inferences here (this might be dangerous), then we can consider what some of the model inferences provide us in this situation. For example, the test for <span class="math inline">\(H_0: {\color{red}{\boldsymbol{\beta_3}}}=0\)</span> vs <span class="math inline">\(H_A: {\color{red}{\boldsymbol{\beta_3}}}\ne 0\)</span> provides an interesting comparison. Under the null hypothesis, the two groups would have the same slope so it provides an opportunity to directly consider whether the relationship (via the slope) is different between the groups in their respective populations. We find <span class="math inline">\(t=-1.907\)</span> which, if the assumptions are met, follows a <span class="math inline">\(t(40)\)</span>-distribution under the null hypothesis. This test statistic has a corresponding p-value of 0.0638. So it provides some evidence of a difference in the slopes but it isn’t strong evidence of that. There are serious issues (like getting the wrong idea about directions of relationships) if we ignore a potentially important interaction and some statisticians would recommend retaining interactions even if the evidence is only moderate for its inclusion in the model. For the original research question of whether the relationships differ for the two groups, we only have marginal evidence to support that result. Possibly with a larger sample size or a reading test that only a few students could get 100% on, the researchers might have detected a more pronounced difference in the slopes for the two groups.</p>
<p>In the presence of a categorical by quantitative interaction, term-plots can be generated that plot the results for each group on the same display. This basically provides a plot of the “simplified” SLR models for each group. In Figure <a href="8-11-section8-11.html#fig:Figure8-28">2.167</a> we can see noticeable differences in the slopes and intercepts. Note that testing for differences in intercepts between groups is not very interesting when there are different slopes because if you change the slope, you have to change the intercept. The plot shows that there are clear differences in the means even though we don’t have a test to directly assess that in this complicated of a model<a href="#fn109" class="footnoteRef" id="fnref109"><sup>109</sup></a>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_model), <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>, <span class="dt">multiline=</span>T)</code></pre></div>
<div class="figure"><span id="fig:Figure8-28"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-28-1.png" alt="Term-plots for interaction model for reading scores." width="576" />
<p class="caption">
Figure 2.167: Term-plots for interaction model for reading scores.
</p>
</div>
<p>It certainly appears in the plots that IQ has a different impact on the mean score in the two groups (even though the p-value only provided marginal evidence). To reinforce the potential dangers of forcing the same slope for both groups, consider the additive model for these data. Again, this just shifts one group off the other one, but both have the same slope. The following model summary and term-plots (Figure <a href="8-11-section8-11.html#fig:Figure8-29">2.168</a>) suggest the potentially dangerous conclusion that can come from assuming a common slope when that might not be the case.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dys_modelR &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">+</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_modelR)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq + dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26062 -0.05565  0.02932  0.07577  0.13217 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.89178    0.02312  38.580  &lt; 2e-16
## ziq          0.02620    0.01957   1.339    0.188
## dys1        -0.26879    0.03905  -6.883 2.41e-08
## 
## Residual standard error: 0.1049 on 41 degrees of freedom
## Multiple R-squared:  0.6858, Adjusted R-squared:  0.6705 
## F-statistic: 44.75 on 2 and 41 DF,  p-value: 4.917e-11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR))</code></pre></div>
<div class="figure"><span id="fig:Figure8-29"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-29-1.png" alt="Term-plots for additive model for reading scores." width="576" />
<p class="caption">
Figure 2.168: Term-plots for additive model for reading scores.
</p>
</div>
<p>This model provides little to no evidence that IQ is related to reading score for all students (<span class="math inline">\(t_{41}=1.34\)</span>, p-value=0.188) but strong evidence of a difference in the y-intercepts (<span class="math inline">\(t_{41}=-6.88\)</span>, p-value <span class="math inline">\(&lt;0.00001\)</span>).</p>
<p>Since the IQ term has a large p-value, then we could drop it from the model – leaving a model that only includes the grouping variable:</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">dys_modelR2 &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_modelR2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = score ~ dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.25818 -0.04510  0.02514  0.09520  0.09694 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.90480    0.02117  42.737   &lt;2e-16
## dys1        -0.29892    0.03222  -9.278    1e-11
## 
## Residual standard error: 0.1059 on 42 degrees of freedom
## Multiple R-squared:  0.6721, Adjusted R-squared:  0.6643 
## F-statistic: 86.08 on 1 and 42 DF,  p-value: 1e-11</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR2))</code></pre></div>
<div class="figure"><span id="fig:Figure8-30"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-30-1.png" alt="Term-plot for dyslexia status only model for reading scores." width="576" />
<p class="caption">
Figure 2.169: Term-plot for dyslexia status only model for reading scores.
</p>
</div>
<p>These results, including the term-plot in Figure <a href="8-11-section8-11.html#fig:Figure8-30">2.169</a>, show that there is evidence of a difference in the mean reading scores between the two groups and maybe that is all these data really say… This is the logical outcome if we decide that the interaction is not important IN THIS DATA SET. In general, if the interaction is dropped, the interaction model can be reduced to considering an additive model with the categorical and quantitative predictor variables. Either or both of those variables could also be considered for removal, possibly starting with the variable with the larger p-value, leaving a string of ever-simpler models possible if large p-values are continually encountered<a href="#fn110" class="footnoteRef" id="fnref110"><sup>110</sup></a>.</p>
<p>It is useful to note that the last model has returned us to the first model we encountered in Chapter <a href="2-chapter2.html#chapter2">2</a> where we were just comparing the means for two groups. However, the researchers probably were not seeking to make the discovery that dyslexic students have a tougher time than non-dyslexic students on a reading test but sometimes that is all that the data support. The key part of this sequence of decisions was how much evidence you think a p-value of 0.06 contains…</p>
<p>For more than two categories in a categorical variable, the model contains more indicators to keep track of but uses the same ideas. We have to deal with modifying the intercept and slope coefficients for <strong>every</strong> deviation group so the task is onerous but relatively repetitive. The general model is:</p>
<p><span class="math display">\[\begin{array}{rl}
y_i=\beta_0 &amp;+ \beta_1x_i +\beta_2I_{\text{Level }2,i}+\beta_3I_{\text{Level }3,i}
+\cdots+\beta_JI_{\text{Level }J,i} \\
&amp;+\beta_{J+1}I_{\text{Level }2,i}\:x_i+\beta_{J+2}I_{\text{Level }3,i}\:x_i
+\cdots+\beta_{2J-1}I_{\text{Level }J,i}\:x_i +\varepsilon_i\ .
\end{array}\]</span></p>
<p>Specific to the audible tolerance/headache data that had four groups. The model with an interaction present is</p>
<p><span class="math display">\[\begin{array}{rl}
\text{du2}_i = \beta_0 &amp;+ \beta_1\cdot\text{du1}_i + \beta_2I_{T2,i} +
\beta_3I_{T3,i} + \beta_4I_{\text{Control},i} \\
&amp;+ \beta_5I_{T2,i}\cdot\text{du1}_i + \beta_6I_{T3,i}\cdot\text{du1}_i
+ \beta_7I_{\text{Control},i}\cdot\text{du1}_i+\varepsilon_i\ .
\end{array}\]</span></p>
<p>Based on the following output, the estimated general regression model is</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{du2}}_i = 1.33 &amp;+ 0.733\cdot\text{du1}_i - 0.236I_{T2,i} -
0.316I_{T3,i} - 1.091I_{\text{Control},i} \\
&amp;+ 0.066I_{T2,i}\cdot\text{du1}_i + 0.199I_{T3,i}\cdot\text{du1}_i
+ 0.106I_{\text{Control},i}\cdot\text{du1}_i\ .
\end{array}\]</span></p>
<p>Then we could work out the specific equation for <strong>each group</strong> with replacing their indicator variable in two places with 1s and the rest of the indicators with 0. For example, for the <em>Control</em> group:</p>
<p><span class="math display">\[\begin{array}{rll}
\widehat{\text{du2}}_i &amp;= 1.33 &amp;+ 0.733\cdot\text{du1}_i - 0.236*0 - 0.316*0
- 1.091*1 \\
&amp;&amp;+ 0.066*0*\text{du1}_i + 0.199*0*\text{du1}_i+ 0.106*1*\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=1.33&amp;+0.733\cdot\text{du1}_i - 1.091 + 0.106\cdot\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=0.239 &amp;+ 0.839\cdot\text{du1}_i\ 
\end{array}\]</span></p>

<div class="figure"><span id="fig:Figure8-31"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-31-1.png" alt="Term-plot for decibel tolerance interaction model (version 1)." width="672" />
<p class="caption">
Figure 2.170: Term-plot for decibel tolerance interaction model (version 1).
</p>
</div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">head2 &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1<span class="op">*</span>treatment, <span class="dt">data=</span>Headache)
<span class="kw">summary</span>(head2)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = du2 ~ du1 * treatment, data = Headache)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.8072 -1.0969 -0.3285  0.8192 10.6039 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)           1.33157    0.66027   2.017   0.0467
## du1                   0.73319    0.09969   7.355 8.53e-11
## treatmentT2          -0.23560    1.13414  -0.208   0.8359
## treatmentT3          -0.31613    0.95767  -0.330   0.7421
## treatmentControl     -1.09084    0.95020  -1.148   0.2540
## du1:treatmentT2       0.06623    0.17473   0.379   0.7055
## du1:treatmentT3       0.19904    0.13350   1.491   0.1395
## du1:treatmentControl  0.10604    0.14326   0.740   0.4611
## 
## Residual standard error: 2.148 on 90 degrees of freedom
## Multiple R-squared:  0.7573, Adjusted R-squared:  0.7384 
## F-statistic: 40.12 on 7 and 90 DF,  p-value: &lt; 2.2e-16</code></pre>

<p>Or we can let the term-plots (Figures <a href="8-11-section8-11.html#fig:Figure8-31">2.170</a> and <a href="8-11-section8-11.html#fig:Figure8-32">2.171</a>) show us all four different simplified models. Here we can see that all the slopes “look” to be pretty similar. When the interaction model is fit and the results “look” like the additive model, there is a good chance that we will be able to avoid all this complication and just use the additive model without missing anything interesting. There are two different options for displaying interaction models. Version 1 (Figure <a href="8-11-section8-11.html#fig:Figure8-31">2.170</a>) has a different panel for each level of the categorical variable and Version 2 (Figure <a href="8-11-section8-11.html#fig:Figure8-32">2.171</a>) puts all the lines on the same plot. In this case, neither version shows much of a difference and Version 2 overlaps so much that you can’t see all the groups. In these situations, it can be useful to make the term-plots with <code>multiline=T</code> and <code>multiline=F</code> and select the version that captures the results best.</p>

<div class="figure"><span id="fig:Figure8-32"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-32-1.png" alt="Term-plot for decibel tolerance interaction model (version 2). This plot is not printed in color because it is impossible to distinguish the four groups whether in color or black and white." width="576" />
<p class="caption">
Figure 2.171: Term-plot for decibel tolerance interaction model (version 2). This plot is not printed in color because it is impossible to distinguish the four groups whether in color or black and white.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(head2), <span class="dt">x.var=</span><span class="st">&quot;du1&quot;</span>, <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(head2), <span class="dt">x.var=</span><span class="st">&quot;du1&quot;</span>, <span class="dt">multiline=</span>T, <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>)</code></pre></div>
<p>In situations with more than 2 levels, the <span class="math inline">\(t\)</span>-tests for the interaction or changing y-intercepts are not informative for deciding if you really need different slopes or intercepts for all the groups. They only tell you if a specific group is potentially different from the baseline group and the choice of the baseline is arbitrary. To assess whether we really need to have varying slopes or intercepts with more than two groups we need to develop <span class="math inline">\(F\)</span>-tests for the interaction part of the model.</p>

</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-smdata">
<p>Merkle, Ed, and Michael Smithson. 2013. <em>Smdata: Data to Accompany Smithson &amp; Merkle, 2013</em>. <a href="https://CRAN.R-project.org/package=smdata" class="uri">https://CRAN.R-project.org/package=smdata</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="108">
<li id="fn108"><p>Standardizing quantitative predictor variables is popular in social sciences, often where the response variable is also standardized. In those situations, they generate what are called “standardized slopes” (<a href="https://en.wikipedia.org/wiki/Standardized_coefficient" class="uri">https://en.wikipedia.org/wiki/Standardized_coefficient</a>) that estimate the change in SDs in the response for a 1 SD change in the explanatory variable.<a href="8-11-section8-11.html#fnref108">↩</a></p></li>
<li id="fn109"><p>There is a way to test for a difference in the two lines at a particular <span class="math inline">\(x\)</span> value but it is beyond the scope of this material and uses what are called “contrasts”.<a href="8-11-section8-11.html#fnref109">↩</a></p></li>
<li id="fn110"><p>This is an example of what is called “step down” testing for model refinement which is a commonly used technique for arriving at a final model to describe response variables. Note that each step in the process should be reported, not just the final model that only has variables with small p-values remaining in it.<a href="8-11-section8-11.html#fnref110">↩</a></p></li>
</ol>
</div>
<p style="text-align: center;">
<a href="8-10-section8-10.html"><button class="btn btn-default">Previous</button></a>
<a href="8-12-section8-12.html"><button class="btn btn-default">Next</button></a>
</p>
</div>
</div>


</div>

<script>

// add bootstrap table styles to pandoc tables
$(document).ready(function () {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
});

</script>

</body>
</html>
