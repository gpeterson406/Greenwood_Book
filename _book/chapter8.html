<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Multiple linear regression | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 8 Multiple linear regression | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.11 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Multiple linear regression | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Multiple linear regression | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter7.html">
<link rel="next" href="chapter9.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.1</b> Summary of important R code</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.1</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.2</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.3</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.4</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.5</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.6</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.7</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.8</b> Chapter summary</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter8" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Multiple linear regression</h1>
<div id="section8-1" class="section level2">
<h2><span class="header-section-number">8.1</span> Going from SLR to MLR</h2>
<p>In many situations, especially in observational studies, it is unlikely that
the system is simple enough to be characterized
by a single predictor variable. In experiments, if we randomly assign levels of
a predictor variable we can assume that the impacts of other variables cancel
out as a direct result of the random assignment.

But it is possible even in
these experimental situations that we can “improve” our model for the response
variable by adding additional predictor variables that explain additional
variation in the responses, reducing the amount of unexplained variation. This
can allow more precise inferences to be generated from the model. As mentioned
previously, it might be useful to know the sex or weight of the subjects in the
Beers vs BAC study to account for more of the variation in the responses – this
idea motivates our final topic: <strong><em>multiple linear regression</em></strong> (<strong>MLR</strong>)
models.



In observational studies,
we can think of a suite of characteristics of observations that might be
related to a response variable. For example, consider a study of yearly
salaries and variables that might explain the amount people get paid. We might
be most interested in seeing how incomes change based on age, but it would be
hard to ignore potential differences based on sex and education level. Trying
to explain incomes would likely require more than one predictor variable and we
wouldn’t be able to explain all the variability in the responses just based on
gender and education level, but a model using those variables might still
provide some useful information about each component and about age impacts on
income after we adjust (control) for sex and education. The extension to MLR
allows us to incorporate multiple predictors into a regression model.

Geometrically, this is a way of relating many different dimensions (number of
<span class="math inline">\(x\text{&#39;s}\)</span>) to what happened in a single response variable (one dimension).</p>
<p>We start with the same model as in SLR except now we allow <span class="math inline">\(K\)</span> different
<span class="math inline">\(x\text{&#39;s}\)</span>:
</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}+ \ldots + \beta_Kx_{Ki}
+ \varepsilon_i\]</span></p>
<p>Note that if <span class="math inline">\(K=1\)</span>, we are back to SLR. In the MLR model, there are <span class="math inline">\(K\)</span>
predictors and we still have a
y-intercept.

The MLR model carries the same assumptions as an SLR model with a
couple of slight tweaks specific to MLR (see Section <a href="chapter8.html#section8-2">8.2</a>
for the details on the changes to the validity conditions).</p>
<p>We are able to use the
least squares criterion for estimating the regression coefficients in MLR, but
the mathematics are beyond the scope of this course.

The <code>lm</code> function takes
care of finding the least squares coefficients using a very sophisticated
algorithm<a href="#fn92" class="footnote-ref" id="fnref92"><sup>92</sup></a>. The estimated
regression equation it returns is:</p>
<p><span class="math display">\[\hat{y}_i = b_0 + b_1x_{1i} +b_2x_{2i}+\ldots+b_Kx_{Ki}\]</span></p>
<p>where each <span class="math inline">\(b_k\)</span> estimates its corresponding parameter <span class="math inline">\(\beta_k\)</span>.</p>
<p>An example of snow depths at some high elevation locations on a day in
April provides a nice motivation for these methods. A random sample of
<span class="math inline">\(n=25\)</span> MT locations (from the population of <span class="math inline">\(N=85\)</span> at the time) were obtained
from the Natural Resources Conversation Service’s website
(<a href="http://www.wcc.nrcs.usda.gov/snotel/Montana/montana.html" class="uri">http://www.wcc.nrcs.usda.gov/snotel/Montana/montana.html</a>) a few years ago.
Information on the snow depth (<code>Snow.Depth</code>) in inches, daily Minimum and
Maximum Temperatures (<code>Min.Temp</code> and <code>Max.Temp</code>) in <span class="math inline">\(^\circ F\)</span> and
elevation of the site (<code>Elevation</code>) in feet. A snow science researcher (or
spring back-country skier) might be interested in understanding <em>Snow depth</em>
as a function of <em>Minimum Temperature</em>, <em>Maximum Temperature</em>, and <em>Elevation</em>.
One might assume that colder
and higher places will have more snow, but using just one of the predictor
variables might leave out some important predictive information. The following
code loads the data set and makes the scatterplot matrix
(Figure <a href="chapter8.html#fig:Figure8-1">8.1</a>) to allow
some preliminary assessment of the pairwise relationships.</p>

<pre class="sourceCode r"><code class="sourceCode r">snotel_s &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/snotel_s.csv&quot;</span>)
snotel2 &lt;-<span class="st"> </span>snotel_s[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>,<span class="dv">4</span><span class="op">:</span><span class="dv">6</span>,<span class="dv">3</span>)] <span class="co">#Reorders columns for nicer pairs.panel display</span>
<span class="kw">require</span>(psych)
<span class="kw">pairs.panels</span>(snotel2[,<span class="op">-</span><span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)], <span class="dt">ellipse=</span>F,
             <span class="dt">main=</span><span class="st">&quot;Scatterplot matrix of SNOTEL Data&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-1"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-1-1.png" alt="Scatterplot matrix of data from a sample of SNOTEL sites in April on four variables. " width="672" />
<p class="caption">
Figure 8.1: Scatterplot matrix of data from a sample of SNOTEL sites in April on four variables. 
</p>
</div>
<p>It appears that there are many strong linear relationships between the variables,
with <em>Elevation</em> and <em>Snow Depth</em> having the largest magnitude, <strong><em>r</em></strong> = 0.80.
Higher temperatures seem to be associated with less snow - not a big surprise so
far! There might be an outlier at an elevation of 7400 feet and a snow depth
below 10 inches that we should explore further.</p>
<p>A new issue arises in attempting to build MLR models called <strong><em>multicollinearity</em></strong>.

Again, it is a not surprise that temperature and
elevation are correlated but that creates a
problem if we try to put them both into a model to explain snow depth. Is it
the elevation, temperature, or the combination of both that matters for getting
and retaining more snow? <strong>Correlation between predictor variables</strong> is called
multicollinearity and <strong>makes estimation and interpretation of MLR models more
complicated than in SLR</strong>. Section <a href="chapter8.html#section8-5">8.5</a> deals with this issue
directly and discusses methods
for detecting its presence. For now, remember that in MLR this issue sometimes
makes it difficult to disentangle the impacts of different predictor variables
on the response when the predictors share information – when they are
correlated.</p>
<p>To get familiar with this example, we can start with fitting some
potential SLR
models and plotting the estimated models. Figure <a href="chapter8.html#fig:Figure8-2">8.2</a> contains
the result for the SLR using <em>Elevation</em> and results for two temperature based
models are in Figures <a href="chapter8.html#fig:Figure8-3a">8.3</a> and <a href="chapter8.html#fig:Figure8-3b">8.4</a>.
<em>Snow Depth</em> is selected as the
obvious response variable both due to skier interest and potential scientific
causation (snow can’t change elevation but elevation could be the driver of
snow deposition and retention).</p>
<p>(ref:fig8-2) Plot of estimated
SLR model for Snow Depth with only Elevation as the predictor.
</p>
<div class="figure"><span id="fig:Figure8-2"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-2-1.png" alt="(ref:fig8-2)" width="480" />
<p class="caption">
Figure 8.2: (ref:fig8-2)
</p>
</div>
<p>Based on the model summaries provided below, the three estimated SLR models
are:</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{SnowDepth}}_i &amp;= -72.006 + 0.0163\cdot\text{Elevation}_i, \\
\widehat{\text{SnowDepth}}_i &amp;= 174.096 - 4.884\cdot\text{MinTemp}_i,\text{ and} \\
\widehat{\text{SnowDepth}}_i &amp;= 122.672 - 2.284\cdot\text{MaxTemp}_i.
\end{array}\]</span></p>
<p>The term-plots of the estimated models reinforce
our expected results, showing a positive change in <em>Snow Depth</em> for higher
<em>Elevations</em> and negative impacts for increasing temperatures on <em>Snow Depth</em>.
These plots are made across the observed range<a href="#fn93" class="footnote-ref" id="fnref93"><sup>93</sup></a> of the predictor
variable and help us to get a sense of the total impacts of
predictors. For example, for elevation in Figure <a href="chapter8.html#fig:Figure8-2">8.2</a>, the
smallest observed
value was 4925 feet and the largest was 8300 feet. The regression line goes
from estimating a mean snow depth of 8 inches to 63 inches. That gives you some
practical idea of the size of the estimated <em>Snow Depth</em> change for the changes in
<em>Elevation</em> observed in the data. Putting this together, we can say that
there was around a
55 inch change in predicted snow depths for a close to 3400 foot increase in
elevation. This helps make the slope coefficient of 0.0163 in the model more
easily understood.</p>
<p>Remember that in SLR, the range of <span class="math inline">\(x\)</span> matters just as much
as the units of <span class="math inline">\(x\)</span> in determining the practical importance and size of the slope
coefficient. A value of 0.0163 looks small but is actually at the heart of a
pretty good model for predicting snow depth. A one foot change of elevation is
“tiny” here relative to changes in the response so the slope coefficient can be
small and still amount to big changes in the predicted response across the range
of values of <span class="math inline">\(x\)</span>. If the <em>Elevation</em> had been recorded in thousands of feet,
then the slope would have been <span class="math inline">\(0.0163*1000=16.3\)</span> inches change in mean
<em>Snow Depth</em> for a 1000 foot increase in elevation.</p>
<p>The plots of the two estimated temperature models in
Figures <a href="chapter8.html#fig:Figure8-3a">8.3</a> and <a href="chapter8.html#fig:Figure8-3b">8.4</a> suggest a similar change
in the responses over
the range of observed temperatures. Those predictors range from 22<span class="math inline">\(^\circ F\)</span>
to 34<span class="math inline">\(^\circ F\)</span> (minimum temperature) and from 26<span class="math inline">\(^\circ F\)</span> to 50<span class="math inline">\(^\circ F\)</span>
(maximum temperature). This tells us a 1<span class="math inline">\(^\circ F\)</span> increase in either
temperature is a
greater proportion of the observed range of each predictor than a 1 unit (foot)
increase in elevation, so the two temperature variables will generate larger apparent
magnitudes of slope coefficients. But having large slope coefficients is no
guarantee of a good model – in fact, the elevation model has the highest
<em>R</em><sup>2</sup> value of these three models even though its slope coefficient looks tiny
compared to the other models.</p>

<div class="figure"><span id="fig:Figure8-3a"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-3a-1.png" alt="Plot of the estimated SLR model using only Min Temp as predictor." width="480" />
<p class="caption">
Figure 8.3: Plot of the estimated SLR model using only Min Temp as predictor.
</p>
</div>

<div class="figure"><span id="fig:Figure8-3b"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-3b-1.png" alt="Plot of the estimated SLR model using only Max Temp as predictor." width="480" />
<p class="caption">
Figure 8.4: Plot of the estimated SLR model using only Max Temp as predictor.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation, <span class="dt">data=</span>snotel2)
m2 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Min.Temp, <span class="dt">data=</span>snotel2)
m3 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Max.Temp, <span class="dt">data=</span>snotel2)
<span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(m1, <span class="dt">xlevels=</span><span class="kw">list</span>(<span class="dt">Elevation=</span>snotel2<span class="op">$</span>Elevation)),
     <span class="dt">main=</span><span class="st">&quot;SLR: Effect of Elevation&quot;</span>)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(m2, <span class="dt">xlevels=</span><span class="kw">list</span>(<span class="dt">Min.temp=</span>snotel2<span class="op">$</span>Min.Temp)),
     <span class="dt">main=</span><span class="st">&quot;SLR: Effect of Min Temp&quot;</span>)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(m3, <span class="dt">xlevels=</span><span class="kw">list</span>(<span class="dt">Max.Temp=</span>snotel2<span class="op">$</span>Max.Temp)),
     <span class="dt">main=</span><span class="st">&quot;SLR: Effect of Max Temp&quot;</span>)</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation, data = snotel2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -36.416  -5.135  -1.767   7.645  23.508 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -72.005873  17.712927  -4.065 0.000478
## Elevation     0.016275   0.002579   6.311 1.93e-06
## 
## Residual standard error: 13.27 on 23 degrees of freedom
## Multiple R-squared:  0.634,  Adjusted R-squared:  0.618 
## F-statistic: 39.83 on 1 and 23 DF,  p-value: 1.933e-06</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Min.Temp, data = snotel2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.156 -11.238   2.810   9.846  26.444 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 174.0963    25.5628   6.811 6.04e-07
## Min.Temp     -4.8836     0.9148  -5.339 2.02e-05
## 
## Residual standard error: 14.65 on 23 degrees of freedom
## Multiple R-squared:  0.5534, Adjusted R-squared:  0.534 
## F-statistic:  28.5 on 1 and 23 DF,  p-value: 2.022e-05</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(m3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Max.Temp, data = snotel2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -26.447 -10.367  -4.394  10.042  34.774 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 122.6723    19.6380   6.247 2.25e-06
## Max.Temp     -2.2840     0.5257  -4.345 0.000238
## 
## Residual standard error: 16.25 on 23 degrees of freedom
## Multiple R-squared:  0.4508, Adjusted R-squared:  0.4269 
## F-statistic: 18.88 on 1 and 23 DF,  p-value: 0.0002385</code></pre>

<p>Since all three variables look like they are potentially useful in predicting
snow depth, we want to consider if an MLR model might explain more of the
variability in <em>Snow Depth</em>. To fit an MLR model, we use the same general format
as in other topics but with adding “<code>+</code>” between any additional
predictors<a href="#fn94" class="footnote-ref" id="fnref94"><sup>94</sup></a> we want to add to the model,
<code>y~x1+x2+...+xk</code>: </p>
<p>(ref:fig8-4) Term-plots for the MLR for Snow Depth based on Elevation,
Min Temp and Max Temp. Note that the x-axis ranges are different than those
used in Figures <a href="chapter8.html#fig:Figure8-2">8.2</a>, <a href="chapter8.html#fig:Figure8-3a">8.3</a> and
<a href="chapter8.html#fig:Figure8-3b">8.4</a> for the comparable SLR models.</p>
<pre class="sourceCode r"><code class="sourceCode r">m4 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2)
<span class="kw">summary</span>(m4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -29.508  -7.679  -3.139   9.627  26.394 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -10.506529  99.616286  -0.105   0.9170
## Elevation     0.012332   0.006536   1.887   0.0731
## Min.Temp     -0.504970   2.042614  -0.247   0.8071
## Max.Temp     -0.561892   0.673219  -0.835   0.4133
## 
## Residual standard error: 13.6 on 21 degrees of freedom
## Multiple R-squared:  0.6485, Adjusted R-squared:  0.5983 
## F-statistic: 12.91 on 3 and 21 DF,  p-value: 5.328e-05</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(m4), <span class="dt">main=</span><span class="st">&quot;MLR model with Elev, Min &amp; Max Temps&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-4"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-4-1.png" alt="(ref:fig8-4)" width="960" />
<p class="caption">
Figure 8.5: (ref:fig8-4)
</p>
</div>
<p>Based on the output, the estimated MLR model is</p>
<p><span class="math display">\[\widehat{\text{SnowDepth}}_i = -10.51 + 0.0123\cdot\text{Elevation}_i
-0.505\cdot\text{MinTemp}_i - 0.562\cdot\text{MaxTemp}_i\ .\]</span></p>
<p>The direction of the estimated slope coefficients were similar but they
all changed in magnitude as compared to the respective SLRs, as seen in the
estimated term-plots from the MLR model in Figure <a href="chapter8.html#fig:Figure8-4">8.5</a>.</p>
<p>There are two ways to think about the changes from individual SLR slope
coefficients to the similar MLR results.</p>
<ol style="list-style-type: decimal">
<li><p>Each term in the MLR is the result for estimating each
slope after controlling for the other two variables (and we will always
use this interpretation any time we interpret MLR effects). For the elevation slope,
we will need to say that the slope coefficient is “corrected for” or “adjusted for” the variability that is explained by the
temperature variables in the model.</p></li>
<li><p>Because of multicollinearity in the predictors, the
variables might share information that is useful for explaining the
variability in the response variable, so the slope coefficients of each
predictor get perturbed because the model cannot separate their effects on
the response. This issue disappears when the predictors are uncorrelated
or even just minimally correlated.</p></li>
</ol>
<p>There are some ramifications of multicollinearity in MLR:
</p>
<ol style="list-style-type: decimal">
<li><p>Adding variables to a model might lead to almost no improvement in the
overall variability explained by the model.</p></li>
<li><p>Adding variables to a model can cause slope coefficients to change signs
as well as magnitudes.</p></li>
<li><p>Adding variables to a model can lead to inflated standard errors for some
or all of the coefficients (this is less obvious
but is related to the shared information in predictors making it less
clear what slope coefficient to use for each variable).</p></li>
<li><p>In extreme cases of multicollinearity, it may even be impossible to obtain
any coefficient estimates.</p></li>
</ol>
<p>These seem like pretty serious issues and they are but there are many, many
situations where we proceed with MLR even
in the presence of potentially correlated predictors.

It is likely that you
have heard or read about inferences from models that are dealing with this
issue – for example, medical studies often report the increased risk of death
from some behavior or trait after controlling for sex, age, etc. In many
research articles, it is becoming common practice to report the slope for a
variable that is of most interest with it in the model alone (SLR) and in models
after adjusting for the other variables that are expected to matter. These types
of results are built with MLR or related multiple-predictor models like MLR.
</p>
</div>
<div id="section8-2" class="section level2">
<h2><span class="header-section-number">8.2</span> Validity conditions in MLR</h2>
<p>But before we get too excited about any results, we should always assess our
validity conditions. For MLR, they are similar to those for SLR:
</p>
<ul>
<li><p><strong>Quantitative variables condition:</strong></p>
<ul>
<li>The response and all predictors need to be quantitative variables.
This condition is relaxed to allow a categorical predictor in two ways
in Sections <a href="chapter8.html#section8-9">8.9</a> and <a href="chapter8.html#section8-11">8.11</a>.</li>
</ul></li>
<li><p><strong>Independence of observations:</strong></p>
<ul>
<li><p>This assumption is about the responses – we must assume that they were
collected in a fashion so that they can be assumed to be independent. This
implies that we also have independent random errors.
</p></li>
<li><p>This is not an assumption about the predictor variables!</p></li>
</ul></li>
<li><p><strong>Linearity of relationship (</strong><b><font color='red'>NEW VERSION FOR MLR!</font></b><strong>):</strong></p>
<ul>
<li><p>Linearity is assumed between the response variable and <strong>each</strong>
explanatory variable (<span class="math inline">\(y\)</span> and each <span class="math inline">\(x\)</span>).</p></li>
<li><p>We can check this two ways:</p>
<ol style="list-style-type: decimal">
<li><p>Make plots of the response versus each explanatory variable:</p>
<ul>
<li>Only visual evidence of a curving relationship is a problem here.
Transformations of individual explanatory variables or the
response are possible.</li>
</ul></li>
<li><p>Examine the Residuals vs Fitted plot:</p>
<ul>
<li>When using MLR, curves in the residuals vs. fitted values
suggest a missed curving relationship with at least one predictor
variable, but it will not be specific as to which one is non-linear.
Revisit the scatterplots to identify the source of the issue.
</li>
</ul></li>
</ol></li>
</ul></li>
<li><p><strong>Multicollinearity effects checked for:</strong></p>
<ul>
<li><p>Issues here do not mean we cannot proceed with a given model, but it can
impact our ability to trust and interpret the estimated terms.</p></li>
<li><p>Check a scatterplot or correlation matrix  to assess the potential for
shared information in different predictor variables.</p></li>
<li><p>Use the diagnostic measure called a <strong><em>variance inflation factor</em></strong>
(<strong><em>VIF</em></strong>) discussed in Section <a href="chapter8.html#section8-5">8.5</a> (we need to develop
some ideas first to understand this measure). </p></li>
</ul></li>
<li><p><strong>Equal (constant) variance:</strong></p>
<ul>
<li>Same as before since it pertains to the residuals.</li>
</ul></li>
<li><p><strong>Normality of residuals:</strong></p>
<ul>
<li>Same as before since it pertains to the residuals.</li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li><p>Leverage is now determined by how unusual a point is for multiple
explanatory variables.</p></li>
<li><p>The <strong><em>leverage</em></strong> values in the Residuals vs Leverage plot are
scaled to add up to the <em>degrees of freedom (df) used for the model</em>
which is the number of explanatory variables (<span class="math inline">\(K\)</span>) plus 1, so <span class="math inline">\(K+1\)</span>.
</p></li>
<li><p>The scale of leverages depends on the complexity of the model through
the <em>df</em> and the sample size.</p></li>
<li><p>The interpretation is still that the larger the leverage value, the
more leverage the point has.</p></li>
<li><p>The mean leverage is always <em>(model used df)/n = (K+1)/n</em> – so focus
on the values with above average leverage.</p>
<ul>
<li>For example, with <span class="math inline">\(K=3\)</span> and <span class="math inline">\(n=20\)</span>, the average leverage is
<span class="math inline">\(4/20=1/5\)</span>.</li>
</ul></li>
<li><p>High leverage points whose response does not follow the pattern defined
by the other observations (now based on patterns for multiple <span class="math inline">\(x\text{&#39;s}\)</span>
with the response) will be influential.</p></li>
<li><p>Use the Residual’s vs Leverage plot to identify problematic points.
Explore further with Cook’s D continuing to provide a measure of the
influence of each observation.</p>
<ul>
<li>The rules and interpretations for Cook’s D are the same as in SLR
(over 0.5 is possibly influential and over 1 is definitely influential).</li>
</ul></li>
</ul></li>
</ul>
<p>While not a condition for use of the methods, a note about RA and RS is useful
here in considering the scope of inference of any results.

To make inferences
about a population, we need to have a representative sample. If we have
randomly assigned levels of treatment variables(s), then we can make causal
inferences to subjects like those that we could have observed. And if we both
have a representative sample and randomization, we can make causal inferences
for the population. It is possible to randomly assign levels of variable(s) to
subjects and still collect additional information from other explanatory
(sometimes called <strong><em>control</em></strong>) variables. The causal interpretations would
only be associated with the explanatory variables that were randomly assigned
even though the model might
contain other variables. Their interpretation still involves noting all the
variables included in the model, as demonstrated below. It is even possible to
include interactions between randomly assigned variables and other variables –
like drug dosage and sex of the subjects. In these cases, causal inference
could apply to the treatment levels but noting that the impacts differ based on
the non-randomly assigned variable.</p>
<p>For the <em>Snow Depth</em> data, the conditions can be assessed as:</p>
<ul>
<li><p><strong>Quantitative variables condition:</strong></p>
<ul>
<li>These are all met.</li>
</ul></li>
<li><p><strong>Independence of observations:</strong></p>
<ul>
<li>The observations are based on a random sample of sites from the
population and the sites are spread around the mountains in Montana. Many
people would find it to be reasonable to assume that the sites are
independent of one another but others would be worried that sites closer
together in space might be more similar than they are to far-away
observations (this is called <strong><em>spatial correlation</em></strong>). I have
been in a heated discussion with statistics colleagues about whether
spatial dependency should be considered or if it is valid to ignore it in
this sort of situation. It is certainly possible to be concerned about
independence of observations here but it takes more advanced statistical
methods to actually assess whether there is spatial dependency in these data. Even if you were going to pursue models that incorporate spatial correlations, the first task would be to fit this sort of model
and then explore the results. When data are collected across space, you should note that there might be some sort of spatial dependency that <em>could</em> violate the independence assumption.</li>
</ul></li>
</ul>
<p>We need our diagnostic plots to assess the remaining assumptions.

The same code
as before will provide diagnostic plots. There is some extra code
(<code>par(...)</code>) added to allow us to add labels to the plots to know which model
is being displayed since we have so many to discuss here.</p>
<p>(ref:fig8-5) Diagnostic plots for model m4:
<span class="math inline">\(\text{Snow.Depth}\sim \text{Elevation} + \text{Min.Temp} + \text{Max.Temp}\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(m4, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for m4&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-5"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-5-1.png" alt="(ref:fig8-5)" width="960" />
<p class="caption">
Figure 8.6: (ref:fig8-5)
</p>
</div>
<ul>
<li><p><strong>Linearity of relationship (</strong><b><font color='red'>NEW VERSION FOR MLR!</font></b><strong>):</strong></p>
<ul>
<li><p>Make plots of the response versus each explanatory variable:</p>
<ul>
<li>In Figure <a href="chapter8.html#fig:Figure8-1">8.1</a>, the plots of each variable versus
snow depth do not clearly show any nonlinearity except for a little
dip around 7000 feet in the plot vs <em>Elevation</em>.</li>
</ul></li>
<li><p>Examine the Residuals vs Fitted plot in Figure <a href="chapter8.html#fig:Figure8-5">8.6</a>:</p>
<ul>
<li>Generally, there is no clear curvature in the Residuals vs Fitted
panel and that would be an acceptable answer. However, there is some
pattern in the smoothing line that could suggest a more complicated
relationship between at least one predictor and the response. This also
resembles the pattern in the <em>Elevation</em> vs. <em>Snow depth</em> panel in
Figure <a href="chapter8.html#fig:Figure8-1">8.1</a> so that might be the source of this
“problem”. This suggests that there is the potential to do a little
bit better but that it is not unreasonable to proceed on with the MLR,
ignoring this little wiggle in the diagnostic plot.</li>
</ul></li>
</ul></li>
<li><p><strong>Multicollinearity effects checked for:</strong></p>
<ul>
<li><p>The predictors certainly share information in this application
(correlations between -0.67 and -0.91) and multicollinearity looks to be
a major concern in being able to understand/separate the impacts of
temperatures and elevations on snow depths.</p></li>
<li><p>See Section <a href="chapter8.html#section8-5">8.5</a> for more on this issue in these data.</p></li>
</ul></li>
</ul>

<ul>
<li><p><strong>Equal (constant) variance:</strong></p>
<ul>
<li>While there is a little bit more variability in the middle of the fitted
values, this is more an artifact of having a smaller data set with a couple
of moderate outliers that fell in the same range of fitted values and maybe
a little bit of missed curvature. So there is not too much of an issue with
this condition.</li>
</ul></li>
<li><p><strong>Normality of residuals:</strong></p>
<ul>
<li>The residuals match the normal distribution fairly closely the QQ-plot, showing only a little
deviation for observation 9 from a normal distribution and that deviation
is extremely minor. There is certainly no indication of a violation of the normality
assumption here. </li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li><p>With <span class="math inline">\(K=3\)</span> predictors and <span class="math inline">\(n=25\)</span> observations, the average
leverage is <span class="math inline">\(4/25=0.16\)</span>. This gives us a scale to interpret the leverage
values on the x-axis of the lower right panel of our diagnostic plots.</p></li>
<li><p>There are three higher leverage points (leverages over 0.3) with only
one being influential (point 9) with Cook’s D close to 1.</p>
<ul>
<li>Note that point 10 had the same leverage but was not influential with
Cook’s D less than 0.5.</li>
</ul></li>
<li><p>We can explore both of these points to see how two observations can have
the same leverage and different amounts of influence.</p></li>
</ul></li>
</ul>
<p>The two flagged points, observations 9 and 10 in the data set, are for the
sites “Northeast Entrance” (to Yellowstone) and “Combination”. We can use the
MLR equation to do some prediction for each observation and calculate residuals
to see how far the model’s predictions are from the actual observed values for
these sites. For the Northeast Entrance, the <em>Max.Temp</em> was 45, the <em>Min.Temp</em>
was 28, and the <em>Elevation</em> was 7350 as you can see in
this printout of just the two rows of the data set available by referencing
rows 9 and 10 in the bracket from <code>snotel2</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">snotel2[<span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">10</span>),]</code></pre>
<pre><code>## # A tibble: 2 x 6
##      ID Station            Max.Temp Min.Temp Elevation Snow.Depth
##   &lt;dbl&gt; &lt;chr&gt;                 &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1    18 Northeast Entrance       45       28      7350       11.2
## 2    53 Combination              36       28      5600       14</code></pre>
<p>The estimated <em>Snow Depth</em> for the <em>Northeast Entrance</em> site (observation 9)
is found using the estimated model with</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{SnowDepth}}_9 &amp;= -10.51 + 0.0123\cdot\text{Elevation}_9 -
0.505\cdot\text{MinTemp}_9 - 0.562\cdot\text{MaxTemp}_9 \\
&amp; = -10.51 + 0.0123*\boldsymbol{7350} -0.505*\boldsymbol{28} - 
0.562*\boldsymbol{45} \\
&amp; = 40.465 \text{ inches,}
\end{array}\]</span></p>
<p>but the observed snow depth was actually <span class="math inline">\(y_9=11.2\)</span> inches. The observed <strong><em>residual</em></strong> is then</p>
<p><span class="math display">\[e_9=y_9-\hat{y}_9 = 11.2-40.465 = -29.265 \text{ inches.}\]</span></p>
<p>So the model “misses” the snow depth by over 29 inches with the model suggesting
over 40 inches of snow but only 11 inches actually being present<a href="#fn95" class="footnote-ref" id="fnref95"><sup>95</sup></a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">-10.51</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.0123</span><span class="op">*</span><span class="dv">7350</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.505</span><span class="op">*</span><span class="dv">28</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.562</span><span class="op">*</span><span class="dv">45</span></code></pre>
<pre><code>## [1] 40.465</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">11.2</span> <span class="op">-</span><span class="st"> </span><span class="fl">40.465</span></code></pre>
<pre><code>## [1] -29.265</code></pre>
<p>This point is being rated as influential (Cook’s D <span class="math inline">\(\approx\)</span> 1) with a
leverage of nearly 0.35 and a standardized residual (y-axis of Residuals vs. 
Leverage plot) of nearly -3. This suggests that even
with this observation impacting/distorting the slope coefficients (that is what
<strong><em>influence</em></strong> means), the model is still doing really poorly at fitting this
observation. We’ll drop it and re-fit the model in a second to see how the slopes
change. First, let’s compare that result to what happened for data point 10
(“Combination”) which was just as high leverage but not identified as
influential.</p>
<p>The estimated snow depth for “Combination” is </span></p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{SnowDepth}}_{10} &amp;= -10.51 + 0.0123\cdot\text{Elevation}_{10} -
0.505\cdot\text{MinTemp}_{10} - 0.562\cdot\text{MaxTemp}_{10} \\
&amp; = -10.51 + 0.0123*\boldsymbol{5600} -0.505*\boldsymbol{28} - 
0.562*\boldsymbol{36} \\
&amp; = 23.998 \text{ inches.}
\end{array}\]</span></p>
<p>The observed snow depth here was <span class="math inline">\(y_{10} = 14.0\)</span> inches so the observed
residual is then</p>
<p><span class="math display">\[e_{10}=y_{10}-\hat{y}_{10} = 14.0-23.998 = -9.998 \text{ inches.}\]</span></p>
<p>This results in a standardized residual of around -1. This is still a “miss”
but not as glaring as the previous result and also is not having a major impact
on the model’s estimated slope coefficients based on the small Cook’s D value.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="fl">-10.51</span> <span class="op">+</span><span class="st"> </span><span class="fl">0.0123</span><span class="op">*</span><span class="dv">5600</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.505</span><span class="op">*</span><span class="dv">28</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.562</span><span class="op">*</span><span class="dv">36</span></code></pre>
<pre><code>## [1] 23.998</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">14</span> <span class="op">-</span><span class="st"> </span><span class="fl">23.998</span></code></pre>
<pre><code>## [1] -9.998</code></pre>
<p>Note that any predictions using this model presume that it is
trustworthy, but
the large Cook’s D on one observation suggests we should consider the model
after removing that observation. We can re-run the model without the
9<sup>th</sup> observation using the data set <code>snotel2[-9,]</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m5 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="dv">9</span>,])
<span class="kw">summary</span>(m5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2[-9, 
##     ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -29.2918  -4.9757  -0.9146   5.4292  20.4260 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -1.424e+02  9.210e+01  -1.546  0.13773
## Elevation    2.141e-02  6.101e-03   3.509  0.00221
## Min.Temp     6.722e-01  1.733e+00   0.388  0.70217
## Max.Temp     5.078e-01  6.486e-01   0.783  0.44283
## 
## Residual standard error: 11.29 on 20 degrees of freedom
## Multiple R-squared:  0.7522, Adjusted R-squared:  0.715 
## F-statistic: 20.24 on 3 and 20 DF,  p-value: 2.843e-06</code></pre>
<p>(ref:fig8-6) Term-plots for the MLR for Snow Depth based on Elevation,
Min Temp, and Max Temp with Northeast entrance observation removed from
data set (n=24).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(m5), <span class="dt">main=</span><span class="st">&quot;MLR model with NE Ent. Removed&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-6"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-6-1.png" alt="(ref:fig8-6)" width="960" />
<p class="caption">
Figure 8.7: (ref:fig8-6)
</p>
</div>
<p>The estimated MLR model with <span class="math inline">\(n=24\)</span> after removing the influential
“NE Entrance” observation is</p>
<p><span class="math display">\[\widehat{\text{SnowDepth}}_i = -142.4 + 0.0214\cdot\text{Elevation}_i
+0.672\cdot\text{MinTemp}_i +0.508\cdot\text{MaxTemp}_i\ .\]</span></p>
<p>Something unusual has happened here: there is a positive slope for both
temperature terms in Figure <a href="chapter8.html#fig:Figure8-6">8.7</a> that both
contradicts reasonable expectations (warmer temperatures are related to higher
snow levels?) and our original SLR results. So what happened? First, removing
the influential point has drastically changed the slope coefficients (remember
that was the definition of an influential point). Second, when there are
predictors that share information, the results can be somewhat unexpected for
some or all the predictors when they are all in the model together. Note that
the <em>Elevation</em> term looks like what we might expect and seems to have a big
impact on the predicted <em>Snow Depths</em>. So when the temperature variables
are included in the model they might be functioning to explain some differences
in sites that the <em>Elevation</em> term could not explain. This is where our
“adjusting for” terminology comes into play. The unusual-looking slopes for
the temperature effects can be explained
by interpreting them as the estimated change in the response for changes in
temperature <strong>after we control for the impacts of elevation</strong>. Suppose that
<em>Elevation</em> explains most of the variation in <em>Snow Depth</em> except for a few
sites where the elevation cannot explain all the variability and the site
characteristics happen to show higher temperatures and more snow (or lower
temperatures and less snow). This could be because warmer areas might have been
hit by a recent snow storm while colder areas might have been missed (this is
just one day and subject to spatial and temporal fluctuations in precipitation
patterns). Or maybe there is another factor related to having marginally warmer
temperatures that are accompanied by more snow (maybe the lower snow sites for
each elevation were so steep that they couldn’t hold much snow but were also
relatively colder?). Thinking about it this way, the temperature model
components could provide useful corrections to what <em>Elevation</em> is providing in
an overall model and explain more variability than any of the variables could
alone. It is also possible that the
temperature variables are not needed in a model with <em>Elevation</em> in it, are just
“explaining noise”, and should be removed from the model. Each of the next
sections take on various aspects of these
issues and eventually lead to a general set of modeling and model selection
recommendations to help you work in situations as complicated as this.
Exploring the results for this model assumes we trust it and, once again, we
need to check diagnostics before getting too focused on any particular results
from it.</p>
<p>The Residuals vs. Leverage diagnostic plot in Figure <a href="chapter8.html#fig:Figure8-7">8.8</a>
for the model fit to the data set without NE Entrance (now <span class="math inline">\(n=24\)</span>) reveals a new
point that is somewhat influential (point 22 in the data set has Cook’s D
<span class="math inline">\(\approx\)</span> 0.5). It is for a location called "Bloody
<span class="math inline">\(\require{color}\colorbox{black}{Redact.}\)</span><a href="#fn96" class="footnote-ref" id="fnref96"><sup>96</sup></a> which has a
leverage of nearly 0.2 and a standardized residual of nearly -3.
This point did not show up as influential in the original version of the data
set with the same model but it is now. It also shows up as a potential outlier.
As we did before, we can explore it a bit by comparing the model predicted snow
depth to the observed snow depth. The predicted snow depth for this site (see output below for variable values) is</p>
<p><span class="math display">\[\widehat{\text{SnowDepth}}_{22} = -142.4 + 0.0214*\boldsymbol{7550}
+0.672*\boldsymbol{26} +0.508*\boldsymbol{39} = 56.45 \text{ inches.}\]</span></p>
<p>The observed snow depth was 27.2 inches, so the estimated residual is -39.25
inches. Again, this point is potentially influential and an outlier.
Additionally, our model contains results that are not what we would have
expected <em>a priori</em>, so it is not unreasonable to consider removing this
observation to be able to work towards a model that is fully trustworthy.</p>
<p>(ref:fig8-7) Diagnostic plots for MLR for Snow Depth based on Elevation,
Min Temp and Max Temp with Northeast entrance observation removed from
data set.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(m5, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for m5&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-7"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-7-1.png" alt="(ref:fig8-7)" width="960" />
<p class="caption">
Figure 8.8: (ref:fig8-7)
</p>
</div>
<p>(ref:fig8-8) Diagnostic plots for MLR for Snow Depth based on Elevation,
Min Temp and Max Temp with two observations removed (<span class="math inline">\(n=23\)</span>).</p>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2[-c(9, 
##     22), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -14.878  -4.486   0.024   3.996  20.728 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -2.133e+02  7.458e+01  -2.859   0.0100
## Elevation    2.686e-02  4.997e-03   5.374 3.47e-05
## Min.Temp     9.843e-01  1.359e+00   0.724   0.4776
## Max.Temp     1.243e+00  5.452e-01   2.280   0.0343
## 
## Residual standard error: 8.832 on 19 degrees of freedom
## Multiple R-squared:  0.8535, Adjusted R-squared:  0.8304 
## F-statistic:  36.9 on 3 and 19 DF,  p-value: 4.003e-08</code></pre>
<div class="figure"><span id="fig:Figure8-8"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-8-1.png" alt="(ref:fig8-8)" width="960" />
<p class="caption">
Figure 8.9: (ref:fig8-8)
</p>
</div>
<p>This worry-some observation is located in the 22<sup>nd</sup> row of the
original data set:</p>
<pre class="sourceCode r"><code class="sourceCode r">snotel2[<span class="dv">22</span>,]</code></pre>
<pre><code>## # A tibble: 1 x 6
##      ID Station          Max.Temp Min.Temp Elevation Snow.Depth
##   &lt;dbl&gt; &lt;fct&gt;               &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;
## 1    36 Bloody [Redact.]       39       26      7550       27.2</code></pre>
<p>With the removal of both the “Northeast Entrance” and “Bloody
<span class="math inline">\(\require{color}\colorbox{black}{Redact.}\)</span>” sites, there are <span class="math inline">\(n=23\)</span> observations
remaining. This model (<code>m6</code>) seems to contain residual diagnostics (Figure
<a href="chapter8.html#fig:Figure8-8">8.9</a>) that are finally generally reasonable.</p>
<pre class="sourceCode r"><code class="sourceCode r">m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),])
<span class="kw">summary</span>(m6)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(m6, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for m6&quot;</span>)</code></pre>
<p>It is hard to suggest that there any curvature issues and the slight variation
in the Scale-Location plot is mostly
due to few observations with fitted values around 30 happening to be well
approximated by the model. The normality assumption is generally reasonable and
no points seem to be overly influential on this model (finally!).</p>
<p>The term-plots (Figure <a href="chapter8.html#fig:Figure8-9">8.10</a>) show that the temperature slopes
are both positive although in this model <em>Max.Temp</em> seems to be more
“important” than <em>Min.Temp</em>. We have ruled out individual
influential points as the source of un-expected directions in slope coefficients
and the more likely issue is multicollinearity – in a model that
includes <em>Elevation</em>, the temperature
effects may be positive, again acting with the <em>Elevation</em> term to generate
the best possible predictions of the
observed responses. Throughout this discussion, we have mainly focused on the
slope coefficients and diagnostics. We have other tools in MLR to more
quantitatively assess and compare different regression models that are considered
in the next sections.
</p>
<p>(ref:fig8-9) Term-plots for the MLR for Snow Depth based on Elevation,
Min Temp and Max Temp with two observations removed.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(m6), <span class="dt">main=</span><span class="st">&quot;MLR model with n=23&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-9"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-9-1.png" alt="(ref:fig8-9)" width="768" />
<p class="caption">
Figure 8.10: (ref:fig8-9)
</p>
</div>
</div>
<div id="section8-3" class="section level2">
<h2><span class="header-section-number">8.3</span> Interpretation of MLR terms</h2>
<p>Since these results (finally) do not contain any highly influential points,
we can formally discuss
interpretations of the slope coefficients and how the term-plots
(Figure <a href="chapter8.html#fig:Figure8-9">8.10</a>) aid
our interpretations. Term-plots in MLR are constructed by holding all the other
variables at their mean and generating predictions and 95% CIs for the mean
response across the levels of observed values for each predictor variable. This
idea also helps us to work towards interpretations of each term in an MLR
model. For example, for <em>Elevation</em>,
the term-plot starts at an elevation around 5000 feet and ends at an elevation
around 8000 feet. To generate that line and CIs for the mean snow depth at
different elevations, the MLR model of</p>
<p><span class="math display">\[\widehat{\text{SnowDepth}}_i = -213.3 + 0.0269\cdot\text{Elevation}_i
+0.984\cdot\text{MinTemp}_i +1.243\cdot\text{MaxTemp}_i\]</span></p>
<p>is used, but we need to have “something” to put in for the two temperature
variables to predict <em>Snow Depth</em> for different <em>Elevations</em>. The typical
convention is to hold the “other” variables at their means to
generate these plots. This tactic also provides a way of interpreting each
slope coefficient. Specifically, we can interpret the <em>Elevation</em> slope as:
For a 1 foot increase in <em>Elevation</em>, we expect the mean <em>Snow Depth</em> to
increase by 0.0269 inches, holding the minimum and maximum temperatures
constant. More generally, the <strong><em>slope interpretation in an MLR</em></strong> is:
</p>
<blockquote>
<p>For a 1 <strong>[<em>units of <span class="math inline">\(\boldsymbol{x_k}\)</span></em>]</strong> increase in
<span class="math inline">\(\boldsymbol{x_k}\)</span>, we expect the mean of <span class="math inline">\(\boldsymbol{y}\)</span> to change by
<span class="math inline">\(\boldsymbol{b_k}\)</span> <strong>[<em>units of y</em>]</strong>, after controlling for
<strong>[list of other explanatory variables in model]</strong>.</p>
</blockquote>
<p>To make this more concrete, we can recreate some points in the Elevation
term-plot. To do this, we first need the mean of the “other” predictors,
<em>Min.Temp</em> and <em>Max.Temp</em>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),]<span class="op">$</span>Min.Temp)</code></pre>
<pre><code>## [1] 27.82609</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mean</span>(snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),]<span class="op">$</span>Max.Temp)</code></pre>
<pre><code>## [1] 36.3913</code></pre>
<p>We can put these values into the MLR equation and simplify it by combining like terms, to
an equation that is in terms of
just <em>Elevation</em> given that we are holding <em>Min.Temp</em> and <em>Max.Temp</em> at their
means:</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{SnowDepth}}_i &amp;= -213.3 + 0.0269\cdot\text{Elevation}_i
+0.984*\boldsymbol{27.826} +1.243*\boldsymbol{36.391} \\
&amp;= -213.3 + 0.0269\cdot\text{Elevation}_i + 27.38 + 45.23 \\
&amp;= \boldsymbol{-140.69 + 0.0269\cdot\textbf{Elevation}_i}.
\end{array}\]</span></p>
<p>So at the means on the two temperature variables,
the model looks like an SLR with an estimated y-intercept of -140.69 (mean
<em>Snow Depth</em> for <em>Elevation</em> of 0 if temperatures are at their means)
and an estimated
slope of 0.0269. Then we can plot the predicted changes in <span class="math inline">\(y\)</span> across all the
values of the predictor variable (<em>Elevation</em>) while holding the other
variables constant. To generate the needed values to define a line, we can plug
various <em>Elevation</em> values into the simplified equation:</p>
<ul>
<li><p>For an elevation of 5000 at the average temperatures, we predict a mean snow
depth of <span class="math inline">\(-140.69 + 0.0269*5000 = -6.19\)</span> inches.</p></li>
<li><p>For an elevation of 6000 at the average temperatures, we predict a mean snow
depth of <span class="math inline">\(-140.69 + 0.0269*6000 = 20.71\)</span> inches.</p></li>
<li><p>For an elevation of 8000 at the average temperatures, we predict a mean snow
depth of <span class="math inline">\(-140.69 + 0.0269*8000 = 74.51\)</span> inches.</p></li>
</ul>
<p>We can plot this information (Figure <a href="chapter8.html#fig:Figure8-10">8.11</a>) using the <code>plot</code>
function to show the points we calculated and the <code>lines</code> function to add
a line that connects the dots. In the <code>plot</code> function, we used the
<code>ylim=...</code> option to make the scaling on the y-axis match the previous
term-plot’s scaling.</p>
<p>(ref:fig8-10) Term-plot for Elevation “by-hand”, holding temperature
variables constant at their means.</p>
<div class="figure"><span id="fig:Figure8-10"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-10-1.png" alt="(ref:fig8-10)" width="576" />
<p class="caption">
Figure 8.11: (ref:fig8-10)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Making own effect plot:</span>
elevs &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">5000</span>,<span class="dv">6000</span>,<span class="dv">8000</span>)
snowdepths &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">6.19</span>,<span class="fl">20.71</span>,<span class="fl">74.51</span>)
<span class="kw">plot</span>(snowdepths<span class="op">~</span>elevs, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">20</span>,<span class="dv">90</span>), <span class="dt">cex=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,
     <span class="dt">main=</span><span class="st">&quot;Effect plot of elevation by hand&quot;</span>)
<span class="kw">lines</span>(snowdepths<span class="op">~</span>elevs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p>Note that we only needed 2 points to
define the line but need a denser grid of elevations if we want to add the 95%
CIs for the true mean snow depth across the different elevations since they
vary as a function of the distance from the mean of the explanatory variables.</p>
<p>To get the associated 95% CIs, we could return to using the <code>predict</code>
function for the MLR, again holding the temperatures at their
mean values. The <code>predict</code> function is sensitive and needs the same variable
names as used in the original model fitting to work. First we create a “new”
data set using the <code>seq</code> function to generate the desired grid of
elevations and the <code>rep</code> function<a href="#fn97" class="footnote-ref" id="fnref97"><sup>97</sup></a> to repeat the means of the
temperatures for each of elevation values we need to make the plot. The code
creates a specific version of the predictor variables to force the <code>predict</code>
function to provide fitted values and CIs across different elevations with
temperatures held constant that is stored in <code>newdata1</code>.  </p>
<pre class="sourceCode r"><code class="sourceCode r">elevs &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">5000</span>, <span class="dt">to=</span><span class="dv">8000</span>, <span class="dt">length.out=</span><span class="dv">30</span>)
newdata1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">Elevation=</span>elevs, <span class="dt">Min.Temp=</span><span class="kw">rep</span>(<span class="fl">27.826</span>,<span class="dv">30</span>),
                       <span class="dt">Max.Temp=</span><span class="kw">rep</span>(<span class="fl">36.3913</span>,<span class="dv">30</span>))
newdata1</code></pre>
<pre><code>## # A tibble: 30 x 3
##    Elevation Min.Temp Max.Temp
##        &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
##  1     5000      27.8     36.4
##  2     5103.     27.8     36.4
##  3     5207.     27.8     36.4
##  4     5310.     27.8     36.4
##  5     5414.     27.8     36.4
##  6     5517.     27.8     36.4
##  7     5621.     27.8     36.4
##  8     5724.     27.8     36.4
##  9     5828.     27.8     36.4
## 10     5931.     27.8     36.4
## # ... with 20 more rows</code></pre>
<p>The predicted snow depths along with 95% confidence intervals for the mean,
holding temperatures at their means, are:</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(m6, <span class="dt">newdata=</span>newdata1, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)</code></pre>
<pre><code>##           fit        lwr      upr
## 1  -6.3680312 -24.913607 12.17754
## 2  -3.5898846 -21.078518 13.89875
## 3  -0.8117379 -17.246692 15.62322
## 4   1.9664088 -13.418801 17.35162
## 5   4.7445555  -9.595708 19.08482
## 6   7.5227022  -5.778543 20.82395
## 7  10.3008489  -1.968814 22.57051
## 8  13.0789956   1.831433 24.32656
## 9  15.8571423   5.619359 26.09493
## 10 18.6352890   9.390924 27.87965
## 11 21.4134357  13.140233 29.68664
## 12 24.1915824  16.858439 31.52473
## 13 26.9697291  20.531902 33.40756
## 14 29.7478758  24.139153 35.35660
## 15 32.5260225  27.646326 37.40572
## 16 35.3041692  31.002236 39.60610
## 17 38.0823159  34.139812 42.02482
## 18 40.8604626  36.997617 44.72331
## 19 43.6386092  39.559231 47.71799
## 20 46.4167559  41.866745 50.96677
## 21 49.1949026  43.988619 54.40119
## 22 51.9730493  45.985587 57.96051
## 23 54.7511960  47.900244 61.60215
## 24 57.5293427  49.759987 65.29870
## 25 60.3074894  51.582137 69.03284
## 26 63.0856361  53.377796 72.79348
## 27 65.8637828  55.154251 76.57331
## 28 68.6419295  56.916422 80.36744
## 29 71.4200762  58.667725 84.17243
## 30 74.1982229  60.410585 87.98586</code></pre>

<p>So we could do this with any model <strong>for each predictor</strong> variable to create
term-plots, or we can just use the <code>allEffects</code> function to do this for us.
This exercise is useful to complete once to understand what is being displayed
in term-plots but using the <code>allEffects</code> function makes getting these plots
much easier.
</p>
<p>There are two other model components of possible interest in this model. The
slope of 0.984 for <em>Min.Temp</em> suggests that for a 1<span class="math inline">\(^\circ F\)</span> increase in
<em>Minimum Temperature</em>, we expect a 0.984 inch change in the mean <em>Snow Depth</em>,
after controlling for <em>Elevation</em> and <em>Max.Temp</em> at the sites. Similarly, the
slope of 1.243 for the <em>Max.Temp</em> suggests that for a 1<span class="math inline">\(^\circ F\)</span> increase in
<em>Maximum Temperature</em>, we expect a 1.243 inch change in the mean <em>Snow Depth</em>,
holding <em>Elevation</em> and <em>Min.Temp</em> constant. Note that there are a variety of ways
to note that each term in an
MLR is only a particular value given the other variables in the model. We can
use words such as “holding the other variables constant” or “after adjusting
for the other variables” or “in a model with…”. The main
point is to find words that reflect that this single slope coefficient might be
different if we had a different overall model and the only way to interpret it
is conditional on the other model components.</p>
<p>Term-plots have a few general uses to enhance our regular slope
interpretations. They can help us assess how much change in the mean of <span class="math inline">\(y\)</span> the model predicts over the range of each
observed <span class="math inline">\(x\)</span>. This can
help you to get a sense of the “practical” importance of each term.
Additionally, the term-plots show 95% confidence intervals for the mean response
across the range of each variable,
holding the other variables at their means. These intervals can be useful for
assessing the precision in the estimated mean at different values of each
predictor. However, note that you should not use these plots for deciding
whether the term should be retained in the model – we have other tools for
making that assessment. And one last note about term-plots – they do not mean
that the relationships are really linear between the predictor and response
variable being displayed. The model <strong>forces</strong> the relationship to be linear
even if that is not the real functional form. <strong>Term-plots are not diagnostics
for the model, they are summaries of the model you assumed was correct!</strong>
Any time we do linear regression, the inferences are contingent upon the
model we chose. We know our model is not
perfect, but we hope that it helps us learn something about our research
question(s).</p>
</div>
<div id="section8-4" class="section level2">
<h2><span class="header-section-number">8.4</span> Comparing multiple regression models</h2>
<p>With more than one variable, we now have many potential models that we could
consider.

We could include only one of the predictors, all of them, or
combinations of sets of the
variables. For example, maybe the model that includes <em>Elevation</em> does not “need”
both <em>Min.Temp</em> and <em>Max.Temp</em>? Or maybe the model isn’t improved over an SLR
with just <em>Elevation</em> as a predictor. Or maybe none of the predictors are
“useful”? In this section,
we discuss some general model comparison issues and a metric that can be used
to pick among a suite of different models (often called a set of <strong><em>candidate
models</em></strong> to reflect that they are all potentially interesting and we need to
compare them and possibly pick one). </p>
<p>It is certainly possible the researchers may have an <em>a priori</em>
reason to only consider a single model. For example, in a designed experiment
where combinations of, say, three different predictors are randomly assigned,
the initial model with all three predictors may be sufficient to address all the
research questions of interest. One advantage in these situations is that the
variable combinations can be created to prevent multicollinearity among the
predictors and avoid that complication in interpretations. However, this is
more the exception than the rule. Usually, there are competing predictors or
questions about whether some predictors matter more than others. This type of
research always introduces the potential for multicollinearity to complicate the
interpretation of each predictor in the presence of others. Because of this,
multiple models are often considered, where “unimportant” variables are dropped
from the model. The assessment of “importance” using p-values will be discussed
in Section <a href="chapter8.html#section8-6">8.6</a>, but for now we will consider other reasons to pick
one model over another. </p>
<p>There are some general reasons to choose a particular model:</p>
<ol style="list-style-type: decimal">
<li><p>Diagnostics are better with one model compared to others.</p></li>
<li><p>One model predicts/explains the responses better than the others
(<strong><em>R</em></strong><sup>2</sup>).</p></li>
<li><p><em>a priori</em> reasons to “use” a particular model, for example in a designed
experiment or it includes variable(s) whose slopes need to be estimated to find
interesting results (even if the variables are not “important” in the model).</p></li>
<li><p>Model selection “criteria” suggest one model is better than the others<a href="#fn98" class="footnote-ref" id="fnref98"><sup>98</sup></a>.</p></li>
</ol>
<p>It is OK to consider multiple reasons to select a model but it is dangerous to
“shop” for a model across many possible models – a practice which is sometimes
called <strong><em>data-dredging</em></strong> and leads to a high chance of spurious results from a
single model that is usually reported based on this type of exploration. Just
like in other discussions of
multiple testing issues previously, if you explore many versions of a model,
maybe only keeping the best ones, this is very different from picking one model
(and tests) <em>a priori</em> and just exploring that result.</p>
<p>As in SLR, we can use the <strong><em>R</em></strong><sup>2</sup> (the <strong><em>coefficient of
determination</em></strong>)  to measure the percentage of the variation in the
response variable that the model explains. In MLR, it is important to remember
that <strong><em>R</em></strong><sup>2</sup> is now an overall
measure for the model and not specific to a single variable. It is comparable to
other models including those fit with only a single predictor (SLR). So to
meet criterion (2), we could simply find the model with the largest
<strong><em>R</em></strong><sup>2</sup> value, finding the model that explains the most variation in
the responses.
Unfortunately for this idea, when you add more “stuff” to a regression model
(even “unimportant” predictors), the <strong><em>R</em></strong><sup>2</sup> will always go up. This
can be seen by considering</p>
<p><span class="math display">\[R^2 = \frac{\text{SS}_{\text{regression}}}{\text{SS}_{\text{total}}}\ 
\text{ where }\  \text{SS}_{\text{regression}} = \text{SS}_{\text{total}}
- \text{SS}_{\text{error}}\ 
\text{ and }\  \text{SS}_{\text{error}} = \Sigma(y-\hat{y})^2\ .\]</span></p>
<p>Because adding extra variables to a linear model will only make the fitted
values better, not worse, the <span class="math inline">\(\text{SS}_{\text{error}}\)</span> will always go down
if more predictors are added to the model. If <span class="math inline">\(\text{SS}_{\text{error}}\)</span>
goes down and <span class="math inline">\(\text{SS}_{\text{total}}\)</span> is fixed, then adding extra variables
will always increase <span class="math inline">\(\text{SS}_{\text{regression}}\)</span> and, thus, increase
<strong><em>R</em></strong><sup>2</sup>. This means that <strong><em>R</em></strong><sup>2</sup> is only useful for
selecting models when you are picking between two models of the same size
(same number of predictors). So we mainly use it as a summary of model quality
once we pick a model, not a method of picking among a set of candidate models.
Remember that <strong><em>R</em></strong><sup>2</sup> continues to have the property of being between
0 and 1 (or 0% and 100%) and that value refers to the <strong>proportion (percentage)
of variation in the response explained by the model</strong>, whether we are using it
for SLR or MLR.</p>
<p>However, there is an adjustment to the <strong><em>R</em></strong><sup>2</sup> measure that makes it
useful for selecting among models. The measure is called the <strong><em>adjusted</em></strong>
<strong><em>R</em></strong><sup>2</sup>.  The <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> measure adds a
penalty for adding more variables to the model, providing the potential for this
measure to decrease if the extra variables do not really benefit the model. The
measure is calculated as</p>
<p><span class="math display">\[R^2_{\text{adjusted}} = 1 - 
\frac{\text{SS}_{\text{error}}/df_{\text{error}}}{\text{SS}_{\text{total}}/(N-1)}
= 1 - \frac{\text{MS}_{\text{error}}}{\text{MS}_{\text{total}}},\]</span></p>
<p>which incorporates the <em>degrees of freedom</em> for the model via the error
<em>degrees of freedom</em> which go
down as the model complexity increases. This adjustment means that just
adding extra useless variables (variables that do not explain very much extra
variation) do not increase this measure. That makes this measure useful for
model selection since it can help us to stop adding unimportant variables and
find a “good” model among a set of candidates. Like the regular
<strong><em>R</em></strong><sup>2</sup>, larger values are better. The downside to
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is that it <strong>is no longer a percentage
of variation in the response that is explained by the model</strong>; it can be less
than 0 and so has no interpretable scale. It is just “larger is better”. It
provides
one method for building a model (different from using p-values to drop
unimportant variables as discussed below), by fitting a set of candidate models
containing different variables and then <strong>picking the model with the largest</strong>
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>.

You will want to interpret this new
measure on a percentage scale, but do not do that. It is a just a measure to
help you pick a model and that is all it is!</p>
<p>One other caveat in
model comparison is worth mentioning: make sure you are comparing models for
the same responses. That may sound trivial and usually it is. But when there
are missing values in the data set, especially on some explanatory variables
and not others, it is important to be careful that the <span class="math inline">\(y\text{&#39;s}\)</span> do not
change between models you are comparing. This relates to our <em>Snow Depth</em>
modeling because responses were being removed due to their influential nature.
We can’t compare <strong><em>R</em></strong><sup>2</sup> or <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
for <span class="math inline">\(n=25\)</span> to a model when <span class="math inline">\(n=23\)</span> – it isn’t a fair comparison on
either measure since they based on the total variability which is changing as
the responses used change.</p>
<p>In the MLR (or SLR) model summaries, both the <strong><em>R</em></strong><sup>2</sup> and
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
are available. Make sure you are able to pick out the correct one. For the
reduced data set (<span class="math inline">\(n=23\)</span>) <em>Snow Depth</em> models, the pertinent part of the model
summary for the model with all three predictors is the last three lines:</p>
<pre class="sourceCode r"><code class="sourceCode r">m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),])
<span class="kw">summary</span>(m6)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2[-c(9, 
##     22), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -14.878  -4.486   0.024   3.996  20.728 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -2.133e+02  7.458e+01  -2.859   0.0100
## Elevation    2.686e-02  4.997e-03   5.374 3.47e-05
## Min.Temp     9.843e-01  1.359e+00   0.724   0.4776
## Max.Temp     1.243e+00  5.452e-01   2.280   0.0343
## 
## Residual standard error: 8.832 on 19 degrees of freedom
## Multiple R-squared:  0.8535, Adjusted R-squared:  0.8304 
## F-statistic:  36.9 on 3 and 19 DF,  p-value: 4.003e-08</code></pre>
<p>There is a value for <span class="math inline">\(\large{\textbf{Multiple R-Squared}} \text{ of } 0.8535\)</span>,
this is the <strong><em>R</em></strong><sup>2</sup> value and suggests that the model with
<em>Elevation</em>, <em>Min</em> and <em>Max</em> temperatures explains 85.4% of the variation in
<em>Snow Depth</em>. The <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is 0.8304 and is
available further to the right labeled as
<span class="math inline">\(\color{red}{\large{\textbf{Adjusted R-Squared}}}\)</span>. We repeated this for a
suite of different models for this same <span class="math inline">\(n=23\)</span> data set and found the following
results in Table <a href="chapter8.html#tab:Table8-1">8.1</a>. The top <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>
model is the model with <em>Elevation</em> and <em>Max.Temp</em>, which beats out the model with
all three variables on <span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>. Note that the top
<em>R</em><sup>2</sup> model is the model with three predictors, but the most complicated model
will always have that characteristic.</p>

<table style="width:100%;">
<caption><span id="tab:Table8-1">Table 8.1: </span> Model comparisons for Snow Depth data, sorted by model complexity.</caption>
<colgroup>
<col width="33%" />
<col width="10%" />
<col width="11%" />
<col width="21%" />
<col width="21%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Model</strong>        </th>
<th align="center"><span class="math inline">\(\boldsymbol{K}\)</span></th>
<th align="right"><span class="math inline">\(\boldsymbol{R^2}\)</span></th>
<th align="right"><span class="math inline">\(\boldsymbol{R^2_{\text{adjusted}}}\)</span></th>
<th align="center"><span class="math inline">\(\boldsymbol{R^2_{\text{adjusted}}}\)</span>
<strong>Rank</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation</td>
<td align="center">1</td>
<td align="right">0.8087</td>
<td align="right">0.7996</td>
<td align="center">3</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Min.Temp</td>
<td align="center">1</td>
<td align="right">0.6283</td>
<td align="right">0.6106</td>
<td align="center">5</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Max.Temp</td>
<td align="center">1</td>
<td align="right">0.4131</td>
<td align="right">0.3852</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Min.Temp</td>
<td align="center">2</td>
<td align="right">0.8134</td>
<td align="right">0.7948</td>
<td align="center">4</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Max.Temp</td>
<td align="center">2</td>
<td align="right">0.8495</td>
<td align="right">0.8344</td>
<td align="center">1</td>
</tr>
<tr class="even">
<td align="left">SD <span class="math inline">\(\sim\)</span> Min.Temp + Max.Temp</td>
<td align="center">2</td>
<td align="right">0.6308</td>
<td align="right">0.5939</td>
<td align="center">6</td>
</tr>
<tr class="odd">
<td align="left">SD <span class="math inline">\(\sim\)</span> Elevation + Min.Temp
+ Max.Temp</td>
<td align="center">3</td>
<td align="right">0.8535</td>
<td align="right">0.8304</td>
<td align="center">2</td>
</tr>
</tbody>
</table>
<p>The top adjusted <strong><em>R</em></strong><sup>2</sup> model contained <em>Elevation</em> and <em>Max.Temp</em> and has an <strong><em>R</em></strong><sup>2</sup> of
0.8495, so we can say that the model with
<em>Elevation</em> and <em>Maximum Temperature</em> explains 84.95% percent of the variation
in <em>Snow Depth</em> and also that this model was selected based on the
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span>. One of the important features of
<span class="math inline">\(\boldsymbol{R}^2_{\text{adjusted}}\)</span> is available in this example – adding
variables often does not always increase its value even though
<strong><em>R</em></strong><sup>2</sup> does increase with <strong>any</strong> addition. In
Section <a href="chapter8.html#section8-13">8.13</a> we consider a competitor for this model selection
criterion that may “work” a bit better and be extendable into more complicated
modeling situations; that measure is called the <strong><em>AIC</em></strong>.</p>
</div>
<div id="section8-5" class="section level2">
<h2><span class="header-section-number">8.5</span> General recommendations for MLR interpretations and VIFs</h2>
<p>There are some important issues to remember<a href="#fn99" class="footnote-ref" id="fnref99"><sup>99</sup></a> when interpreting
regression models that can result in common mistakes.</p>
<ul>
<li><p><strong>Don’t claim to “hold everything constant” for a single individual</strong>:</p>
<p>Mathematically this is a correct interpretation of the MLR model but it is
rarely the case that we could have this occur in real applications. Is it
possible to increase the <em>Elevation</em> while holding the <em>Max.Temp</em> constant?
We discussed making term-plots doing exactly this – holding the other
variables constant at their means. If we
interpret each slope coefficient in an MLR conditionally then we can craft
interpretations such as: For locations that have a <em>Max.Temp</em> of, say,
<span class="math inline">\(45^\circ F\)</span> and <em>Min.Temp</em> of, say, <span class="math inline">\(30^\circ F\)</span>, a 1 foot increase in
<em>Elevation</em> tends to be associated with a 0.0268 inch increase in <em>Snow Depth</em>
on average. This does not try to imply that we can actually make that sort
of change but that given those other variables, the change for that variable
is a certain magnitude.</p></li>
<li><p><strong>Don’t interpret the regression results causally (or casually?)…</strong></p>
<p>Unless you are analyzing the results of a designed experiment (where the
levels
of the explanatory variable(s) were randomly assigned) you cannot state that a
change in that <span class="math inline">\(x\)</span> <strong>causes</strong> a change in <span class="math inline">\(y\)</span>, especially for a given
individual. The multicollinearity in predictors makes it especially difficult
to
put too much emphasis on a single slope coefficient because it may
be corrupted/modified by the other variables being in the model. In
observational studies, there are also all the potential lurking variables that
we did not measure or even confounding variables that we did measure but can’t
disentangle from the variable used in a particular model.

While we do have a
complicated mathematical model relating various <span class="math inline">\(x\text{&#39;s}\)</span> to the response,
do not lose that fundamental focus on causal vs non-causal inferences based on
the design of the study.</p></li>
<li><p><strong>Be cautious about doing prediction in MLR – you might be doing extrapolation!</strong></p>
<p>It is harder to know if you are doing extrapolation in MLR since you could be
in a region of the <span class="math inline">\(x\text{&#39;s}\)</span> that no observations were obtained. Suppose we
want to predict the <em>Snow Depth</em> for an <em>Elevation</em> of 6000 and <em>Max.Temp</em>
of 30. Is this extrapolation based on Figure <a href="chapter8.html#fig:Figure8-11">8.12</a>? In other
words, can you find any observations “nearby” in the plot of the two
variables together? What about an <em>Elevation</em> of 6000 and a <em>Max.Temp</em> of
40? The first prediction is in a different proximity to observations than
the second one… In situations with more than two explanatory variables it
becomes even more challenging to know whether you are doing extrapolation
and the problem grows as the number of dimensions to search increases… In
fact, in complicated MLR models we typically do not know whether there are
observations “nearby” if we are doing predictions for unobserved
combinations of our predictors. Note that Figure
<a href="chapter8.html#fig:Figure8-11">8.12</a> also reinforces our potential collinearity problem
between <em>Elevation</em> and <em>Max.Temp</em> with higher elevations being strongly
associated with lower temperatures.</p>
<div class="figure"><span id="fig:Figure8-11"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-11-1.png" alt="Scatterplot of observed Elevations and Maximum Temperatures for SNOTEL data." width="480" />
<p class="caption">
Figure 8.12: Scatterplot of observed Elevations and Maximum Temperatures for SNOTEL data.
</p>
</div></li>
<li><p><strong>Don’t think that the sign of a coefficient is special…</strong></p>
<p>Adding other variables into the MLR models can cause a switch in the
coefficients or change their magnitude or make them go from “important” to
“unimportant” without changing the slope too much. This is related to the
conditionality of the relationships being estimated in MLR and the potential
for sharing of information in the predictors when it is present.</p></li>
<li><p><strong>Multicollinearity in MLR models:</strong></p>
<p>When explanatory variables are not independent (related) to one another, then
including one variable will have an impact on the other variable. Consider the
correlations among the predictors in the SNOTEL data set or visually displayed
in Figure <a href="chapter8.html#fig:Figure8-12">8.13</a>:

</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(corrplot)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">0</span>))
<span class="kw">corrplot.mixed</span>(<span class="kw">cor</span>(snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),<span class="dv">3</span><span class="op">:</span><span class="dv">6</span>]), <span class="dt">upper.col=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="st">&quot;orange&quot;</span>),
               <span class="dt">lower.col=</span><span class="kw">c</span>(<span class="dv">1</span>, <span class="st">&quot;orange&quot;</span>))
<span class="kw">round</span>(<span class="kw">cor</span>(snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),<span class="dv">3</span><span class="op">:</span><span class="dv">6</span>]),<span class="dv">2</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-12"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-12-1.png" alt="Plot of correlation matrix in the snow depth data set with influential points removed" width="624" />
<p class="caption">
Figure 8.13: Plot of correlation matrix in the snow depth data set with influential points removed
</p>
</div>
<pre><code>##            Max.Temp Min.Temp Elevation Snow.Depth
## Max.Temp       1.00     0.77     -0.84      -0.64
## Min.Temp       0.77     1.00     -0.91      -0.79
## Elevation     -0.84    -0.91      1.00       0.90
## Snow.Depth    -0.64    -0.79      0.90       1.00</code></pre>
<p>The predictors all share at least moderately strong linear relationships. For
example, the <span class="math inline">\(\boldsymbol{r}=-0.91\)</span> between <em>Min.Temp</em> and <em>Elevation</em>
suggests 
that they contain very similar information and that extends to other pairs of
variables as well. When variables share information, their addition to models
may not improve the performance of the model and actually can make the
estimated
coefficients <strong><em>unstable</em></strong>, creating uncertainty in the correct coefficients
because of the shared information. It seems that <em>Elevation</em> is related to
<em>Snow Depth</em> but maybe it is because it has lower <em>Minimum Temperatures</em>? So
you might wonder how we can find the “correct” slopes when they are sharing
information in the response variable. The short answer is that we can’t. But
we
do use <strong><em>Least Squares</em></strong> to find coefficient estimates as we did before –
except that we have to remember that these <strong>estimates are conditional on
other
variables in the model</strong> for our interpretation since they impact one another
within the model. It ends up that the uncertainty
of pinning those variables down in the presence of shared information leads to
larger SEs for all the slopes. And that we can actually measure <strong>how much
each of the SEs are inflated</strong> because of multicollinearity with other
variables
in the model using what are called <strong><em>Variance Inflation Factors</em></strong> (or
<strong><em>VIFs</em></strong>).</p></li>
</ul>
<p><strong><em>VIFs</em></strong> provide a way to assess the multicollinearity in the MLR model that
is caused by including specific variables.  The amount of information that is
shared between a single explanatory variable and the others can be found by
regressing that variable on the others and calculating <strong><em>R</em></strong><sup>2</sup>
for that model. The code for this regression is something like:
<code>lm(X1~X2+X3+...+XK)</code>, which regresses <em>X1</em>on <em>X2</em> through <em>XK</em>.

The
<span class="math inline">\(1-\boldsymbol{R}^2\)</span> from this regression is the amount of independent
information in <em>X1</em> that is not explained by the other variables in the model. 
The VIF for each variable is defined using this quantity as
<span class="math inline">\(\textbf{VIF}_{\boldsymbol{k}}\boldsymbol{=1/(1-R^2_k)}\)</span> for variable <span class="math inline">\(k\)</span>.
If there is no shared information <span class="math inline">\((\boldsymbol{R}^2=0)\)</span>, then the VIF will be
1. But if the information is completely shared with other variables
<span class="math inline">\((\boldsymbol{R}^2=1)\)</span>, then the VIF goes to infinity (1/0). Basically, large
VIFs are bad, with the rule of thumb that values over 5 or 10 are considered
“large” values indicating high multicollinearity in the model for that particular
variable. We use this scale to determine if multicollinearity is a problem for a
variable of interest. Additionally, the <span class="math inline">\(\boldsymbol{\sqrt{\textbf{VIF}_k}}\)</span> is
also very interesting as it is the number of times larger than the SE for the
slope for variable <span class="math inline">\(k\)</span> is due to collinearity with other variables in the model.
This is the most useful scale to understand VIFs even though the rules of thumb
are on the original scale. An example will show how to easily get these results
and where the results come from.</p>
<p>In general, the easy way to obtain VIFs is using the <code>vif</code> function from the
<code>car</code> package (<span class="citation">Fox, Weisberg, and Price (<a href="#ref-R-carData" role="doc-biblioref">2018</a><a href="#ref-R-carData" role="doc-biblioref">b</a>)</span>, <span class="citation">Fox (<a href="#ref-Fox2003" role="doc-biblioref">2003</a>)</span>).
 
It has the advantage of also providing a reasonable
result when we include categorical variables in models
(Sections <a href="chapter8.html#section8-9">8.9</a> and <a href="chapter8.html#section8-11">8.11</a>) over some other sources of this information in R. We apply the <code>vif</code>
function directly to a model of interest and it generates values for each explanatory variable.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(car)
<span class="kw">vif</span>(m6)</code></pre>
<pre><code>## Elevation  Min.Temp  Max.Temp 
##  8.164201  5.995301  3.350914</code></pre>
<p>Not surprisingly, there is an indication of problems with multicollinearity in
two of the three variables in the model with the largest issues identified for
<em>Elevation</em> and <em>Min.Temp</em>. Both of their VIFs exceed 5 indicating large
multicollinearity problems. On the square-root scale, the VIFs show more
interpretation utility.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">vif</span>(m6))</code></pre>
<pre><code>## Elevation  Min.Temp  Max.Temp 
##  2.857307  2.448530  1.830550</code></pre>
<p>The result for <em>Elevation</em> of 2.86 suggests that the SE for <em>Elevation</em> is 2.86
times larger than it should be because of multicollinearity with other variables
in the model. Similarly, the <em>Min.Temp</em> SE is 2.45 times larger and the
<em>Max.Temp</em> SE is 1.83 times larger. All of this generally suggests issues with
multicollinearity in the model and that we need to be cautious in interpreting
any slope coefficients from this model because they are all being impacted by shared information in the predictor variables.</p>
<p>In order to see how the VIF is calculated for <em>Elevation</em>, we need to
regress <em>Elevation</em> on <em>Min.Temp</em> and <em>Max.Temp</em>. Note that this model is only
fit to find the percentage of variation in elevation explained by the temperature
variables. It ends up being 0.8775 – so a high percentage of <em>Elevation</em> can be
explained by the linear model using min and max temperatures.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#VIF calc:</span>
elev1 &lt;-<span class="st"> </span><span class="kw">lm</span>(Elevation<span class="op">~</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),])
<span class="kw">summary</span>(elev1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Elevation ~ Min.Temp + Max.Temp, data = snotel2[-c(9, 
##     22), ])
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1120.05  -142.99    14.45   186.73   624.61 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 14593.21     699.77  20.854 4.85e-15
## Min.Temp     -208.82      38.94  -5.363 3.00e-05
## Max.Temp      -56.28      20.90  -2.693    0.014
## 
## Residual standard error: 395.2 on 20 degrees of freedom
## Multiple R-squared:  0.8775, Adjusted R-squared:  0.8653 
## F-statistic: 71.64 on 2 and 20 DF,  p-value: 7.601e-10</code></pre>

<p>Using this result, we can calculate</p>
<p><span class="math display">\[\text{VIF}_{\text{elevation}} = \dfrac{1}{1-R^2_{\text{elevation}}} = \dfrac{1}{1-0.8775} = \dfrac{1}{0.1225} = 8.16\]</span></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.8775</span></code></pre>
<pre><code>## [1] 0.1225</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">1</span><span class="op">/</span><span class="fl">0.1225</span></code></pre>
<pre><code>## [1] 8.163265</code></pre>
<p>Note that when we observe small VIFs, that provides us with confidence that
multicollinearity is not causing problems under the surface of
a particular MLR model. And that we can’t use the VIFs to do anything about
multicollinearity in the models – it is just a diagnostic to understand the
magnitude of the problem.</p>
</div>
<div id="section8-6" class="section level2">
<h2><span class="header-section-number">8.6</span> MLR inference: Parameter inferences using the t-distribution</h2>
<p>We have been deliberately vague about what an important variable is up to this
point, and chose to focus on some bigger modeling issues. We now turn our
attention to one
of the most common tasks in any basic statistical model – assessing whether a
particular result is more unusual than we would expect by chance. The
previous discussions of estimation in MLR models informs our interpretations of
of the tests.

The <span class="math inline">\(t\)</span>-tests for slope coefficients are based on our standard
recipe – take the estimate, divide it by its standard error and then, assuming
the statistic follows a <span class="math inline">\(t\)</span>-distribution under the null hypothesis, find a
p-value.

This tests whether each true slope coefficient, <span class="math inline">\(\beta_k\)</span>, is 0 or not,
in a model that contains the other variables. Again, sometimes we say
“after adjusting for” the other <span class="math inline">\(x\text{&#39;s}\)</span> or
“conditional on” the other <span class="math inline">\(x\text{&#39;s}\)</span> in the model or “after allowing for”…
as in the slope coefficient interpretations above. The main point is that
<strong>you should not interpret anything related to slope coefficients in MLR without
referencing the other variables that are in the model!</strong> The tests for the
slope coefficients assess <span class="math inline">\(\boldsymbol{H_0:\beta_k=0}\)</span>, which in words is
a test that there is no linear relationship between explanatory variable <span class="math inline">\(k\)</span>
and the response variable, <span class="math inline">\(y\)</span>, in the population, given the other variables in
model. The typical alternative hypothesis is <span class="math inline">\(\boldsymbol{H_0:\beta_k\ne 0}\)</span>.
In words, the alternative hypothesis is that there is some linear relationship
between explanatory variable <span class="math inline">\(k\)</span> and the response variable, <span class="math inline">\(y\)</span>, in population,
given the other variables in the model. It is also possible to test for positive
or negative slopes in the alternative, but this is rarely the first concern,
especially when MLR slopes can occasionally come out in unexpected directions.</p>
<p>The test statistic for these hypotheses is
<span class="math inline">\(\boldsymbol{t=\dfrac{b_k}{\textbf{SE}_k}}\)</span> and, if our assumptions are met,
follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-K-1\)</span> <em>df</em> where <span class="math inline">\(K\)</span> is the number of
predictor variables in the model.

We perform the test for each slope
coefficient, but the test is conditional on the other variables in the model – the order the variables are fit in does
<strong>not</strong> change <span class="math inline">\(t\)</span>-test results. For the <em>Snow Depth</em> example with <strong>Elevation</strong>
and <em>Maximum Temperature</em> as predictors, the pertinent output is in the four
columns of the <strong><em>Coefficient table</em></strong> that is the first part of the model
summary we’ve been working with. You can find the estimated slope
(<code>Estimate</code> column), the SE of the slopes (<code>Std. Error</code> column), the
<span class="math inline">\(t\)</span>-statistics (<code>t value</code> column), and the p-values (<code>Pr(&gt;|t|)</code> column).
The degrees of freedom for the <span class="math inline">\(t\)</span>-distributions show up below the coefficients
and the <span class="math inline">\(df=20\)</span> here. This is because <span class="math inline">\(n=23\)</span> and <span class="math inline">\(K=2\)</span>, so <span class="math inline">\(df=23-2-1=20\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m5 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),])
<span class="kw">summary</span>(m5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Snow.Depth ~ Elevation + Max.Temp, data = snotel2[-c(9, 
##     22), ])
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -14.652  -4.645   0.518   3.744  20.550 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -1.675e+02  3.924e+01  -4.269 0.000375
## Elevation    2.407e-02  3.162e-03   7.613 2.48e-07
## Max.Temp     1.253e+00  5.385e-01   2.327 0.030556
## 
## Residual standard error: 8.726 on 20 degrees of freedom
## Multiple R-squared:  0.8495, Adjusted R-squared:  0.8344 
## F-statistic: 56.43 on 2 and 20 DF,  p-value: 5.979e-09</code></pre>
<p>The hypotheses for the <em>Maximum Temperature</em> term (<em>Max.Temp</em>) are:</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{H_0: \beta_{\textbf{Max.Temp}}=0}\)</span> <strong>given that</strong> <strong><em>Elevation</em></strong>
<strong>is in the model vs</strong></p></li>
<li><p><span class="math inline">\(\boldsymbol{H_A: \beta_{\textbf{Max.Temp}}\ne 0}\)</span> <strong>given that</strong>
<strong><em>Elevation</em></strong> <strong>is in the model.</strong></p></li>
</ul>
<p>The test statistic is <span class="math inline">\(t=2.327\)</span> with <span class="math inline">\(df = 20\)</span> (so under the null hypothesis
the test statistic follows a <span class="math inline">\(t_{20}\)</span>-distribution).</p>
<p>The output provides a p-value of <span class="math inline">\(0.0306\)</span> for this test. We can also find this
using <code>pt</code>:

</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="dv">2</span><span class="op">*</span><span class="kw">pt</span>(<span class="fl">2.327</span>, <span class="dt">df=</span><span class="dv">20</span>, <span class="dt">lower.tail=</span>F)</code></pre>
<pre><code>## [1] 0.03058319</code></pre>
<p>The chance of observing a
slope for <em>Max.Temp</em> as extreme or more extreme than assuming there really is no
linear relationship between <em>Max.Temp</em> and <em>Snow Depth</em> (in a model with
<em>Elevation</em>), is about 3% so this presents moderate evidence against the null hypothesis, in favor of retaining this term in the model.</p>
<p>Conclusion: There is moderate evidence to suggest that there is a linear
relationship between <em>Max.Temp</em> and <em>Snow Depth</em>, once we account for
<em>Elevation</em>, in the population of SNOTEL sites in Montana on this day. Because
we cannot randomly assign the temperatures to sites, we cannot conclude that
temperature causes changes in the snow depth – in fact it might even be
possible for a location to have different temperatures because of different
snow depths.</p>
<p>Similarly, we can test for <em>Elevation</em> after controlling for the <em>Maximum Temperature</em>:</p>
<p><span class="math display">\[\boldsymbol{H_0: \beta_{\textbf{Elevation}}=0 \textbf{ vs } H_A: \beta_{\textbf{Elevation}}\ne 0},\]</span></p>
<p>given that <em>Max.Temp</em> is in the model:</p>
<p><span class="math inline">\(t=7.613\)</span> (<span class="math inline">\(df=20\)</span>) with a p-value of <span class="math inline">\(0.00000025\)</span> or just <span class="math inline">\(&lt;0.00001\)</span>.</p>
<p>So there is strong evidence to
suggest that there is a linear relationship between <em>Elevation</em> and
<em>Snow Depth</em>, once we adjust for <em>Max.Temp</em> in the population of SNOTEL
sites in Montana on this day.</p>
<p>There is one last test that is of dubious interest in almost every
situation – to test that the y-intercept <span class="math inline">\((\boldsymbol{\beta_0})\)</span> in an MLR
is 0. This tests if the
true mean response is 0 when all the predictor variables are set to 0. I see
researchers reporting this p-value frequently and it is possibly the most
useless piece of information in the regression model summary.

Sometimes less
educated statistics users even think this result is proof of something interesting
or are disappointed when the p-value is not small. Unless you want to do some
prediction and are interested in whether the mean response when all the
predictors are set to 0 is different from 0, this test should not be reported
or, if reported, is certainly not very interesting<a href="#fn100" class="footnote-ref" id="fnref100"><sup>100</sup></a>.

But we should at least go through the motions on this
test once so you don’t make the same mistakes:</p>
<p><span class="math inline">\(\boldsymbol{H_0: \beta_0=0 \textbf{ vs } H_A: \beta_0\ne 0}\)</span> in a model with
<em>Elevation</em> and <em>Maximum Temperature</em>.</p>
<p><span class="math inline">\(t=-4.269\)</span>, with an assumption that the test statistic follows a
<span class="math inline">\(t_{20}\)</span>-distribution under the null hypothesis, and the p-value <span class="math inline">\(= 0.000375\)</span>.</p>
<p>There is strong evidence to suggest that the true mean
<em>Snow Depth</em> is different from 0 when the <em>Maximum Temperature</em> is 0 and the
<em>Elevation</em> is 0 in the population of SNOTEL sites. To reinforce the general
uselessness of this test, think about the combination of <span class="math inline">\(x\text{&#39;s}\)</span> – is that
even physically possible in Montana (or the continental US) in April?</p>
<p>Remember when testing slope coefficients in MLR, that if we fail to
reject the null hypothesis, it does not mean that there is no relationship or
even no linear
relationship between the variables, but that there is insufficient evidence to declare a linear
relationship is present <strong>once we account for the other variables in the model</strong>. If you do
not find a small p-value for a variable, you should
either be cautious when interpreting the coefficient, or not interpret it. Some
model building strategies would lead to dropping the term from the model but
sometimes we will have models to interpret that contain terms with larger
p-values. Sometimes they are still of interest but the weight on the
interpretation isn’t as heavy as if the term had a small p-value – you should
remember that you can’t prove that coefficient is different from 0 in that
model. It also may mean that you don’t know too much about its specific value.
Confidence intervals will help us pin down where we think the true slope
coefficient might be located, given the other variables in the model, and so are usually pretty interesting to report.</p>
<p>Confidence intervals provide the dual uses of inferences for the
location of the true slope and whether the true slope seems to be different
from 0. The confidence intervals here have our regular format of estimate
<span class="math inline">\(\mp\)</span> margin of error. Like the previous tests,
we work with <span class="math inline">\(t\)</span>-distributions with <span class="math inline">\(n-K-1\)</span> degrees of freedom.


Specifically
the 95% confidence interval for slope coefficient <span class="math inline">\(k\)</span> is</p>
<p><span class="math display">\[\boldsymbol{b_k \mp t^*_{n-K-1}\textbf{SE}_{b_k}}\ .\]</span></p>
<p>The interpretation is the same as in SLR with the additional tag of “after
controlling for the other variables in the model” for the reasons
discussed before. The general slope CI interpretation for predictor
<span class="math inline">\(\boldsymbol{x_k}\)</span> in an MLR is:
</p>
<blockquote>
<p>For a 1 <strong>[<em>unit of <span class="math inline">\(\boldsymbol{x_k}\)</span></em>]</strong> increase in <span class="math inline">\(\boldsymbol{x_k}\)</span>, we
are 95% confident that the true mean of <span class="math inline">\(\boldsymbol{y}\)</span> changes by between
<strong>LL</strong> and <strong>UL</strong> <strong>[<em>units of <span class="math inline">\(\boldsymbol{Y}\)</span></em>]</strong> in the population, after
adjusting for the other <span class="math inline">\(x\text{&#39;s}\)</span> <strong>[list them!]</strong>.</p>
</blockquote>
<p>We can either calculate these intervals as we have many times before or
rely on the <code>confint</code> function to do this:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(m5)</code></pre>
<pre><code>##                     2.5 %       97.5 %
## (Intercept) -249.37903311 -85.67576239
## Elevation      0.01747878   0.03067123
## Max.Temp       0.13001718   2.37644112</code></pre>
<p>So for a <span class="math inline">\(1^\circ F\)</span> increase in <em>Maximum Temperature</em>, we are 95% confident
that the true mean <em>Snow Depth</em> will change by between 0.13 and 2.38 inches
in the population, after adjusting for the <em>Elevation</em> of the sites. Similarly,
for a 1 foot increase in <em>Elevation</em>, we are 95% confident that the true mean
<em>Snow Depth</em> will change by between 0.0175 and 0.0307 inches in the population,
after adjusting for the <em>Maximum Temperature</em> of the sites.</p>
</div>
<div id="section8-7" class="section level2">
<h2><span class="header-section-number">8.7</span> Overall F-test in multiple linear regression</h2>
<p>In the MLR summary, there is an <span class="math inline">\(F\)</span>-test and p-value reported at the bottom
of the output. For the model with <em>Elevation</em> and <em>Maximum Temperature</em>, the
last row of the model summary is:</p>
<pre><code>## F-statistic: 56.43 on 2 and 20 DF,  p-value: 5.979e-09</code></pre>
<p>This test is called the <strong><em>overall F-test</em></strong> in MLR and is very similar to
the <span class="math inline">\(F\)</span>-test in a reference-coded One-Way ANOVA model.

It tests the null
hypothesis that involves setting every coefficient except the y-intercept to
0 (so all the slope coefficients equal 0). We saw this reduced model in the
One-Way material when we considered setting all the deviations from the
baseline group to 0 under the null hypothesis. We can frame this as a
comparison between a full and reduced model as follows:</p>
<ul>
<li><p><strong><em>Full Model:</em></strong>   <span class="math inline">\(y_i = \beta_0 + \beta_1x_{1i} + \beta_2x_{2i}+\cdots + \beta_Kx_{Ki}+\varepsilon_i\)</span></p></li>
<li><p><strong><em>Reduced Model:</em></strong>   <span class="math inline">\(y_i = \beta_0 + 0x_{1i} + 0x_{2i}+\cdots + 0x_{Ki}+\varepsilon_i\)</span></p></li>
</ul>
<p>The reduced model estimates the same values for all <span class="math inline">\(y\text{&#39;s}\)</span>,
<span class="math inline">\(\hat{y}_i=\bar{y}=b_0\)</span> and corresponds to the null hypothesis of:</p>
<p><span class="math inline">\(\boldsymbol{H_0:}\)</span> <strong>No explanatory variables should be included in the model:</strong> <span class="math inline">\(\beta_1=\beta_2=\cdots=\beta_K=0\)</span>.</p>
<p>The full model corresponds to the alternative:</p>
<p><span class="math inline">\(\boldsymbol{H_A:}\)</span> <strong>At least one explanatory variable should be included in the model: Not all</strong> <span class="math inline">\(\beta_k\text{&#39;s}=0\)</span> for <span class="math inline">\((k=1,\ldots,K)\)</span>.</p>
<p>Note that <span class="math inline">\(\beta_0\)</span> is not set to 0 in the reduced model (under the null
hypothesis) – it becomes the true mean of <span class="math inline">\(y\)</span> for all values of the
<span class="math inline">\(x\text{&#39;s}\)</span> since all the predictors are multiplied by coefficients of 0.</p>
<p>The test statistic to assess these hypotheses is
<span class="math inline">\(F = \text{MS}_{\text{model}}/\text{MS}_E\)</span>, which is assumed to follow an
<span class="math inline">\(F\)</span>-distribution with <span class="math inline">\(K\)</span> numerator <em>df</em> and <span class="math inline">\(n-K-1\)</span> denominator <em>df</em> under the
null hypothesis.

The output provides us with <span class="math inline">\(F(2, 20)=56.43\)</span> and a p-value
of <span class="math inline">\(5.979*10^{-9}\)</span> (p-value <span class="math inline">\(&lt;0.00001\)</span>)
and strong evidence against the null hypothesis. Thus, there is strong evidence that
at least one of the two slope coefficients (<em>Max.Temp</em>’s or <em>Elevation</em>’s) is
different from 0 in the population of SNOTEL sites in Montana on this date.
While this test is a little bit interesting and a good indicator of something
interesting in the model, the moment you see this result, you want to know more
about each predictor variable. If neither predictor variable is important, we
will discover that in the <span class="math inline">\(t\)</span>-tests for each coefficient and so our general recommendation is to start there.</p>
<p>The overall F-test, then, is really about testing whether there is something
good in the model somewhere.

And that certainly is important but it is also not
too informative. There is one situation where this test is really interesting,
when there is only one predictor variable in the model (SLR). In that situation,
this test provides exactly the same p-value as the <span class="math inline">\(t\)</span>-test. <span class="math inline">\(F\)</span>-tests will be
important when we are mixing categorical and quantitative predictor variables
in our MLR models (Section <a href="chapter8.html#section8-12">8.12</a>), but the overall <span class="math inline">\(F\)</span>-test is of
very limited utility.</p>
</div>
<div id="section8-8" class="section level2">
<h2><span class="header-section-number">8.8</span> Case study: First year college GPA and SATs</h2>
<p>Many universities require students to have certain test scores in order to be
admitted into their institutions. They
obviously must think that those scores are useful predictors of student success
to use them in this way. Quality assessments of recruiting classes are also
based on their test scores. The Educational Testing Service (the company behind
such fun exams as the SAT and GRE) collected a data set to validate their SAT
on <span class="math inline">\(n=1000\)</span> students from an unnamed Midwestern university; the data set is
available in the <code>openintro</code> package <span class="citation">(Diez, Barr, and Cetinkaya-Rundel <a href="#ref-R-openintro" role="doc-biblioref">2017</a>)</span>
in the <code>satGPA</code> data set.

It is unclear from the documentation whether a
random sample was collected, in fact it looks like it certainly wasn’t a random
sample of all incoming students at a large university (more later). What
potential issues would arise if a company was providing a data set to show the
performance of their test and it was not based on a random sample?</p>
<p>We will proceed assuming they used good methods in developing their test
(there are sophisticated
statistical models underlying the development of the SAT and GRE) and in
obtaining a data set for testing out the performance of their tests that is at
least representative of the students (or some types of students) at this
university. They provided information on the <em>Sex</em> (<code>sex</code>) of the students
(coded 1 and 2 with possibly 1 for males and 2 for females – but should this
even be displayed in a plot with correlations?), <em>SAT Verbal</em> (<code>SATV</code>)
and <em>Math</em> (<code>SATM</code>) percentiles (these are not the scores but the ranking
percentile that each score translated to in a particular year),
<em>High School GPA</em> (<code>HSGPA</code>), and <em>First Year</em> of college <em>GPA</em> (<code>FYGPA</code>).
Our interests here are in whether the two SAT percentiles are (together?)
related to the first year college GPA, describing the size of their impacts
and assessing the predictive potential of SAT-based measures for first year in
college GPA. There are certainly other possible research questions that can be
addressed with these data but this will keep us focused.
</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(openintro)
<span class="kw">data</span>(satGPA)
<span class="kw">require</span>(tibble)
satGPA &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(satGPA)
<span class="kw">require</span>(psych)
<span class="kw">pairs.panels</span>(satGPA[,<span class="op">-</span><span class="dv">4</span>], <span class="dt">ellipse=</span>F, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-13"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-13-1.png" alt="Scatterplot matrix of SAT and GPA data set." width="672" />
<p class="caption">
Figure 8.14: Scatterplot matrix of SAT and GPA data set.
</p>
</div>
<p>There are positive relationships in Figure <a href="chapter8.html#fig:Figure8-13">8.14</a> among all the
pre-college measures and the <em>college GPA</em> but none are above the moderate
strength level. The <em>HSGPA</em> has a highest correlation with
first year of college results but its correlation is not that strong. Maybe
together in a model the SAT percentiles can also be useful… Also note that
plot shows an odd <em>HSGPA</em> of 4.5 that probably should be removed<a href="#fn101" class="footnote-ref" id="fnref101"><sup>101</sup></a> if that variable is going to be used (<em>HSGPA</em>
was not used in the following models so the observation remains in the data).</p>
<p>In MLR, the modeling process is a bit more complex and often involves
more than one model, so we will often avoid the 6+ steps in testing initially
and try to generate a model we can use in that more specific process. In this
case, the first model of interest using the two SAT percentiles,</p>
<p><span class="math display">\[\text{FYGPA}_i = \beta_0 + \beta_{\text{SATV}}\text{SATV}_i 
+ \beta_{\text{SATM}}\text{SATM}_i +\varepsilon_i,\]</span></p>
<p>looks like it might be worth interrogating further so we can jump straight
into considering the 6+ steps involved in hypothesis testing for the two slope
coefficients.

We will use <span class="math inline">\(t\)</span>-based
inferences, assuming that we can trust the assumptions.</p>
<p>Note that this is not a randomized experiment but we can assume that
it is representative of the students at that
single university. We would not want to extend these inferences to other
universities (who might be more or less selective) or to students who did not
get into this university and, especially, not to students that failed to complete
the first year. The second and third constraints point to a severe limitation
in this research – only students who were accepted, went to, and finished one
year at this university could be studied. Lower SAT percentile students might
not have been allowed in or may not have finished the first year and higher SAT
students might have been attracted to other more prestigious institutions. So
the scope of inference is just limited to students that were invited and chose
to attend this institution and successfully completed one year of courses. It
is hard to know if the SAT “works” when the inferences are so restricted in who
they might apply to…</p>
<p>The following code fits the model of interest, provides a model summary,
and the diagnostic plots, allowing us to consider the tests of interest:</p>
<p>(ref:fig8-14) Diagnostic plots for the
<span class="math inline">\(\text{FYGPA}\sim\text{ SATV }+\text{ SATM}\)</span> model.</p>
<pre class="sourceCode r"><code class="sourceCode r">gpa1 &lt;-<span class="st"> </span><span class="kw">lm</span>(FYGPA<span class="op">~</span>SATV<span class="op">+</span>SATM, <span class="dt">data=</span>satGPA)
<span class="kw">summary</span>(gpa1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = FYGPA ~ SATV + SATM, data = satGPA)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.19647 -0.44777  0.02895  0.45717  1.60940 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 0.007372   0.152292   0.048    0.961
## SATV        0.025390   0.002859   8.879  &lt; 2e-16
## SATM        0.022395   0.002786   8.037 2.58e-15
## 
## Residual standard error: 0.6582 on 997 degrees of freedom
## Multiple R-squared:  0.2122, Adjusted R-squared:  0.2106 
## F-statistic: 134.2 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(gpa1, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for GPA model with SATV and SATM&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-14"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-14-1.png" alt="(ref:fig8-14)" width="960" />
<p class="caption">
Figure 8.15: (ref:fig8-14)
</p>
</div>
<ol style="list-style-type: decimal">
<li><p>Hypotheses of interest:</p>
<ul>
<li><p><span class="math inline">\(H_0: \beta_\text{SATV}=0\)</span> given <em>SATM</em> in the model vs
<span class="math inline">\(H_A: \beta_\text{SATV}\ne 0\)</span> given <em>SATM</em> in the model.</p></li>
<li><p><span class="math inline">\(H_0: \beta_\text{SATM}=0\)</span> given <em>SATV</em> in the model vs
<span class="math inline">\(H_A: \beta_\text{SATM}\ne 0\)</span> given <em>SATV</em> in the model.</p></li>
</ul></li>
<li><p>Validity conditions:</p>
<ul>
<li><p><strong>Quantitative variables condition:</strong></p>
<ul>
<li>The variables used here in this model are quantitative. Note that <em>sex</em> was
plotted in the previous scatterplot matrix and is not quantitative –
we will explore its use later.</li>
</ul></li>
</ul>
<p></p>
<ul>
<li><p><strong>Independence of observations:</strong></p>
<ul>
<li>With a sample from a single university from (we are assuming) a
single year of students, there is no particular reason to assume a
violation of the independence assumption.</li>
</ul></li>
<li><p><strong>Linearity of relationships:</strong></p>
<ul>
<li><p>The initial scatterplots (Figure <a href="chapter8.html#fig:Figure8-13">8.14</a>) do not show
any clear nonlinearities with each predictor used in this model.</p></li>
<li><p>The Residuals vs Fitted and Scale-Location plots (Figure
<a href="chapter8.html#fig:Figure8-14">8.15</a>) do not show much more than a football shape,
which is our desired result.</p>
<ul>
<li>Together, there is no suggestion of a violation of the linearity
assumption.</li>
</ul></li>
</ul></li>
<li><p><strong>Multicollinearity checked for:</strong></p>
<ul>
<li><p>The original scatterplots suggest that there is some collinearity
between the two SAT percentiles with a correlation of 0.47. That is
actually a bit lower than one might expect and suggests that each
score must be measuring some independent information about different
characteristics of the students.</p></li>
<li><p>VIFs also do not suggest a major issue with multicollinearity in the
model with the VIFs for both variables the same at 1.278<a href="#fn102" class="footnote-ref" id="fnref102"><sup>102</sup></a>. This suggests that both SEs are about 13% larger than they
otherwise would have been due to shared information between the two
predictor variables. </p></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">vif</span>(gpa1)</code></pre>
<pre><code>##     SATV     SATM 
## 1.278278 1.278278</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sqrt</span>(<span class="kw">vif</span>(gpa1))</code></pre>
<pre><code>##    SATV    SATM 
## 1.13061 1.13061</code></pre></li>
<li><p><strong>Equal (constant) variance:</strong></p>
<ul>
<li>There is no clear change in variability as a function of fitted
values so no indication of a violation of the constant variance of residuals assumption.</li>
</ul></li>
<li><p><strong>Normality of residuals:</strong></p>
<ul>
<li>There is a minor deviation in the upper tail of the residual
distribution from normality. It is not pushing towards having larger
values than a normal distribution would generate so should not cause us
any real problems with inferences from this model. Note that this upper limit is likely due to using
GPA as a response variable and it has an upper limit. This is an
example of a potentially <strong><em>censored</em></strong> variable. For a continuous
variable it is possible that the range of a measurement scale doesn’t
distinguish among subjects who differ once they pass a certain point.
For example, a 4.0 high school student is likely going to have a high first
year college GPA, on average, but there is no room for variability in
college GPA up, just down once you are at the top of the GPA scale. For students more in the middle of the range,
they can vary up or down. So in some places you can get symmetric
distributions around the mean and in others you cannot. There are
specific statistical models for these types of responses that are
beyond our scope. In this situation, failing to account for the
censoring may push some slopes toward 0 a little because we can’t have
responses over 4.0 in college GPA to work with.</li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li>There are no influential points. In large data sets, the influence
of any point is decreased and even high leverage and outlying points
can struggle to have any impacts at all on the results.</li>
</ul></li>
</ul></li>
</ol>
<p>So we are fairly comfortable with all the assumptions being at least not clearly
violated and so the inferences from our model should be relatively trustworthy.</p>
<ol start="3" style="list-style-type: decimal">
<li><p>Calculate the test statistics:</p>
<ul>
<li><p>For <em>SATV</em>: <span class="math inline">\(t=\dfrac{0.02539}{0.002859}=8.88\)</span> with <span class="math inline">\(df=997\)</span>.</p></li>
<li><p>For <em>SATM</em>: <span class="math inline">\(t=\dfrac{0.02240}{0.002786}=8.04\)</span> with <span class="math inline">\(df=997\)</span>.</p></li>
</ul></li>
<li><p>Find the p-values:</p>
<ul>
<li><p>For <em>SATV</em>: p-value <span class="math inline">\(&lt;0.0001\)</span></p></li>
<li><p>For <em>SATM</em>: p-value <span class="math inline">\(&lt;0.0001\)</span></p></li>
</ul></li>
<li><p>Decisions:</p>
<ul>
<li><p>For <em>SATV</em>: Reject <span class="math inline">\(H_0\)</span> because there is almost no chance of observing
a test statistic as extreme or more extreme than was observed if there
really were no linear relationship between <em>FYGPA</em> and <em>SATV</em>, in a model
that controls for <em>SATM</em>.</p></li>
<li><p>For <em>SATM</em>: Reject <span class="math inline">\(H_0\)</span> because there is almost no chance of observing
a test statistic as extreme or more extreme than was observed if there
really were no linear relationship between <em>FYGPA</em> and <em>SATM</em>, in a model
that controls for <em>SATV</em>.</p></li>
</ul></li>
<li><p>Conclusions and Scope of Inference:</p>
<ul>
<li><p>For <em>SATV</em>: There is strong evidence to reject the null hypothesis of
no linear relationship between <em>SATV</em> and <em>FYGPA</em> (<span class="math inline">\(t_{997}=8.88\)</span>,
p-value &lt; 0.0001) and conclude that, in
fact, there is a linear relationship between <em>SATV</em> percentile and the
first year of college <em>GPA</em>, after controlling for the <em>SATM</em> percentile,
in the population of students that completed their first year at this
university.</p></li>
<li><p>For <em>SATM</em>: There is strong evidence to reject the null hypothesis of
no linear relationship between <em>SATM</em> and <em>FYGPA</em> (<span class="math inline">\(t_{997}=8.04\)</span>,
p-value &lt; 0.0001)and conclude that, in
fact, there is a linear relationship between <em>SATM</em> percentile and the
first year of college <em>GPA</em>, after controlling for the <em>SATV</em> percentile,
in the population of students that completed their first year at this
university.</p></li>
<li><p>Note that neither inference is causal because there was no random
assignment of SAT percentiles to the subjects. The inferences
are also limited to students who stayed in school long enough to get a
<em>GPA</em> from their first year of college at this university.</p></li>
</ul></li>
</ol>
<p>The model seems to be valid and have predictors with small p-values, but note how much of the variation is not explained by the model. It only explains 21.22%  of the variation in the responses. So we found evidence that these variables are useful in predicting the responses, but are they useful enough to use for decisions on admitting students? By quantifying
the size of the estimated results, we can add to the information about how potentially useful this model might be. The estimated MLR model is</p>
<p><span class="math display">\[\widehat{\text{FYGPA}}_i=0.00737+0.0254\cdot\text{SATV}_i
+0.0224\cdot\text{SATM}_i\ .\]</span></p>
<p>So for a 1 percent increase in the <em>SATV</em> percentile, we expect, on average,
to get a 0.0254 point change in <em>GPA</em>, after controlling for <em>SATM</em> percentile.
Similarly, for a 1 percent increase in the <em>SATM</em> percentile, we expect, on
average, to get a 0.0224 point change in <em>GPA</em>, after controlling for <em>SATV</em>
percentile. While this is a correct interpretation of the slope coefficients,
it is often easier to assess “practical” importance of the results by considering
how much change this implies over the range of observed predictor values.</p>
<p>The term-plots (Figure <a href="chapter8.html#fig:Figure8-15">8.16</a>) provide a visualization of the
“size” of the differences in the response variable explained by each predictor. 
The <em>SATV</em> term-plot shows that for the range of percentiles from around the
30<sup>th</sup> percentile to the 70<sup>th</sup> percentile, the mean first
year <em>GPA</em> is predicted to go from approximately 1.9 to 3.0. That is a pretty
wide range of differences in GPAs across the range of observed percentiles.
This looks like a pretty interesting and important change in the mean first
year GPA across that range of different SAT percentiles. Similarly, the <em>SATM</em>
term-plot shows that the <em>SATM</em> percentiles were observed to range between
around the 30<sup>th</sup> percentile and 70<sup>th</sup> percentile and
predict mean GPAs between 1.95 and 2.8. It seems that the SAT Verbal
percentiles produce slightly more impacts
in the model, holding the other variable constant, but that both are important
variables. The 95% confidence intervals for the means in both plots suggest
that the results are fairly precisely estimated – there is little variability
around the predicted means in each plot. This is mostly a function of the
sample size as opposed to the model itself explaining most of the variation in the
responses. </p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(gpa1))</code></pre>
<p>(ref:fig8-15) Term-plots for the <span class="math inline">\(\text{FYGPA}\sim\text{SATV} + \text{SATM}\)</span>
model.</p>
<div class="figure"><span id="fig:Figure8-15"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-15-1.png" alt="(ref:fig8-15)" width="576" />
<p class="caption">
Figure 8.16: (ref:fig8-15)
</p>
</div>
<p>These plots also inform the types of students attending this university and
successfully completing the first year of school. This seems like a good, but
maybe not great, institution with few students scoring over the 75<sup>th</sup>
percentile on either SAT Verbal or Math (at least that ended up in this data
set). This result makes questions about their sampling mechanism re-occur as
to who this data set might actually be representative of…</p>
<p>The confidence intervals also help us pin down the uncertainty in each
estimated slope coefficient. As always, the “easy” way to get 95% confidence
intervals is using the <code>confint</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(gpa1)</code></pre>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.29147825 0.30622148
## SATV         0.01977864 0.03100106
## SATM         0.01692690 0.02786220</code></pre>
<p>So, for a 1 percent increase in the <em>SATV</em> percentile, we are 95% confident
that the true mean <em>FYGPA</em> changes between 0.0198 and 0.031 points, in the
population of students who completed this year at this institution, after
controlling for <em>SATM</em>. The <em>SATM</em> result is similar with an interval from
0.0169 and 0.0279. Both of these intervals might benefit from re-scaling
the interpretation to, say, a 10 percentile increase in the predictor variable, with
the change in the <em>FYGPA</em> for that level of increase of <em>SATV</em> providing an
interval from 0.198 to 0.31 points and for <em>SATM</em> providing an interval from
0.169 to 0.279. So a boost of 10% in either exam percentile likely results in a
noticeable but not huge average <em>FYGPA</em> increase.</p>
<p>One final use of these methods is to do prediction and generate prediction
intervals, which could be quite informative for a student considering going to
this university who has a particular set of SAT scores. For example, suppose
that the student is interested in the average <em>FYGPA</em> to expect with <em>SATV</em>
at the 30<sup>th</sup> percentile and <em>SATM</em> at the 60<sup>th</sup> percentile.
The predicted mean value is</p>
<p><span class="math display">\[\begin{array}{rl}
\hat{\mu}_{\text{GPA}_i} &amp;= 0.00737 + 0.0254\cdot\text{SATV}_i 
+ 0.0224\cdot\text{SATM}_i \\
&amp;= 0.00737 + 0.0254*30 + 0.0224*60 = 2.113.
\end{array}\]</span></p>
<p>This result and the 95% confidence interval for the mean student <em>GPA</em> at these
scores can be found using the <code>predict</code> function as:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>))</code></pre>
<pre><code>##       1 
## 2.11274</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>), <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>)</code></pre>
<pre><code>##       fit      lwr      upr
## 1 2.11274 1.982612 2.242868</code></pre>
<p>For students at the 30<sup>th</sup> percentile of <em>SATV</em> and 60<sup>th</sup>
percentile of <em>SATM</em>, we are 95% confident that the true mean first year GPA
is between 1.98 and 2.24 points. For an individual student, we would want the
95% prediction interval:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">predict</span>(gpa1,<span class="dt">newdata=</span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="dv">30</span>,<span class="dt">SATM=</span><span class="dv">60</span>),<span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>)</code></pre>
<pre><code>##       fit       lwr      upr
## 1 2.11274 0.8145859 3.410894</code></pre>
<p>For a student with <em>SATV</em>=30 and <em>SATM</em>=60, we are 95% sure that their first
year GPA will be between 0.81 and 3.4 points. You can see that while we are
very certain about the mean in this situation, there is a lot of uncertainty
in the predictions for individual students. The PI is so wide as to almost
not be useful.</p>
<p>To support this difficulty in getting a precise prediction for a new student,
review the original scatterplots: there is quite a bit of vertical variability
in first year <em>GPA</em>s for each level of any of the predictors. The residual
SE, <span class="math inline">\(\hat{\sigma}\)</span>, is also informative in this regard –
remember that it is the standard deviation of the residuals around the
regression line. It is 0.6582, so the SD of new observations around the line is
0.66 GPA points and that is pretty large on a GPA scale. Remember that if the residuals meet our assumptions and follow a normal distribution around the line, observations within 2 or 3 SDs of the mean would be expected which is a large range of GPA values.
Figure <a href="chapter8.html#fig:Figure8-16">8.17</a> remakes
both term-plots, holding the other predictor at its mean, and adds the 95%
prediction intervals to show the difference in variability between estimating
the mean and pinning down the value of a new observation. The R code is very messy
and rarely needed, but hopefully this helps reinforce the differences in these
two types of intervals – to make them in MLR, you have to fix all but one of
the predictor variables and we usually do that by fixing the other variables at
their means. </p>
<p>(ref:fig8-16) Term-plots for the <span class="math inline">\(\text{FYGPA}\sim\text{SATV} + \text{SATM}\)</span>
model with 95% confidence intervals (red, dashed lines) and 95% PIs (light grey, dotted lines).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Remake effects plots with 95% PIs</span>
dv1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">24</span>,<span class="dt">to=</span><span class="dv">76</span>,<span class="dt">length.out=</span><span class="dv">50</span>), <span class="dt">SATM=</span><span class="kw">rep</span>(<span class="fl">54.4</span>,<span class="dv">50</span>))
dm1 &lt;-<span class="st"> </span><span class="kw">tibble</span>(<span class="dt">SATV=</span><span class="kw">rep</span>(<span class="fl">48.93</span>,<span class="dv">50</span>), <span class="dt">SATM=</span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">29</span>,<span class="dt">to=</span><span class="dv">77</span>,<span class="dt">length.out=</span><span class="dv">50</span>))

mv1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dv1, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>))
pv1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dv1, <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>))

mm1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dm1, <span class="dt">interval=</span><span class="st">&quot;confidence&quot;</span>))
pm1 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(<span class="kw">predict</span>(gpa1, <span class="dt">newdata=</span>dm1, <span class="dt">interval=</span><span class="st">&quot;prediction&quot;</span>))

<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))

<span class="kw">plot</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>fit, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">ylim=</span><span class="kw">c</span>(pv1<span class="op">$</span>lwr[<span class="dv">1</span>],pv1<span class="op">$</span>upr[<span class="dv">50</span>]), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;SATV Percentile&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">main=</span><span class="st">&quot;SATV Effect, CI and PI&quot;</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, mv1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, pv1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">lines</span>(dv1<span class="op">$</span>SATV, pv1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="kw">c</span>(<span class="st">&quot;Estimate&quot;</span>, <span class="st">&quot;CI&quot;</span>,<span class="st">&quot;PI&quot;</span>), <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>),
       <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>,<span class="st">&quot;grey&quot;</span>))

<span class="kw">plot</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>fit, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">ylim=</span><span class="kw">c</span>(pm1<span class="op">$</span>lwr[<span class="dv">1</span>],pm1<span class="op">$</span>upr[<span class="dv">50</span>]), <span class="dt">type=</span><span class="st">&quot;l&quot;</span>,
     <span class="dt">xlab=</span><span class="st">&quot;SATM Percentile&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;GPA&quot;</span>, <span class="dt">main=</span><span class="st">&quot;SATM Effect, CI and PI&quot;</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, mm1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, pm1<span class="op">$</span>lwr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">lines</span>(dm1<span class="op">$</span>SATM, pm1<span class="op">$</span>upr, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-16"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-16-1.png" alt="(ref:fig8-16)" width="960" />
<p class="caption">
Figure 8.17: (ref:fig8-16)
</p>
</div>

</div>
<div id="section8-9" class="section level2">
<h2><span class="header-section-number">8.9</span> Different intercepts for different groups: MLR with indicator variables</h2>

<p>One of the implicit assumptions up to this point was that the models were being
applied to a single homogeneous population.
 
In many cases, we take a sample
from a population but that overall group is likely a combination of individuals from
different sub-populations. For example, the SAT study was interested in all
students at the university but that contains the obvious sub-populations based
on the sex of the students. It is dangerous to fit MLR models across
subpopulations but we can also use MLR models to address more sophisticated
research questions by comparing groups. We will be able to compare the
intercepts (mean levels) and the slopes to see if they differ between the
groups. For example, does the relationship between the <em>SATV</em> and <em>FYGPA</em>
differ for male and female students? We can add the grouping information to
the scatterplot of <em>FYGPA</em> vs <em>SATV</em> (Figure <a href="chapter8.html#fig:Figure8-17">8.18</a>) and
consider whether there is visual evidence of a difference in the slope and/or
intercept between the two groups, with men coded<a href="#fn103" class="footnote-ref" id="fnref103"><sup>103</sup></a> as 1 and women coded as 2.
</p>
<p>It appears that the slope for females might be larger (steeper) in this
relationship than it is for
males. So increases in SAT Verbal percentiles for females might have more of an
impact on the average first year GPA. We’ll handle this sort of situation in
Section <a href="chapter8.html#section8-11">8.11</a>, where we will formally consider how to change the
slopes for different groups. In this section, we develop new methods needed to
begin to handle these situations and explore creating models with the same
slope coefficient for all groups but different y-intercepts. This material
resembles what we did for the Two-Way ANOVA additive model.</p>
<p>These results contrast with Figure <a href="chapter8.html#fig:Figure8-18">8.19</a> for the relationship
between first year college <em>GPA</em> and <em>SATM</em>
percentile by sex of the students. The lines for the two groups appear to be
mostly parallel and just seem to have different y-intercepts. We can use our
MLR techniques to fit a model to the entire data set that allows for different
y-intercepts. The real power of this idea is that we can then also test whether
the different groups have different y-intercepts – whether the shift between
the groups is “real”. In this example, it appears to suggest that females
generally have slightly higher GPAs than males, on average, but that an
increase in SATM has the same impact for both groups. If this difference in
y-intercepts is not “real”, then there appears to be no difference between the
sexes in their relationship between SATM and GPA and we can safely continue
using a model that does not differentiate the two groups. We could also just
subset the data set and do two analyses, but that approach will not allow us
to assess whether things are “really” different between the two groups.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(car)
<span class="kw">scatterplot</span>(FYGPA<span class="op">~</span>SATV<span class="op">|</span>sex, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">data=</span>satGPA, <span class="dt">smooth=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Scatterplot of GPA vs SATV by Sex&quot;</span>)
<span class="kw">scatterplot</span>(FYGPA<span class="op">~</span>SATM<span class="op">|</span>sex, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">data=</span>satGPA, <span class="dt">smooth=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Scatterplot of GPA vs SATM by Sex&quot;</span>)</code></pre>
<p>To fit one model to a data set that contain multiple groups, we need a way of
entering categorical variable information in an MLR model. Regression models
require quantitative predictor variables for the <span class="math inline">\(x\text{&#39;s}\)</span> so we can’t
directly enter the sex of the students into the regression model since it
contains categories. To be able to
put in “numbers” as predictors, we create what are called
<strong><em>indicator variables</em></strong><a href="#fn104" class="footnote-ref" id="fnref104"><sup>104</sup></a>
that are made up of 0s and 1s, with the 0 reflecting one category and 1 the
other, changing depending on the category of the individual in the data set. The
<code>lm</code> function does this whenever a
categorical variable is used as an explanatory variable. 

It sets up the indicator
variables using a baseline category (gets coded as a 0) and the deviation
category for the other level of the variable. We can see how this works by
exploring what happens when we put <code>SEX</code> into our <code>lm</code><a href="#fn105" class="footnote-ref" id="fnref105"><sup>105</sup></a> with SATM, after first making sure it is categorical using
the <code>factor</code> function and making the factor <code>levels</code> explicit instead of 1s
and 2s.

</p>
<pre class="sourceCode r"><code class="sourceCode r">satGPA<span class="op">$</span>SEX &lt;-<span class="st"> </span><span class="kw">factor</span>(satGPA<span class="op">$</span>sex) <span class="co">#Make 1,2 coded sex into factor SEX</span>
<span class="kw">levels</span>(satGPA<span class="op">$</span>SEX) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;MALE&quot;</span>, <span class="st">&quot;FEMALE&quot;</span>) <span class="co">#Make category names clear</span>
SATSex1 &lt;-<span class="st"> </span><span class="kw">lm</span>(FYGPA<span class="op">~</span>SATM<span class="op">+</span>SEX, <span class="dt">data=</span>satGPA) <span class="co">#Fit lm with SATM and SEX</span>
<span class="kw">summary</span>(SATSex1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = FYGPA ~ SATM + SEX, data = satGPA)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.42124 -0.42363  0.01868  0.46540  1.66397 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.21589    0.14858   1.453    0.147
## SATM         0.03861    0.00258  14.969  &lt; 2e-16
## SEXFEMALE    0.31322    0.04360   7.184 1.32e-12
## 
## Residual standard error: 0.6667 on 997 degrees of freedom
## Multiple R-squared:  0.1917, Adjusted R-squared:  0.1901 
## F-statistic: 118.2 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>

<div class="figure"><span id="fig:Figure8-17"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-17-1.png" alt="Plot of FYGPA vs SATV by Sex of students." width="960" />
<p class="caption">
Figure 8.18: Plot of FYGPA vs SATV by Sex of students.
</p>
</div>

<div class="figure"><span id="fig:Figure8-18"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-18-1.png" alt="Plot of FYGPA vs SATM by Sex of students." width="960" />
<p class="caption">
Figure 8.19: Plot of FYGPA vs SATM by Sex of students.
</p>
</div>
<p>The <code>SEX</code> row contains information that the linear model chose <em>MALE</em> as
the baseline category and <em>FEMALE</em> as the deviation category since <em>MALE</em> does
not show up in the output. To see what <code>lm</code> is doing for us when we give it a
two-level categorical variable, we can create our own “numerical” predictor that
is 0 for <em>males</em> and 1 for <em>females</em> that we called <code>SEXINDICATOR</code>, displayed
for the first 10 observations:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Convert logical to 0 for male, 1 for female</span>
satGPA<span class="op">$</span>SEXINDICATOR &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(satGPA<span class="op">$</span>SEX<span class="op">==</span><span class="st">&quot;FEMALE&quot;</span>) 
<span class="co"># Explore first few observations</span>
<span class="kw">head</span>(<span class="kw">tibble</span>(<span class="dt">SEX=</span>satGPA<span class="op">$</span>SEX, <span class="dt">SEXINDICATOR=</span>satGPA<span class="op">$</span>SEXINDICATOR), <span class="dv">10</span>) </code></pre>
<pre><code>## # A tibble: 10 x 2
##    SEX    SEXINDICATOR
##    &lt;fct&gt;         &lt;dbl&gt;
##  1 MALE              0
##  2 FEMALE            1
##  3 FEMALE            1
##  4 MALE              0
##  5 MALE              0
##  6 FEMALE            1
##  7 MALE              0
##  8 MALE              0
##  9 FEMALE            1
## 10 MALE              0</code></pre>
<p>We can define the indicator variable more generally by calling it
<span class="math inline">\(I_{\text{Female},i}\)</span> to denote that it is an indicator
<span class="math inline">\((I)\)</span> that takes on a value of 1 for
observations in the category <em>Female</em> and 0 otherwise (<em>Male</em>) – changing based
on the observation (<span class="math inline">\(i\)</span>). Indicator variables, once created,
are quantitative variables that take on values of 0 or 1 and we can put them
directly
into linear models with other <span class="math inline">\(x\text{&#39;s}\)</span> (quantitative or categorical). If we
replace the categorical <code>SEX</code> variable with our quantitative <code>SEXINDICATOR</code>
and re-fit the model, we get:</p>
<pre class="sourceCode r"><code class="sourceCode r">SATSex2 &lt;-<span class="st"> </span><span class="kw">lm</span>(FYGPA<span class="op">~</span>SATM<span class="op">+</span>SEXINDICATOR, <span class="dt">data=</span>satGPA)
<span class="kw">summary</span>(SATSex2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = FYGPA ~ SATM + SEXINDICATOR, data = satGPA)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.42124 -0.42363  0.01868  0.46540  1.66397 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)   0.21589    0.14858   1.453    0.147
## SATM          0.03861    0.00258  14.969  &lt; 2e-16
## SEXINDICATOR  0.31322    0.04360   7.184 1.32e-12
## 
## Residual standard error: 0.6667 on 997 degrees of freedom
## Multiple R-squared:  0.1917, Adjusted R-squared:  0.1901 
## F-statistic: 118.2 on 2 and 997 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>This matches all the previous <code>lm</code> output except that we didn’t get any
information on the categories used since <code>lm</code> didn’t know that SEXINDICATOR
was anything different from other quantitative predictors.</p>
<p>Now we want to think about what this model means. We can write the estimated
model as</p>
<p><span class="math display">\[\widehat{\text{FYGPA}}_i = 0.216 + 0.0386\cdot\text{SATM}_i +
0.313I_{\text{Female},i}\ .\]</span></p>
<p>When we have a <em>male</em> observation, the indicator takes on a value of 0 so the
0.313 drops out of the model, leaving an SLR just in terms of <em>SATM</em>. For a
<em>female</em> student, the indicator is 1 and we add 0.313 to the previous
y-intercept. The following
works this out step-by-step, simplifying the MLR into two SLRs:</p>
<ul>
<li><p>Simplified model for <em>Males</em> (plug in a 0 for <span class="math inline">\(I_{\text{Female}}\)</span>):</p>
<ul>
<li><span class="math inline">\(\widehat{\text{FYGPA}}_i = 0.216 + 0.0386\cdot\text{SATM}_i +  0.313*0 = 0.216 + 0.0386\cdot\text{SATM}_i\)</span></li>
</ul></li>
<li><p>Simplified model for <em>Females</em> (plug in a 1 for <span class="math inline">\(I_{\text{Female}}\)</span>):</p>
<ul>
<li><p><span class="math inline">\(\widehat{\text{FYGPA}}_i = 0.216 + 0.0386\cdot\text{SATM}_i + 0.313*1\)</span></p></li>
<li><p><span class="math inline">\(= 0.216 + 0.0386\cdot\text{SATM}_i + 0.313\)</span> (combine “like” terms to
simplify the equation)</p></li>
<li><p><span class="math inline">\(= 0.529 + 0.0386\cdot\text{SATM}_i\)</span> </p></li>
</ul></li>
</ul>
<p>In this situation, we then end up with two SLR models that relate <em>SATM</em> to
<em>GPA</em>, one model for <em>males</em>
<span class="math inline">\((\widehat{\text{FYGPA}}_i= 0.216 + 0.0386\cdot\text{SATM}_i)\)</span> and one for <em>females</em>
<span class="math inline">\((\widehat{\text{FYGPA}}_i= 0.529 + 0.0386\cdot\text{SATM}_i)\)</span>. The only difference
between these two models is in the y-intercept, with the <em>female</em> model’s
y-intercept shifted up from the <em>male</em> y-intercept by 0.313. And that is what
adding indicator variables into models does in general<a href="#fn106" class="footnote-ref" id="fnref106"><sup>106</sup></a> – it shifts the intercept up or down from
the baseline group (here selected as <em>males</em>) to get a new intercept for the
deviation group (here <em>females</em>).</p>
<p>To make this visually clearer, Figure <a href="chapter8.html#fig:Figure8-19">8.20</a> contains the
regression lines that were estimated for each
group. For any <em>SATM</em>, the difference in the groups is the 0.313 coefficient from
the <code>SEXFEMALE</code> or <code>SEXINDICATOR</code> row of the model summaries. For example,
at <em>SATM</em>=50, the difference in terms of predicted average first year GPAs
between males and females is displayed as a difference between 2.15 and 2.46.
This model assumes that the slope on <em>SATM</em> is the same for both groups except
that they are allowed to have different y-intercepts, which is reasonable here
because we saw approximately parallel relationships for the two groups in
Figure <a href="chapter8.html#fig:Figure8-18">8.19</a>.</p>
<p>(ref:fig8-19) Plot of estimated model for <em>FYGPA</em> vs <em>SATM</em> by <em>SEX</em> of
students (female line is thicker red line). Dashed lines aid in seeing the consistent vertical difference of 0.313 in the two estimated lines based on the model containing a different intercept for each group.</p>
<div class="figure"><span id="fig:Figure8-19"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-19-1.png" alt="(ref:fig8-19)" width="576" />
<p class="caption">
Figure 8.20: (ref:fig8-19)
</p>
</div>
<p>Remember that <code>lm</code> selects baseline categories typically based on the
alphabetical order of the levels of the categorical variable. Here, the <code>SEX</code>
variable started with a coding of 1 and 2 and retained that
order even with the recoding of levels that we created to give it more explicit
names. Because we allow <code>lm</code> to create indicator variables for us, the main
thing you need to do is explore the model summary and look for the hint at the
baseline level that is not displayed after the name of the categorical variable. </p>
<p>We can also work out the impacts of adding an indicator variable to the model
in general in the theoretical model with a single quantitative predictor <span class="math inline">\(x_i\)</span>
and indicator <span class="math inline">\(I_i\)</span>. The model starts as</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i + \beta_2I_i + \varepsilon_i\ .\]</span></p>
<p>Again, there are two versions:</p>
<ul>
<li><p>For any observation <span class="math inline">\(i\)</span> in the <strong>baseline</strong> category, <span class="math inline">\(I_i=0\)</span> and the model
is <span class="math inline">\(y_i=\beta_0+\beta_1x_i + \varepsilon_i\)</span>.</p></li>
<li><p>For any observation <span class="math inline">\(i\)</span> in the <strong>non-baseline (deviation)</strong> category, <span class="math inline">\(I_i=1\)</span>
and the model simplifies to <span class="math inline">\(y_i=(\beta_0+\beta_2)+\beta_1x_i + \varepsilon_i\)</span>.</p>
<ul>
<li>This model has a y-intercept of <span class="math inline">\(\beta_0+\beta_2\)</span>.</li>
</ul></li>
</ul>
<p>The interpretation and inferences for <span class="math inline">\(\beta_1\)</span> resemble the work with any
MLR model, noting that these results are “controlled for”, “adjusted for”, or
“allowing for differences based on” the categorical variable in the model. The
interpretation of <span class="math inline">\(\beta_2\)</span> is as a shift up or down in the y-intercept for
the model that includes <span class="math inline">\(x_i\)</span>. When we make term-plots in a model with
a quantitative and additive categorical variable, the two reported model
components match with the previous discussion – the same estimated term from
the quantitative variable for all observations and a shift to reflect the
different y-intercepts in the two groups. In Figure <a href="chapter8.html#fig:Figure8-20">8.21</a>, the
females are estimated to be that same 0.313 points higher on first year GPA.
The males have a mean GPA slightly above 2.3 which is the predicted GPA for the
average SATM percentile (remember that we have to hold the other variable at
its mean to make each term-plot)<a href="#fn107" class="footnote-ref" id="fnref107"><sup>107</sup></a>.</p>
<p>(ref:fig8-20) Term-plots for the estimated model for
<span class="math inline">\(\text{FYGPA}\sim\text{SATM} + \text{SEX}\)</span>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(effects)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(SATSex1))</code></pre>
<div class="figure"><span id="fig:Figure8-20"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-20-1.png" alt="(ref:fig8-20)" width="576" />
<p class="caption">
Figure 8.21: (ref:fig8-20)
</p>
</div>
<p>The model summary and confidence intervals provide some potential interesting
inferences in these models. Again, these are just applications of MLR methods
we have already seen except that the definition of one of the variables is
“different” using the indicator coding idea.  For the same model, the <code>SEX</code>
coefficient can be used to generate inferences for differences in the mean the
groups, controlling for their <em>SATM</em> scores.</p>
<pre><code>## SEXFEMALE    0.31322    0.04360   7.184 1.32e-12</code></pre>
<p>Testing the null hypothesis that <span class="math inline">\(H_0: \beta_2=0\)</span> vs <span class="math inline">\(H_A: \beta_2\ne 0\)</span> using
our regular <span class="math inline">\(t\)</span>-test provides the opportunity to test for a difference in
intercepts between the groups. In this situation, the test
statistic is <span class="math inline">\(t=7.184\)</span> and, based on a <span class="math inline">\(t_{997}\)</span>-distribution
if the null is true, the p-value is <span class="math inline">\(&lt;0.0001\)</span>. We have very strong evidence that there is a difference in the true
y-intercept in a <em>SATM</em> model for first year college GPA between <em>males</em> and <em>females</em>. The confidence
interval is also informative:</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(SATSex1)</code></pre>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.07566665 0.50744709
## SATM         0.03355273 0.04367726
## SEXFEMALE    0.22766284 0.39877160</code></pre>
<p>We are 95% confident that the true mean GPA for females is between 0.228 and
0.399 points higher than for males, after adjusting for the <em>SATM</em> in the
population of students. If we had subset the data set and fit two SLRs, we
could have obtained the same simplified regression models but we never could
have performed inferences for the differences between the two groups without
putting all the observations together in one model and then assessing those
differences with targeted coefficients. We also would not be able to get an
estimate of their common slope for <em>SATM</em>, after adjusting for differences
in the intercept for each group.</p>
</div>
<div id="section8-10" class="section level2">
<h2><span class="header-section-number">8.10</span> Additive MLR with more than two groups: Headache example</h2>
<p>The same techniques can be extended to more than two groups. A study was
conducted to explore sound tolerances using <span class="math inline">\(n=98\)</span> subjects with the data
available in the <code>Headache</code> data set from the <code>heplots</code> package
<span class="citation">(Fox and Friendly <a href="#ref-R-heplots" role="doc-biblioref">2018</a>)</span>.

Each
subject was initially exposed to a tone, stopping when the tone became definitely
intolerable (<em>DU</em>) and that decibel level was recorded (variable called <code>du1</code>).
Then the subjects were randomly assigned to one of four treatments: <em>T1</em>
(Listened again to the tone at their initial <em>DU</em> level, for the same amount of
time they were able to tolerate it before); <em>T2</em> (Same as <em>T1</em>, with one
additional minute of exposure); <em>T3</em> (Same as <em>T2</em>, but the subjects were
explicitly instructed to use the relaxation techniques); and <em>Control</em> (these
subjects experienced no further exposure to the noise tone until the final
sensitivity measures were taken). Then the <em>DU</em> was measured again (variable
called <code>du2</code>). One would expect that there would be a relationship between the
upper tolerance levels of the subjects before and after treatment. But
maybe the treatments impact that relationship? We can use our indicator 
approach to see if the treatments provide a shift to higher tolerances after
accounting for the relationship between the two measurements<a href="#fn108" class="footnote-ref" id="fnref108"><sup>108</sup></a>. The scatterplot<a href="#fn109" class="footnote-ref" id="fnref109"><sup>109</sup></a>
of the results in Figure <a href="chapter8.html#fig:Figure8-21">8.22</a> shows some variation in the
slopes and the intercepts for the groups although the variation in intercepts
seems more prominent than differences in slopes.</p>
<p>(ref:fig8-21) Scatterplot of post-treatment decibel tolerance (du2) vs
pre-treatment tolerance (du1) by treatment level (4 groups).</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(heplots)
<span class="kw">data</span>(Headache)
<span class="kw">require</span>(tibble)
Headache &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(Headache)
Headache</code></pre>
<pre><code>## # A tibble: 98 x 6
##    type    treatment    u1   du1    u2   du2
##    &lt;fct&gt;   &lt;fct&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
##  1 Migrane T3         2.34  5.3   5.8   8.52
##  2 Migrane T1         2.73  6.85  4.68  6.68
##  3 Tension T1         0.37  0.53  0.55  0.84
##  4 Migrane T3         7.5   9.12  5.7   7.88
##  5 Migrane T3         4.63  7.21  5.63  6.75
##  6 Migrane T3         3.6   7.3   4.83  7.32
##  7 Migrane T2         2.45  3.75  2.5   3.18
##  8 Migrane T1         2.31  3.25  2     3.3 
##  9 Migrane T1         1.38  2.33  2.23  3.98
## 10 Tension T3         0.85  1.42  1.37  1.89
## # ... with 88 more rows</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">scatterplot</span>(du2<span class="op">~</span>du1<span class="op">|</span>treatment, <span class="dt">data=</span>Headache, <span class="dt">smooth=</span>F, <span class="dt">lwd=</span><span class="dv">2</span>,
            <span class="dt">main=</span><span class="st">&quot;Plot of Maximum DB tolerances before &amp; after treatment (by treatment)&quot;</span>,
            <span class="dt">legend=</span><span class="kw">list</span>(<span class="dt">coords=</span><span class="st">&quot;topleft&quot;</span>,<span class="dt">columns=</span><span class="dv">2</span>))</code></pre>
<div class="figure"><span id="fig:Figure8-21"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-21-1.png" alt="(ref:fig8-21)" width="960" />
<p class="caption">
Figure 8.22: (ref:fig8-21)
</p>
</div>
<p>This data set contains a categorical variable with 4 levels. To go beyond two
groups, we have to add more than one indicator variable,  defining three
indicators to turn on (1) or off (0) for three of the levels of the variable
with the same reference level used for all the indicators. For this example,
the <em>T1 Treatment</em> group is chosen as the baseline group so it sort of hides in
the background while we define indicators for the other three levels. The
indicators for <em>T2</em>, <em>T3</em>, and <em>Control</em> levels are:</p>
<ul>
<li><p>Indicator for <em>T2</em>: <span class="math inline">\(I_{T2,i}=\left\{\begin{array}{rl} 1 &amp; \text{if Treatment}=T2 \\ 0 &amp; \text{else} \end{array}\right.\)</span></p></li>
<li><p>Indicator for <em>T3</em>: <span class="math inline">\(I_{T3,i}=\left\{\begin{array}{rl} 1 &amp; \text{if Treatment}=T3 \\ 0 &amp; \text{else} \end{array}\right.\)</span></p></li>
<li><p>Indicator for <em>Control</em>: <span class="math inline">\(I_{\text{Control},i}=\left\{\begin{array}{rl} 1 &amp; \text{if Treatment}=\text{Control} \\ 0 &amp; \text{else} \end{array}\right.\)</span></p></li>
</ul>
<p>We can see the values of these indicators for a few observations and their
original variable (<code>treatment</code>) in the following output. The bolded
observations show each of the indicators being “turned on”. For <em>T1</em>, all the
indicators stay at 0.</p>

<table>
<thead>
<tr class="header">
<th align="left">Treatment</th>
<th align="right">I_T2</th>
<th align="right">I_T3</th>
<th align="right">I_Control</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T2</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T2</td>
<td align="right">1</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">T1</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">0</td>
</tr>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
<tr class="even">
<td align="left">Control</td>
<td align="right">0</td>
<td align="right">0</td>
<td align="right">1</td>
</tr>
<tr class="odd">
<td align="left">T3</td>
<td align="right">0</td>
<td align="right">1</td>
<td align="right">0</td>
</tr>
</tbody>
</table>
<p>When we fit the additive model of the form <code>y~x+group</code>, the <code>lm</code>
function takes the <span class="math inline">\(\boldsymbol{J}\)</span> categories and creates <span class="math inline">\(\boldsymbol{J-1}\)</span>
indicator variables.
 
The baseline level is always handled in the intercept.
The true model will be of the form</p>
<p><span class="math display">\[y_i=\beta_0 + \beta_1x_i +\beta_2I_{\text{Level}2,i}+\beta_3I_{\text{Level}3,i}
+\cdots+\beta_{J}I_{\text{Level}J,i}+\varepsilon_i\]</span></p>
<p>where the <span class="math inline">\(I_{\text{CatName}j,i}\text{&#39;s}\)</span> are the different indicator variables.
Note that each indicator variable gets a coefficient associated with it and is
“turned on” whenever the <span class="math inline">\(i^{th}\)</span> observation is in that category. Only one of
the <span class="math inline">\(I_{\text{CatName}j,i}\text{&#39;s}\)</span> is a 1 for any observation, so the
y-intercept will either be <span class="math inline">\(\beta_0\)</span> for the baseline group or <span class="math inline">\(\beta_0+\beta_j\)</span>
for <span class="math inline">\(j=2,\ldots,J\)</span>. It is important to remember that this
is an “additive” model since the effects just add and there is no interaction
between the grouping variable and the quantitative predictor. To be able to
trust this model, we need to check that we do not need different slope
coefficients for the groups as discussed in the next section.</p>
<p>For these types of models, it is always good to start with a plot of the data
set with regression lines for each group – assessing whether the lines look
relatively parallel or not.

In Figure <a href="chapter8.html#fig:Figure8-21">8.22</a>, there are some
differences in slopes – we investigate that further in the next section. For
now, we can proceed with fitting the additive model with different intercepts
for the four levels of <code>treatment</code> and the quantitative explanatory variable, <code>du1</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">head1 &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1<span class="op">+</span>treatment, <span class="dt">data=</span>Headache)
<span class="kw">summary</span>(head1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = du2 ~ du1 + treatment, data = Headache)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9085 -0.9551 -0.3118  1.1141 10.5364 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)       0.80918    0.50095   1.615    0.110
## du1               0.83705    0.05176  16.172   &lt;2e-16
## treatmentT2       0.07692    0.62622   0.123    0.903
## treatmentT3       0.80919    0.59271   1.365    0.175
## treatmentControl -0.55752    0.61830  -0.902    0.370
## 
## Residual standard error: 2.14 on 93 degrees of freedom
## Multiple R-squared:  0.7511, Adjusted R-squared:  0.7404 
## F-statistic: 70.16 on 4 and 93 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The complete estimated regression model is</p>
<p><span class="math display">\[\widehat{\text{du2}}_i=0.809+0.837\cdot\text{du1}_i +0.077I_{\text{T2},i}+0.809I_{\text{T3},i}-0.558I_{\text{Control},i}\ .\]</span></p>
<p>For each group, the model simplifies to an SLR as follows:</p>
<ul>
<li>For <em>T1</em> (baseline):</li>
</ul>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{du2}}_i &amp;=0.809+0.837\cdot\text{du1}_i +0.077I_{\text{T2},i}+0.809I_{\text{T3},i}-0.558I_{\text{Control},i} \\
&amp;= 0.809+0.837\cdot\text{du1}_i+0.077*0+0.809*0-0.558*0 \\
&amp;= 0.809+0.837\cdot\text{du1}_i.
\end{array}\]</span></p>
<ul>
<li>For <em>T2</em>:</li>
</ul>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{du2}}_i &amp;=0.809+0.837\cdot\text{du1}_i+0.077I_{\text{T2},i}+0.809I_{\text{T3},i}-0.558I_{\text{Control},i} \\
&amp;= 0.809+0.837\cdot\text{du1}_i+0.077*1+0.809*0-0.558*0 \\
&amp;= 0.809+0.837\cdot\text{du1}_i + 0.077 \\
&amp;= 0.886+0.837\cdot\text{du1}_i.
\end{array}\]</span></p>

<ul>
<li>Similarly for <em>T3</em>:</li>
</ul>
<p><span class="math display">\[\widehat{\text{du2}}_i = 1.618 + 0.837\cdot\text{du1}_i\ .\]</span></p>
<ul>
<li>Finally, for <em>Control</em>:</li>
</ul>
<p><span class="math display">\[\widehat{\text{du2}}_i = 0.251 + 0.837\cdot\text{du1}_i\ .\]</span></p>
<p>To reinforce what this additive model is doing, Figure <a href="chapter8.html#fig:Figure8-22">8.23</a>
displays the estimated regression lines for all four
groups, showing the shifts in the <em>y</em>-intercepts among the groups.
</p>

<div class="figure"><span id="fig:Figure8-22"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-22-1.png" alt="Plot of estimated noise tolerance additive model." width="960" />
<p class="caption">
Figure 8.23: Plot of estimated noise tolerance additive model.
</p>
</div>
<p>The term-plot (Figure <a href="chapter8.html#fig:Figure8-23">8.24</a>) shows how the <em>T3</em> group
seems to have shifted up the most relative to
the others and the <em>Control</em> group seems to be noticeably lower than the others,
in the model that otherwise assumes that the same relationship holds between
<code>du1</code> and <code>du2</code> for all the groups. After controlling for the <em>Treatment</em>
group, for a 1 decibel increase in initial tolerances, we expect, on average,
to obtain a 0.84 decibel change in the second tolerance measurement. The
<strong><em>R</em></strong><sup>2</sup> shows that this is a decent model for the responses, with
this model explaining 75.1% percent of the
variation in the second decibel tolerance measure. We should check the
diagnostic plots and VIFs to check for any issues – all the diagnostics and
assumptions are as before except that there is no assumption of linearity
between the grouping variable and the responses.

Additionally, sometimes we
need to add group information to diagnostics to see if any patterns in residuals look
different in different groups, like linearity or non-constant variance.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(head1))</code></pre>
<div class="figure"><span id="fig:Figure8-23"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-23-1.png" alt="Term-plots of the additive decibel tolerance model." width="576" />
<p class="caption">
Figure 8.24: Term-plots of the additive decibel tolerance model.
</p>
</div>
<p>The diagnostic plots in Figure <a href="chapter8.html#fig:Figure8-24">8.25</a> provides some
indications of a few observations in the tails that deviate from a normal
distribution to having slightly heavier tails but
only one outlier is of real concern. There is a small indication of increasing
variability as a function of the fitted values as both the Residuals vs. Fitted
and Scale-Location plots show some fanning out for higher values but this is a
minor issue. There are no influential points in the data set. </p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(head1,
     <span class="dt">sub.caption=</span><span class="st">&quot;Plot of diagnostics for additive model with du1 and treatment for du2&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-24"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-24-1.png" alt="Diagnostic plots for the additive decibel tolerance model." width="960" />
<p class="caption">
Figure 8.25: Diagnostic plots for the additive decibel tolerance model.
</p>
</div>
<p>The VIFs are different for categorical variables than for quantitative
predictors in MLR. The 4 levels
are combined in a measure called the <strong><em>generalized VIF (GVIF)</em></strong>. For GVIFs,
we only focus on the inflation of the SE scale (square root for 1 df effects
and raised to the power <span class="math inline">\(1/(2*J)\)</span> for a <span class="math inline">\(J\)</span>-level predictor). On this scale,
the interpretation is as <strong>the multiplicative increase in the SEs for the coefficients on all the indicator variables due to
multicolinearity with other predictors</strong>. In this model, the SE for <code>du1</code> is
1.009 times larger due to multicollinearity with other predictors
and the SEs for the indicator variables are 1.003 times larger due to multicollinearity than they otherwise
would have been. Neither are large so multicollinearity is not a problem in
this model.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(car)
<span class="kw">vif</span>(head1)</code></pre>
<pre><code>##              GVIF Df GVIF^(1/(2*Df))
## du1       1.01786  1        1.008891
## treatment 1.01786  3        1.002955</code></pre>
<p>While there are inferences available in the model output, the tests for the
indicator variables are not too informative since they only compare each group
to the baseline. In Section <a href="chapter8.html#section8-12">8.12</a>, we see how to use ANOVA
<em>F</em>-tests to help us ask general questions about including a categorical predictor in the
model. But we can compare adjusted <strong><em>R</em></strong><sup>2</sup> values with and without
<em>Treatment</em> to see if including the categorical variable was “worth
it”:</p>
<pre class="sourceCode r"><code class="sourceCode r">head1R &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1, <span class="dt">data=</span>Headache)</code></pre>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(head1R)</code></pre>
<pre><code>## 
## Call:
## lm(formula = du2 ~ du1, data = Headache)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.9887 -0.8820 -0.2765  1.1529 10.4165 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.84744    0.36045   2.351   0.0208
## du1          0.85142    0.05189  16.408   &lt;2e-16
## 
## Residual standard error: 2.165 on 96 degrees of freedom
## Multiple R-squared:  0.7371, Adjusted R-squared:  0.7344 
## F-statistic: 269.2 on 1 and 96 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>The adjusted <strong><em>R</em></strong><sup>2</sup> in the model with both <em>Treatment</em> and <em>du1</em> is
0.7404 and the adjusted <em>R</em><sup>2</sup> for this reduced model with just <em>du1</em> is 0.7344,
suggesting the <em>Treatment</em> is useful. The next section
provides a technique to be able to work with different slopes on the
quantitative predictor for each group. Comparing those results to the results
for the additive model allows assessment of the assumption in this section that
all the groups had the same slope coefficient for the quantitative variable.</p>
</div>
<div id="section8-11" class="section level2">
<h2><span class="header-section-number">8.11</span> Different slopes and different intercepts</h2>
<p>Sometimes researchers are specifically interested in whether the slopes vary
across groups or the regression lines in the scatterplot for the different
groups may not look parallel or it may just be hard to tell visually if there
really is a difference in the slopes.

Unless you are <strong>very sure</strong> that there
is not an interaction between the grouping variable and the quantitative
predictor, you should start by fitting a model containing an
interaction and then see if you can drop it.


It may be the case that you end up
with the simpler additive model from the previous sections, but you don’t want
to assume the same slope across groups unless you are absolutely sure that is
the case.

This should remind you a bit of the discussions of the additive and
interaction models in the Two-way ANOVA material. The models, concerns, and
techniques are very similar, but with the quantitative variable replacing one
of the two categorical variables. As always, the scatterplot is a good first
step to understanding whether we need the extra complexity that these models
require.</p>
<p>A new example provides motivation for the consideration of different
slopes and
intercepts. A study was performed to address whether the relationship between
nonverbal IQs and
reading accuracy differs between dyslexic and non-dyslexic students. Two groups
of students were identified, one group of <em>dyslexic</em> students was identified
first (19 students) and then a group of gender and age similar student matches
were identified (25 students) for a total sample size of <span class="math inline">\(n=44\)</span>, provided in the
<code>dyslexic3</code> data set from the <code>smdata</code> package <span class="citation">(Merkle and Smithson <a href="#ref-R-smdata" role="doc-biblioref">2018</a>)</span>.

This type of study design is an attempt to “balance” the data from the two
groups on some important characteristics to make the comparisons of the groups
as fair as possible.

The researchers attempted to balance the characteristics
of the subjects in the two groups so that if they found different results for
the two groups, they could attribute it to the main difference they used to
create the groups – dyslexia or not. This design, <strong><em>case-control</em></strong> or
<strong>case-comparison</strong> where each subject with a trait is matched to one or more
subjects in the “control” group would hopefully reduce confounding from other
factors and then allow stronger conclusions in situations where it is
impossible to randomly
assign treatments to subjects.

We still would avoid using “causal” language but
this design is about as good as you can get when you are unable to randomly
assign levels to subjects.</p>
<p>Using these data, we can explore the relationship between nonverbal
IQ scores and reading accuracy, with reading accuracy measured as a proportion
correct. The fact that there is an
upper limit to the response variable attained by many students will cause
complications below, but we can still learn something from our attempts to
analyze these data using an MLR model. The scatterplot in
Figure <a href="chapter8.html#fig:Figure8-25">8.26</a> seems to indicate some clear differences in the
<em>IQ</em> vs <em>reading score</em> relationship between the <em>dys</em>=0 (non-dyslexic) and
<em>dys</em>=1 (dyslexic) students. Note that the IQ is standardized to have mean 0
and standard deviation of 1 which means that
a 1 unit change in IQ score is a 1 SD change and that the <em>y</em>-intercept (for
<span class="math inline">\(x=0\)</span>) is right in the center of the plot and actually interesting<a href="#fn110" class="footnote-ref" id="fnref110"><sup>110</sup></a>.</p>
<p>(ref:fig8-25) Scatterplot for reading score versus nonverbal IQ by dyslexia
group.</p>
<div class="figure"><span id="fig:Figure8-25"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-25-1.png" alt="(ref:fig8-25)" width="576" />
<p class="caption">
Figure 8.26: (ref:fig8-25)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(smdata)
<span class="kw">data</span>(<span class="st">&quot;dyslexic3&quot;</span>)
dyslexic3 &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(dyslexic3)
dyslexic3<span class="op">$</span>dys &lt;-<span class="st"> </span><span class="kw">factor</span>(dyslexic3<span class="op">$</span>dys)
<span class="kw">scatterplot</span>(score<span class="op">~</span>ziq<span class="op">|</span>dys, <span class="dt">xlab=</span><span class="st">&quot;Standardized nonverbal IQ scores&quot;</span>,
            <span class="dt">ylab=</span><span class="st">&quot;Reading score&quot;</span>, <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Plot of IQ vs Reading by dyslexia status&quot;</span>)</code></pre>
<p>To allow for both different y-intercepts and slope coefficients on the
quantitative predictor, we need to include a “modification” of the slope
coefficient. This is performed using an <strong><em>interaction</em></strong> between the two
predictor variables where we allow the impacts of one variable
(slopes) to change based on the levels of another variable (grouping variable).

The formula notation is <code>y~x*group</code>, remembering that this also includes the
<strong><em>main effects</em></strong> (the additive variable components) as well as the
interaction coefficients as we discussed in the Two-Way ANOVA interaction model.

We can start with the general model for a two-level categorical variable with an
interaction, which is</p>
<p><span class="math display">\[y_i=\beta_0 + \beta_1x_i +\beta_2I_{\text{CatName},i} +
{\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i,\]</span></p>
<p>where the new component involves both the indicator and the quantitative
predictor variable. The <span class="math inline">\(\color{red}{\boldsymbol{\beta_3}}\)</span> coefficient will be
found in a row of output with <strong>both</strong> variable names in it (with the indicator
level name) with a colon between them (something like <code>x:grouplevel</code>). As
always, the best way to understand any
model involving indicators is to plug in 0s or 1s for the indicator variable(s)
and simplify the equations. </p>
<ul>
<li><p>For any observation in the baseline group <span class="math inline">\(I_{\text{CatName},i}=0\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
  {\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>simplifies quickly to:</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\varepsilon_i\ .\]</span></p>
<ul>
<li>So the baseline group’s model involves the initial intercept and
quantitative slope coefficient.</li>
</ul></li>
<li><p>For any observation in the second category <span class="math inline">\(I_{\text{CatName},i}=1\)</span>, so</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2I_{\text{CatName},i}+
  {\color{red}{\boldsymbol{\beta_3I_{\text{CatName},i}x_i}}}+\varepsilon_i\]</span></p>
<p>is</p>
<p><span class="math display">\[y_i=\beta_0+\beta_1x_i+\beta_2*1+
  {\color{red}{\boldsymbol{\beta_3*1*x_i}}}+\varepsilon_i\]</span></p>
<p>which “simplifies” to</p>
<p><span class="math display">\[y_i = (\beta_0+\beta_2) + (\beta_1+{\color{red}{\boldsymbol{\beta_3}}})x_i
  +\varepsilon_i,\]</span></p>
<p>by combining like terms.</p>
<ul>
<li>For the second category, the model contains a modified y-intercept,
now <span class="math inline">\(\beta_0+\beta_2\)</span>, <strong>and</strong> a modified slope coefficient, now
<span class="math inline">\(\beta_1+\color{red}{\boldsymbol{\beta_3}}\)</span>.</li>
</ul></li>
</ul>
<p>We can make this more concrete by applying this to the dyslexia data with
<code>dys</code> as a categorical variable for dyslexia status of subjects (levels of 0
and 1) and <code>ziq</code> the standardized IQ. The estimated model is:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Because dys was just numerically coded - makes it a factor</span>
dyslexic3<span class="op">$</span>dys &lt;-<span class="st"> </span><span class="kw">factor</span>(dyslexic3<span class="op">$</span>dys) 
dys_model &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">*</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq * dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26362 -0.04152  0.01682  0.06790  0.17740 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.87586    0.02391  36.628  &lt; 2e-16
## ziq          0.05827    0.02535   2.299   0.0268
## dys1        -0.27951    0.03827  -7.304 7.11e-09
## ziq:dys1    -0.07285    0.03821  -1.907   0.0638
## 
## Residual standard error: 0.1017 on 40 degrees of freedom
## Multiple R-squared:  0.712,  Adjusted R-squared:  0.6904 
## F-statistic: 32.96 on 3 and 40 DF,  p-value: 6.743e-11</code></pre>
<p>The estimated model can be written as</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i - 0.280I_{\text{Level }1,i}
-{\color{red}{\boldsymbol{0.073}}}I_{\text{Level }1,i}\cdot\text{ZIQ}_i\]</span></p>
<p>and simplified for the two groups as:</p>
<ul>
<li><p>For the baseline (non-dyslexic, <span class="math inline">\(I_{\text{Level}1,i}=0\)</span>) students:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.876+0.058\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>For the deviation (dyslexic, <span class="math inline">\(I_{\text{Level1},i}=1\)</span>) students:</p>
<p><span class="math display">\[\begin{array}{rl}
  \widehat{\text{Score}}_i&amp;=0.876+0.058\cdot\text{ZIQ}_i - 0.280*1-
  0.073*1\cdot\text{ZIQ}_i \\
  &amp;=(0.876- 0.280) + (0.058-0.073)\cdot\text{ZIQ}_i, \\
  \end{array}\]</span></p>
<p>which simplifies finally to:</p>
<p><span class="math display">\[\widehat{\text{Score}}_i=0.596-0.015\cdot\text{ZIQ}_i\ .\]</span></p></li>
<li><p>So the slope switched from 0.058 in the non-dyslexic students to -0.015 in
the dyslexic students. The interpretations of these coefficients are outlined
below:</p>
<ul>
<li><p>For the non-dyslexic students: For a 1 SD increase in verbal IQ score,
we expect, on average, for the mean reading score to go up by 0.058
“points”.</p></li>
<li><p>For the dyslexic students: For a 1 SD increase in verbal IQ score, we
expect, on average, for the mean reading score to change by -0.015
“points”.</p></li>
</ul></li>
</ul>
<p>So, an expected pattern of results emerges for the non-dyslexic students.
Those with higher IQs tend to have
higher reading accuracy; this does not mean higher IQ’s cause more accurate
reading because random assignment of IQ is not possible. However, for the
dyslexic students, the relationship is not what one would might expect. It is
slightly negative, showing that higher IQ’s are related to lower reading
accuracy. What we conclude from this is that we should not expect higher IQ’s
to show higher performance on a test like this.</p>
<p>Checking the assumptions is always recommended before getting focused
on the inferences in the model.
When fitting models with multiple groups, it is possible to see “groups” in the
fitted values (x-axis in Residuals vs Fitted and Scale-Location plots) and that is not a problem – it is a feature of these models.



You
should look for issues in the residuals for each group but the residuals should
overall still be normally distributed and have the same variability everywhere.
It is a bit hard to see issues in Figure <a href="chapter8.html#fig:Figure8-26">8.27</a> because of the
group differences,
but note the line of residuals for the higher fitted values. This is an
artifact of the upper threshold in the reading accuracy test used. As in the
first year of college GPA, these observations were <strong><em>censored</em></strong> – their
true score was outside the range of values we could observe – and so we did not
really get a measure of how good these students were since a lot of their
abilities were higher than the test could detect and they all binned up at the
same value of getting all the questions correct. The relationship in this group might be even stronger if we could
really observe differences in the highest level readers. We should treat the
results for the non-dyslexic group with caution even though they are clearly
scoring on average higher and have a different slope than the results for the
dyslexic students. The normality and influence diagnostics do not suggest any
major issues other than this.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(dys_model,
     <span class="dt">sub.caption=</span><span class="st">&quot;Plot of diagnostics for Dyslexia Interaction model&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-26"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-26-1.png" alt="Diagnostic plots for interaction model for reading scores." width="960" />
<p class="caption">
Figure 8.27: Diagnostic plots for interaction model for reading scores.
</p>
</div>
<p>For these models, we have relaxed an earlier assumption that data were collected
from only one group. In
fact, we are doing specific research that is focused on questions about the
differences between groups. However, these models still make assumptions that,
within a specific group, the linearity assumption is met. They also assume that
the variability in the residuals is the same for all observations. Sometimes it
can be difficult to check the assumptions by looking at the overall diagnostic
plots and it may be easier to go back to the original scatterplot or plot the
residuals vs fitted values by group to fully assess the results.

For example,
Figure <a href="chapter8.html#fig:Figure8-27">8.28</a>
shows a scatterplot of the residuals vs the quantitative explanatory variable
by the groups. The variability in the residuals is a bit larger in the
non-dyslexic group, possibly suggesting that variability in the reading test is
higher for higher scoring individuals even though we couldn’t observe all of
that variability because there were so many perfect scores in this group.</p>
<p>(ref:fig8-27) Plot of Residuals vs Fitted from interaction dyslexia data model
with groups indicated.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">scatterplot</span>(<span class="kw">residuals</span>(dys_model)<span class="op">~</span><span class="kw">fitted</span>(dys_model)<span class="op">|</span>dys,
            <span class="dt">data=</span>dyslexic3, <span class="dt">smooth=</span>F)</code></pre>
<div class="figure"><span id="fig:Figure8-27"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-27-1.png" alt="(ref:fig8-27)" width="576" />
<p class="caption">
Figure 8.28: (ref:fig8-27)
</p>
</div>
<p>If we feel comfortable enough with the assumptions to trust the inferences here
(this might be dangerous), then we can consider what some of the model inferences
provide us in this situation. For example, the test for
<span class="math inline">\(H_0: {\color{red}{\boldsymbol{\beta_3}}}=0\)</span> vs
<span class="math inline">\(H_A: {\color{red}{\boldsymbol{\beta_3}}}\ne 0\)</span> provides an interesting comparison.
Under the null hypothesis, the two groups would have the same slope so it
provides an
opportunity to directly consider whether the relationship (via the slope) is
different between the groups in their respective populations. We find
<span class="math inline">\(t=-1.907\)</span> which, if the assumptions are met, follows a <span class="math inline">\(t(40)\)</span>-distribution
under the null hypothesis. This test
statistic has a corresponding p-value of 0.0638. So it provides some evidence of a difference in the slopes but it isn’t strong evidence of that. There are serious issues (like getting
the wrong idea about directions of relationships) if we ignore a potentially
important
interaction and some statisticians would recommend retaining interactions even
if the evidence is only moderate for its inclusion in the model.

For the
original research question of whether the relationships differ for the two
groups, we only have marginal evidence to support that result. Possibly with a
larger sample size or a reading test that only a few students could get 100%
on, the researchers might have detected a more pronounced difference in the
slopes for the two groups.</p>
<p>In the presence of a categorical by quantitative interaction, term-plots can
be generated that plot the results for each group on the same display.

This
basically provides a plot of the “simplified” SLR models for each group. In
Figure <a href="chapter8.html#fig:Figure8-28">8.29</a> we can see noticeable differences in the slopes
and intercepts. Note that
testing for differences in intercepts between groups is not very interesting
when there are different slopes because if you change the slope, you have to
change the intercept. The plot shows that there are clear differences in the
means even though we don’t have a test to directly assess that in this
complicated of a model<a href="#fn111" class="footnote-ref" id="fnref111"><sup>111</sup></a>.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_model), <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>, <span class="dt">multiline=</span>T, <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</code></pre>
<div class="figure"><span id="fig:Figure8-28"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-28-1.png" alt="Term-plots for interaction model for reading scores." width="576" />
<p class="caption">
Figure 8.29: Term-plots for interaction model for reading scores.
</p>
</div>
<p>It certainly appears in the plots that IQ has a different impact on the mean
score in the two groups (even though the p-value only provided marginal
evidence). To
reinforce the potential dangers of forcing the same slope for both groups,
consider the additive model for these data. Again, this just shifts one group
off the other one, but both have the same slope. The following model summary
and term-plots (Figure <a href="chapter8.html#fig:Figure8-29">8.30</a>) suggest the potentially
dangerous conclusion that
can come from assuming a common slope when that might not be the case.</p>


<pre class="sourceCode r"><code class="sourceCode r">dys_modelR &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>ziq<span class="op">+</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_modelR)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ ziq + dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.26062 -0.05565  0.02932  0.07577  0.13217 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.89178    0.02312  38.580  &lt; 2e-16
## ziq          0.02620    0.01957   1.339    0.188
## dys1        -0.26879    0.03905  -6.883 2.41e-08
## 
## Residual standard error: 0.1049 on 41 degrees of freedom
## Multiple R-squared:  0.6858, Adjusted R-squared:  0.6705 
## F-statistic: 44.75 on 2 and 41 DF,  p-value: 4.917e-11</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR))</code></pre>
<div class="figure"><span id="fig:Figure8-29"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-29-1.png" alt="Term-plots for additive model for reading scores." width="576" />
<p class="caption">
Figure 8.30: Term-plots for additive model for reading scores.
</p>
</div>
<p>This model provides little to no evidence that IQ is related to reading score for all
students (<span class="math inline">\(t_{41}=1.34\)</span>, p-value=0.188) but strong evidence of a difference in the
y-intercepts (<span class="math inline">\(t_{41}=-6.88\)</span>, p-value <span class="math inline">\(&lt;0.00001\)</span>).</p>
<p>Since the IQ term has a large p-value, then we could drop it from the
model – leaving a model that only includes the grouping variable:</p>

<pre class="sourceCode r"><code class="sourceCode r">dys_modelR2 &lt;-<span class="st"> </span><span class="kw">lm</span>(score<span class="op">~</span>dys, <span class="dt">data=</span>dyslexic3)
<span class="kw">summary</span>(dys_modelR2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = score ~ dys, data = dyslexic3)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.25818 -0.04510  0.02514  0.09520  0.09694 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  0.90480    0.02117  42.737   &lt;2e-16
## dys1        -0.29892    0.03222  -9.278    1e-11
## 
## Residual standard error: 0.1059 on 42 degrees of freedom
## Multiple R-squared:  0.6721, Adjusted R-squared:  0.6643 
## F-statistic: 86.08 on 1 and 42 DF,  p-value: 1e-11</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(dys_modelR2))</code></pre>
<div class="figure"><span id="fig:Figure8-30"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-30-1.png" alt="Term-plot for dyslexia status only model for reading scores." width="576" />
<p class="caption">
Figure 8.31: Term-plot for dyslexia status only model for reading scores.
</p>
</div>
<p>These results, including the term-plot in Figure <a href="chapter8.html#fig:Figure8-30">8.31</a>, show
that there is evidence of a difference in the mean reading scores
between the two groups and maybe that is all these data really say… This is the
logical outcome if we decide that the interaction is not important IN THIS DATA
SET. In general, if the interaction is dropped, the interaction model can be
reduced to considering an additive model with the categorical and quantitative
predictor variables.


Either or both of those variables could also be considered
for removal, possibly starting with the variable with the larger p-value,
leaving a string of ever-simpler models possible if large p-values are continually
encountered<a href="#fn112" class="footnote-ref" id="fnref112"><sup>112</sup></a>.</p>
<p>It is useful to note that the last model has returned us to the first
model we encountered in Chapter <a href="chapter2.html#chapter2">2</a> where we were just comparing
the means for two groups.
However, the researchers probably were not seeking to make the discovery that
dyslexic students have a tougher time than non-dyslexic students on a reading
test but sometimes that is all that the data support. The key part of this
sequence of decisions was how much evidence you think a p-value of 0.06
contains…</p>
<p>For more than two categories in a categorical variable, the model
contains more
indicators to keep track of but uses the same ideas. We have to deal with
modifying the intercept and slope coefficients for <strong>every</strong> deviation group so the task is onerous but relatively repetitive. 
The general model is:</p>
<p><span class="math display">\[\begin{array}{rl}
y_i=\beta_0 &amp;+ \beta_1x_i +\beta_2I_{\text{Level }2,i}+\beta_3I_{\text{Level }3,i}
+\cdots+\beta_JI_{\text{Level }J,i} \\
&amp;+\beta_{J+1}I_{\text{Level }2,i}\:x_i+\beta_{J+2}I_{\text{Level }3,i}\:x_i
+\cdots+\beta_{2J-1}I_{\text{Level }J,i}\:x_i +\varepsilon_i.\ 
\end{array}\]</span></p>
<p>Specific to the audible tolerance/headache data that had four groups. The model
with an interaction present is</p>
<p><span class="math display">\[\begin{array}{rl}
\text{du2}_i = \beta_0 &amp;+ \beta_1\cdot\text{du1}_i + \beta_2I_{T2,i} +
\beta_3I_{T3,i} + \beta_4I_{\text{Control},i} \\
&amp;+ \beta_5I_{T2,i}\cdot\text{du1}_i + \beta_6I_{T3,i}\cdot\text{du1}_i
+ \beta_7I_{\text{Control},i}\cdot\text{du1}_i+\varepsilon_i.\ 
\end{array}\]</span></p>
<p>Based on the following output, the estimated general regression model is</p>
<p><span class="math display">\[\begin{array}{rl}
\widehat{\text{du2}}_i = 1.33 &amp;+ 0.733\cdot\text{du1}_i - 0.236I_{T2,i} -
0.316I_{T3,i} - 1.091I_{\text{Control},i} \\
&amp;+ 0.066I_{T2,i}\cdot\text{du1}_i + 0.199I_{T3,i}\cdot\text{du1}_i
+ 0.106I_{\text{Control},i}\cdot\text{du1}_i.\ 
\end{array}\]</span></p>
<p>Then we could work out the specific equation for <strong>each group</strong> with
replacing their indicator variable in two places with 1s and the rest of
the indicators with 0. For example, for the <em>Control</em> group:</p>
<p><span class="math display">\[\begin{array}{rll}
\widehat{\text{du2}}_i &amp;= 1.33 &amp;+ 0.733\cdot\text{du1}_i - 0.236*0 - 0.316*0
- 1.091*1 \\
&amp;&amp;+ 0.066*0*\text{du1}_i + 0.199*0*\text{du1}_i+ 0.106*1*\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=1.33&amp;+0.733\cdot\text{du1}_i - 1.091 + 0.106\cdot\text{du1}_i \\
\widehat{\text{du2}}_i&amp;=0.239 &amp;+ 0.839\cdot\text{du1}_i.\ 
\end{array}\]</span></p>

<div class="figure"><span id="fig:Figure8-31"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-31-1.png" alt="Term-plot for decibel tolerance interaction model (version 1)." width="672" />
<p class="caption">
Figure 8.32: Term-plot for decibel tolerance interaction model (version 1).
</p>
</div>

<pre class="sourceCode r"><code class="sourceCode r">head2 &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1<span class="op">*</span>treatment, <span class="dt">data=</span>Headache)
<span class="kw">summary</span>(head2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = du2 ~ du1 * treatment, data = Headache)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.8072 -1.0969 -0.3285  0.8192 10.6039 
## 
## Coefficients:
##                      Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)           1.33157    0.66027   2.017   0.0467
## du1                   0.73319    0.09969   7.355 8.53e-11
## treatmentT2          -0.23560    1.13414  -0.208   0.8359
## treatmentT3          -0.31613    0.95767  -0.330   0.7421
## treatmentControl     -1.09084    0.95020  -1.148   0.2540
## du1:treatmentT2       0.06623    0.17473   0.379   0.7055
## du1:treatmentT3       0.19904    0.13350   1.491   0.1395
## du1:treatmentControl  0.10604    0.14326   0.740   0.4611
## 
## Residual standard error: 2.148 on 90 degrees of freedom
## Multiple R-squared:  0.7573, Adjusted R-squared:  0.7384 
## F-statistic: 40.12 on 7 and 90 DF,  p-value: &lt; 2.2e-16</code></pre>

<p>Or we can let the term-plots (Figures <a href="chapter8.html#fig:Figure8-31">8.32</a> and
<a href="chapter8.html#fig:Figure8-32">8.33</a>) show us all four different simplified models. Here we
can see that all the slopes “look” to be pretty similar. When the interaction
model is fit and the results “look” like the additive model, there is a good
chance that we will be able to avoid all this complication and just use the
additive model without missing anything interesting.

There are two different
options for displaying interaction models. Version 1 (Figure
<a href="chapter8.html#fig:Figure8-31">8.32</a>) has a
different panel for each level of the categorical variable and Version 2
(Figure <a href="chapter8.html#fig:Figure8-32">8.33</a>) puts all the lines on the same plot. In this
case, neither
version shows much of a difference and Version 2 overlaps so much that you
can’t see all the groups. In these situations, it can be useful to make the term-plots with <code>multiline=T</code> and <code>multiline=F</code> and select the version that captures the results best.</p>
<p>(ref:fig8-32) Term-plot for decibel tolerance interaction model (version 2).
This plot is not printed in color because it is impossible to distinguish the
four groups whether in color or black and white.</p>
<div class="figure"><span id="fig:Figure8-32"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-32-1.png" alt="(ref:fig8-32)" width="576" />
<p class="caption">
Figure 8.33: (ref:fig8-32)
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(head2), <span class="dt">x.var=</span><span class="st">&quot;du1&quot;</span>, <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>)
<span class="kw">plot</span>(<span class="kw">allEffects</span>(head2), <span class="dt">x.var=</span><span class="st">&quot;du1&quot;</span>, <span class="dt">multiline=</span>T, <span class="dt">ci.style=</span><span class="st">&quot;bands&quot;</span>)</code></pre>
<p>In situations with more than 2 levels, the <span class="math inline">\(t\)</span>-tests for the interaction or
changing y-intercepts are not informative for deciding if you really need
different slopes or intercepts for all the groups.

They only tell
you if a specific group is potentially different from the baseline group and
the choice of the baseline is arbitrary. To assess whether we really need to
have varying slopes or intercepts with more than two groups we need to develop
<span class="math inline">\(F\)</span>-tests for the interaction part of the model.</p>



</div>
<div id="section8-12" class="section level2">
<h2><span class="header-section-number">8.12</span> F-tests for MLR models with quantitative and categorical variables and interactions</h2>

<p>For models with multi-category <span class="math inline">\((J&gt;2)\)</span> categorical variables we need a method
for deciding if all the extra
complexity present in the additive or interaction models is necessary. We can
appeal to model selection methods such as the adjusted <strong><em>R</em></strong><sup>2</sup> that
focus on balancing model fit and complexity but interests often move to trying to
decide if the differences are more extreme
than we would expect by chance if there were no group differences in intercepts
or slopes. Because of the multi-degree of freedom aspects of the use of indicator
variables (<span class="math inline">\(J-1\)</span> variables for a <span class="math inline">\(J\)</span> level categorical variable), we have to
develop tests that combine and assess information across multiple
“variables” – even though these indicators all pertain to a single original
categorical variable.  ANOVA <span class="math inline">\(F\)</span>-tests did exactly
this sort of thing in the One and Two-Way ANOVA models and can do that for us
here. There are two models that we perform tests in – the additive and the
interaction models.


We start with a discussion of the tests in an interaction
setting since that provides us the <strong>first test to consider</strong> in most situations
to assess evidence of whether the extra complexity of varying slopes is really
needed. If we don’t “need” the varying
slopes or if the plot really does have lines for the groups that look
relatively parallel, we can fit the additive model and either assess evidence of
the need for different intercepts or for the quantitative predictor – either is
a reasonable next step.

Basically this establishes a set of <strong><em>nested models</em></strong>
(each model is a reduced version of another more complicated
model higher in the tree of models) displayed in Figure <a href="chapter8.html#fig:Figure8-33">8.34</a>.
This is based on
the assumption that we would proceed through the model, dropping terms if the
p-values are large (“not significant” in the diagram) to arrive at a final
model. </p>

<div class="figure"><span id="fig:Figure8-33"></span>
<img src="chapter8_files/nestedModelTree_medium.png" alt="Diagram of models to consider in an interaction model." width="338" />
<p class="caption">
Figure 8.34: Diagram of models to consider in an interaction model.
</p>
</div>
<p>If the initial interaction test suggests the interaction is important, then no
further refinement should be considered and that model should be explored (this
was the same protocol suggested in the 2-WAY ANOVA situation, the other place
where we considered interactions).

If the interaction is not deemed important
based on the test, then the model should be re-fit using both variables in an
additive model.

In that additive model, both variables can be assessed
conditional on the other one. If both have small p-values, then that is the
final model and should be explored further. If either the categorical or
quantitative variable have large p-values, then they can be dropped from the
model and the model re-fit with only one variable in it, usually starting with
dropping the component with the largest p-value if both are not “small”. Note
that if there is only a categorical variable remaining, then we would call that
linear model a One-Way ANOVA (quantitative response and <span class="math inline">\(J\)</span> group categorical
explanatory) and if the only remaining variable is quantitative, then a SLR model
is being fit. If that final variable has a
large p-value in either model, it can be removed and all that is left to
describe the responses is a mean-only model. Otherwise the single variable
model is the final model. Usually we will not have to delve deeply into this
tree of models, but it is good to consider the potential paths that an analysis
could involve before it is started.</p>
<p>To perform the first test (after checking that assumptions are met, of
course),
we can apply the <code>Anova</code> function from the <code>car</code> package to an interaction
model<a href="#fn113" class="footnote-ref" id="fnref113"><sup>113</sup></a>.

It will provide three tests, one for each variable by themselves, which are not
too interesting, and then the interaction test. This will result in an
<span class="math inline">\(F\)</span>-statistic that, if the assumptions are met, will follow an
<span class="math inline">\(F(J-1, n-2J)\)</span>-distribution
under the null hypothesis. This tests
the hypotheses:</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{H_0:}\)</span> <strong>The slope for <span class="math inline">\(\boldsymbol{x}\)</span> is the same for all
<span class="math inline">\(\boldsymbol{J}\)</span> groups in the population vs</strong></p></li>
<li><p><span class="math inline">\(\boldsymbol{H_A:}\)</span> <strong>The slope for <span class="math inline">\(\boldsymbol{x}\)</span> in at least one group
differs from the others in the population.</strong></p></li>
</ul>
<p>This test is also legitimate in the case of a two-level categorical variable
<span class="math inline">\((J=2)\)</span> and then follows an <span class="math inline">\(F(1, n-4)\)</span>-distribution under the null
hypothesis. With <span class="math inline">\(J=2\)</span>, the p-value from this test matches the results for the
<span class="math inline">\(t\)</span>-test <span class="math inline">\((t_{n-4})\)</span> for the single slope-changing coefficient in the model
summary output. The noise tolerance study, introduced in Section
<a href="chapter8.html#section8-10">8.10</a>, provides a situation for exploring the results in
detail.</p>
<p>With the <span class="math inline">\(J=4\)</span> level categorical variable (<em>Treatment</em>), the model for the
second noise tolerance measurement (<em>du2</em>) as a function of the interaction
between <em>Treatment</em> and initial noise tolerance (<em>du1</em>) is</p>
<p><span class="math display">\[\begin{array}{rl}
\text{du2}_i = \beta_0 &amp;+ \beta_1\cdot\text{du1}_i + \beta_2I_{T2,i} +
\beta_3I_{T3,i} + \beta_4I_{\text{Control},i} \\
&amp;+ \beta_5I_{T2,i}\cdot\text{du1}_i + \beta_6I_{T3,i}\cdot\text{du1}_i
+ \beta_7I_{\text{Control},i}\cdot\text{du1}_i+\varepsilon_i.
\end{array}\]</span></p>
<p>We can re-write the previous hypotheses in one of two more specific ways:</p>
<ul>
<li><p><span class="math inline">\(H_0:\)</span> The slope for <em>du1</em> is the same for all four <em>Treatment</em> groups in the
population OR</p></li>
<li><p><span class="math inline">\(H_0: \beta_5=\beta_6=\beta_7=0\)</span></p>
<ul>
<li>This defines a null hypothesis that all the deviation coefficients for
getting different slopes for the different treatments are 0 in the
population.</li>
</ul></li>
<li><p><span class="math inline">\(H_A:\)</span> The slope for <em>du1</em> is NOT the same for all four <em>Treatment</em> groups in
the population (at least one group has a different slope) OR</p></li>
<li><p><span class="math inline">\(H_A:\)</span> At least one of <span class="math inline">\(\beta_5,\beta_6,\beta_7\)</span> is different from 0 in the
population.</p>
<ul>
<li>The alternative states that at least one of the deviation coefficients
for getting different slopes for the different <em>Treatments</em> is not 0 in the
population.</li>
</ul></li>
</ul>
<p>In this situation, the results for the test of these hypotheses is in the row
labeled <code>du1:treatment</code> in the <code>Anova</code> output. The ANOVA table below shows
a test statistic of <span class="math inline">\(F=0.768\)</span> with the <em>numerator df</em> of 3, coming from <span class="math inline">\(J-1\)</span>,
and the <em>denominator df</em> of 90, coming from <span class="math inline">\(n-2J=98-2*4=90\)</span> and also provided
in the <code>Residuals</code> row in the table, leading to an <span class="math inline">\(F(3, 90)\)</span>-distribution
for the test statistic under the null hypothesis.

The p-value from this
distribution is 0.515, showing little to no evidence against
the null hypothesis. The conclusion is that there is insufficient evidence to
claim that the slope coefficient for <em>du1</em> in explaining <em>du2</em> is different for
at least one of the <em>Treatment</em> groups in the population.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">Anova</span>(head2)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: du2
##                Sum Sq Df  F value Pr(&gt;F)
## du1           1197.78  1 259.5908 &lt;2e-16
## treatment       23.90  3   1.7265 0.1672
## du1:treatment   10.63  3   0.7679 0.5150
## Residuals      415.27 90</code></pre>
<p>Without evidence to support an interaction, we should consider both the
quantitative and categorical variables in an additive model. The ANOVA table
for the additive model contains two interesting tests.


One test is for the
quantitative variable discussed previously. The other is for the categorical
variable, assessing whether different y-intercepts are needed. The additive
model here is</p>
<p><span class="math display">\[\text{du2}_i = \beta_0 + \beta_1\cdot\text{du1}_i + \beta_2I_{T2,i} +
\beta_3I_{T3,i} + \beta_4I_{\text{Control},i} +\varepsilon_i.\ \]</span></p>
<p>The hypotheses assessed in the ANOVA test for treatment are:</p>
<ul>
<li><p><span class="math inline">\(H_0:\)</span> The y-intercept for the model with <em>du1</em> is the same for all four
<em>Treatment</em> groups in the population OR</p></li>
<li><p><span class="math inline">\(H_0: \beta_2=\beta_3=\beta_4=0\)</span></p>
<ul>
<li>This defines a null hypothesis that all the deviation coefficients for
getting different y-intercepts for the different <em>Treatments</em> are 0 in
the population.</li>
</ul></li>
<li><p><span class="math inline">\(H_A:\)</span> The y-intercepts for the model with <em>du1</em> is NOT the same for all four
<em>Treatment</em> groups in the population (at least one group has a different
y-intercept) OR</p></li>
<li><p><span class="math inline">\(H_A:\)</span> At least one of <span class="math inline">\(\beta_2,\beta_3,\beta_4\)</span> is different from 0 in the
population.</p>
<ul>
<li>The alternative states that at least one of the deviation coefficients
for getting different y-intercepts for the different <em>Treatments</em> is not 0
in the population.</li>
</ul></li>
</ul>
<p>The <span class="math inline">\(F\)</span>-test for the categorical variable in an additive model follows
<span class="math inline">\(F(J-1, n-J-1)\)</span>-distribution under the null hypothesis.


For this example, the
test statistic for <em>Treatment</em> follows an <span class="math inline">\(F(3, 93)\)</span>-distribution under the null
hypothesis. The observed test statistic has a value of 1.74, generating a p-value
of 0.164. So we would fail to reject the null hypothesis and conclude that there
is little to no evidence to support the conclusion of some difference in y-intercepts
between the <em>treatment</em> groups, in a model with <em>du1</em>, in the population. We
could interpret this in the fashion we used initially in MLR by stating this
result as: there is little to no evidence of a difference in the mean <em>du2</em> for the
<em>Treatment</em> groups after controlling for <em>du1</em>.</p>
<pre class="sourceCode r"><code class="sourceCode r">head1 &lt;-<span class="st"> </span><span class="kw">lm</span>(du2<span class="op">~</span>du1<span class="op">+</span>treatment, <span class="dt">data=</span>Headache)
<span class="kw">Anova</span>(head1)</code></pre>
<pre><code>## Anova Table (Type II tests)
## 
## Response: du2
##           Sum Sq Df  F value Pr(&gt;F)
## du1       1197.8  1 261.5491 &lt;2e-16
## treatment   23.9  3   1.7395 0.1643
## Residuals  425.9 93</code></pre>
<p>In the same ANOVA table, there is a test for the <em>du1</em> effect. This tests
<span class="math inline">\(H_0: \beta_1=0\)</span> vs <span class="math inline">\(H_A: \beta_1\ne 0\)</span> in a model with different y-intercepts
estimated for the different groups. If we remove this term from the model, all
we are left with is different
y-intercepts for the groups. A model just with different y-intercepts is typically
called a One-Way ANOVA model. Here, there is evidence that the quantitative
variable is needed in the model after controlling for the different
y-intercepts for different treatments. Note that this interpretation retains
the conditional wording regardless of whether the other variable had a small
p-value or it did not. If you want an unconditional interpretation for a variable, then you
will need to refit the model without the other variable(s) after deciding that
they are not important.</p>
</div>
<div id="section8-13" class="section level2">
<h2><span class="header-section-number">8.13</span> AICs for model selection</h2>
<p>There are a variety of techniques for selecting among a set of potential models
or refining an initially fit MLR
model. Hypothesis testing can be used (in the case where we have nested models either by adding or deleting a single term at a time)
or comparisons of adjusted <strong><em>R</em></strong><sup>2</sup>
across different potential models (which is valid for nested or non-nested
model comparisons).

Diagnostics should play a role in the models considered and
in selecting among models that might appear to be similar on a model comparison
criterion.
In this section, a new model selection method is introduced that has stronger
theoretical underpinnings, a slightly more interpretable scale, and, often,
better performance in picking an optimal<a href="#fn114" class="footnote-ref" id="fnref114"><sup>114</sup></a>
model than the <strong><em>adjusted</em></strong> <strong><em>R</em></strong><sup>2</sup>. The measure is called the
<strong><em>AIC</em></strong> (Akaike’s An Information Criterion<a href="#fn115" class="footnote-ref" id="fnref115"><sup>115</sup></a>,
<span class="citation">(Akaike <a href="#ref-Akaike1974" role="doc-biblioref">1974</a>)</span>. It is extremely popular, but sometimes misused, in some fields
such as Ecology and has been applied in almost every other potential
application area where statistical models can be compared. <span class="citation">Burnham and Anderson (<a href="#ref-Burnham2002" role="doc-biblioref">2002</a>)</span>
have been responsible for popularizing the use of AIC for model
selection, especially in Ecology. The <strong>AIC is an estimate of the distance
(or discrepancy or divergence) between a candidate model and the true model,
on a log-scale</strong>, based on a measure called the Kullback-Leibler divergence. The
models that are closer (have a smaller distance) to the truth are better and we
can compare how close two models are to the truth, picking the one that has a
smaller distance (smaller AIC) as better. The AIC includes a component that is
on the log-scale, so negative values are possible and you should not be
disturbed if you are comparing large magnitude negative numbers – just pick the
model with the smallest AIC score.</p>
<p>The AIC is optimized (smallest) for a model that contains the optimal
balance of
simplicity of the
model with quality of fit to the observations. Scientists are driven to
different degrees by what is called the <strong><em>principle of parsimony</em></strong>: that
<strong>simpler explanations (models) are better if everything else is equal or even
close to equal</strong>. In this case, it would
mean that if two models are similarly good on AIC, then select the simpler of
the two models since it is more likely to be correct in general than the more
complicated model. The AIC is calculated as <span class="math inline">\(AIC=-2log(Likelihood)+2m\)</span>, where
the <strong><em>Likelihood</em></strong> provides a measure of fit of the model (we let R
calculate it for us) and gets smaller for better fitting models and
<span class="math inline">\(m\)</span> = (number of estimated <span class="math inline">\(\beta\text{&#39;s}+1\)</span>). The value <span class="math inline">\(m\)</span> is called the
<em>model degrees of freedom</em> for AIC calculations and relates to how
many total parameters are estimated.

Note that it is a different measure of
<em>degrees of freedom</em> than used in ANOVA <span class="math inline">\(F\)</span>-tests. The main things to understand
about the formula for the AIC is that as <span class="math inline">\(m\)</span> increases, the AIC will go up and
that as the fit improves, the <em>likelihood</em> will increase (so -2<em>log-likelihood</em> will get smaller)<a href="#fn116" class="footnote-ref" id="fnref116"><sup>116</sup></a>.</p>
<p>There are some facets of this discussion to keep in mind when comparing
models. More complicated models always fit better (we saw this for the
<strong><em>R</em></strong><sup>2</sup> measure, as the proportion of variation explained always goes
up if more “stuff” is put into the model even if the “stuff” isn’t useful). The
AIC resembles the adjusted <strong><em>R</em></strong><sup>2</sup> in that it incorporates the count
of the number of parameters estimated. This allows the AIC to make sure that
enough extra variability is explained in the responses to justify making the
model more complicated (increasing <span class="math inline">\(m\)</span>). The optimal model on AIC has to balance
adding complexity and increasing quality of the fit. Since this measure
provides an estimate of the distance or discrepancy to the “true model”, the
model with the smallest value “wins” – it is top-ranked on the AIC. Note that
the <strong>top-ranked AIC model</strong> will often <strong>not be the best fitting</strong> model
since the best fitting model is always the most complicated model considered.
The top AIC model is the one that is estimated to be closest to the truth,
where the truth is still unknown…</p>
<p>To help with interpreting the scale of AICs, they are often reported in a table
sorted from smallest to largest values with the AIC and the “delta AIC” or,
simply, <span class="math inline">\(\Delta\text{AIC}\)</span> reported. The</p>
<p><span class="math display">\[\Delta\text{AIC}=\text{AIC}_{\text{model}} - \text{AIC}_{\text{topModel}}\]</span></p>
<p>and so provides a value of 0 for the top-ranked AIC model and a measure of how
much worse on the AIC scale the other models are. A rule of thumb is that a 2
unit difference on AICs <span class="math inline">\((\Delta\text{AIC}=2)\)</span> is decent evidence of a
difference in the models and more than 4 units <span class="math inline">\((\Delta\text{AIC}&gt;4)\)</span> is a
really big difference. This is more based on experience than a distinct reason
or theoretical result but seems to provide reasonable results in most situations.
Often researchers will consider any models within 2 AIC units of the top model
<span class="math inline">\((\Delta\text{AIC}&lt;2)\)</span> as indistinguishable on
AICs and so either select the simplest model of the choices or report all the
models with similar “support”, allowing the reader to explore the suite of
similarly supported potential models . It is important to remember that if you
search across too many models, even with the AIC to support your model
comparisons, you might find a spuriously top model. Individual results that are
found by exploring many tests or models have higher chances to be <strong><em>spurious</em></strong>
and results found in this manner are difficult to <strong><em>reproduce</em></strong> when
someone repeats a similar study<a href="#fn117" class="footnote-ref" id="fnref117"><sup>117</sup></a>.
For these reasons, there is a set of general recommendations that have been
developed for using AICs:</p>
<ul>
<li><p>Consider a suite of models (often pre-specified and based on prior research in
the area of interest) and find the models with the top (in other words,
smallest) AIC results.</p>
<ul>
<li>The suite of candidate models need to contain at least some good models.
Selecting the best of a set of BAD models only puts you at the top of
$%#%-mountain, which is not necessarily a good thing.</li>
</ul></li>
<li><p>Report a table with the models considered, sorted from smallest to largest
AICs (<span class="math inline">\(\Delta\text{AICs}\)</span> from smaller to larger) that includes a count of
number of parameters estimated<a href="#fn118" class="footnote-ref" id="fnref118"><sup>118</sup></a>, the
AICs, and <span class="math inline">\(\Delta\text{AICs}\)</span>.</p>
<ul>
<li>Remember to incorporate the mean-only model in the model selection results. This allows you to compare the top model to one that does not contain any predictors.</li>
</ul></li>
<li><p>Interpret the top model or top models if a few are close on the AIC-scale to
the top model.</p></li>
<li><p><strong>DO NOT REPORT P-VALUES OR CALL TERMS “SIGNIFICANT” when models were selected
using AICs.</strong> </p>
<ul>
<li>Hypothesis testing and AIC model selection are not compatible philosophies
and testing in models selected by AICs invalidates the tests as they have
inflated Type I error rates.


The AIC results are your “evidence” – you don’t need anything else. If you wanted to report p-values, use them to select your model.</li>
</ul></li>
<li><p>You can describe variables as “important” or “useful” and report confidence
intervals to aid in interpretation of the terms in the selected model(s)
but need to avoid performing hypothesis tests with the confidence intervals.</p></li>
<li><p>Remember that the selected model is not the “true” model – it is only the
best model <em>according to AIC</em> among the set of models <em>you provided</em>.</p></li>
<li><p>Model assumptions need to be met to use AICs. They assume that the model is
specified correctly up to possibly comparing different predictor variables. Perform diagnostic checks on your initial model and the top model. </p></li>
</ul>
<p>When working with AICs, there are two options. Fit the models of interest and
then run the <code>AIC</code> function on each model. This can be tedious, especially
when we have many possible models to consider. We can make
it easy to fit all the potential candidate models that are implied by a
complicated starting model by using the <code>dredge</code> function from the <code>MuMIn</code>
package <span class="citation">(Barton <a href="#ref-R-MuMIn" role="doc-biblioref">2019</a>)</span>.

The name (dredge) actually speaks to what <strong><em>fitting
all possible models</em></strong> really engages – what is called <strong><em>data dredging</em></strong>.
The term is meant to
refer to considering way too many models for your data set, probably finding
something good from the process, but maybe identifying something spurious since
you looked at so many models. Note that if you take a hypothesis testing
approach where you plan to remove any terms with large p-values in this same
situation, you are really considering all possible models as well because you
could have removed some or all model components.

Methods that consider all
possible models are probably best used in exploratory analyses where you do not
know if any or all terms should be important. If you have more specific
research questions, then you probably should try to focus on comparisons of models
that help you directly answer those questions, either with AIC or p-value methods.</p>
<p>The <code>dredge</code> function provides an automated method of assessing all possible
simpler models based on an initial (full) model. It generates a table of AIC
results, <span class="math inline">\(\Delta\text{AICs}\)</span>, and also shows when various predictors are in or
out of the model for all reduced models possible from an initial model. For
quantitative predictors, the estimated slope is reported when that predictor is
in the model. For categorical variables and interactions with them, it just
puts a “+” in the table to let you know that the term is in the models. Note
that you must run the <code>options(na.action = "na.fail")</code> code to get <code>dredge</code>
to work.</p>
<p>To explore the AICs and compare their results to the adjusted <strong><em>R</em></strong><sup>2</sup>
that we used before for model selection, we can revisit the <em>Snow Depth</em> data set
with related results found in Section <a href="chapter8.html#section8-4">8.4</a> and
Table <a href="chapter8.html#tab:Table8-1">8.1</a>. In that situation we were considering
a “full” model that included <em>Elevation</em>, <em>Min.Temp</em>, and <em>Max.Temp</em> as potential
predictor variables after removing two influential points.   And we considered all
possible reduced models from that “full”<a href="#fn119" class="footnote-ref" id="fnref119"><sup>119</sup></a> model. Note
that the <code>dredge</code> output adds one more model that adjusted <strong><em>R</em></strong><sup>2</sup>
can’t consider – the mean-only model that contains no predictor variables. In
the following output it is the last model in the output (worst ranked on AIC).
Including the mean-only model in these results helps us “prove” that there is
support for having something in the model, but only if there is better support for
other models than this simplest possible model.</p>
<p>In reading <code>dredge</code> output<a href="#fn120" class="footnote-ref" id="fnref120"><sup>120</sup></a> as it is constructed here, the models are sorted by
top to bottom AIC values (smallest AIC to largest). The column <code>delta</code> is for
the <span class="math inline">\(\Delta\text{AICs}\)</span> and shows a 0 for the first row, which is the
top-ranked AIC model. Here it is for the model with <em>Elevation</em> and <em>Max.Temp</em> but
not including <em>Min.Temp</em>. This was also the top ranked model from adjusted
<strong><em>R</em></strong><sup>2</sup>, which is reproduced in the <code>adjRsq</code> column. The <code>AIC</code>
is calculated using the previous formula based on the <code>df</code> and <code>logLik</code>
columns. The <code>df</code> is also a useful column for comparing models as it helps
you see how complex each model is. For example, the top model used up 4
<em>model df</em> (three <span class="math inline">\(\beta\text{&#39;s}\)</span> and the residual error variance) and the
most complex model that included four predictor variables used up 5 <em>model df</em>.</p>

<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MuMIn)
<span class="kw">options</span>(<span class="dt">na.action =</span> <span class="st">&quot;na.fail&quot;</span>) <span class="co">#Must run this code once to use dredge</span>
snotel2R&lt;-snotel2[<span class="op">-</span><span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">22</span>),]; m6 &lt;-<span class="st"> </span><span class="kw">lm</span>(Snow.Depth<span class="op">~</span>Elevation<span class="op">+</span>Min.Temp<span class="op">+</span>Max.Temp, <span class="dt">data=</span>snotel2R)
<span class="kw">dredge</span>(m6, <span class="dt">rank=</span><span class="st">&quot;AIC&quot;</span>,
       <span class="dt">extra =</span> <span class="kw">c</span>(<span class="st">&quot;R^2&quot;</span>, <span class="dt">adjRsq=</span><span class="cf">function</span>(x) <span class="kw">summary</span>(x)<span class="op">$</span>adj.r.squared))</code></pre>
<pre><code>## Global model call: lm(formula = Snow.Depth ~ Elevation + Min.Temp + Max.Temp, data = snotel2R)
## ---
## Model selection table 
##     (Int)     Elv Max.Tmp Min.Tmp    R^2 adjRsq df   logLik   AIC delta
## 4 -167.50 0.02408  1.2530         0.8495 0.8344  4  -80.855 169.7  0.00
## 8 -213.30 0.02686  1.2430  0.9843 0.8535 0.8304  5  -80.541 171.1  1.37
## 2  -80.41 0.01791                 0.8087 0.7996  3  -83.611 173.2  3.51
## 6 -130.70 0.02098          1.0660 0.8134 0.7948  4  -83.322 174.6  4.93
## 5  179.60                 -5.0090 0.6283 0.6106  3  -91.249 188.5 18.79
## 7  178.60         -0.2687 -4.6240 0.6308 0.5939  4  -91.170 190.3 20.63
## 3  119.50         -2.1800         0.4131 0.3852  3  -96.500 199.0 29.29
## 1   40.21                         0.0000 0.0000  2 -102.630 209.3 39.55
##   weight
## 4  0.568
## 8  0.286
## 2  0.098
## 6  0.048
## 5  0.000
## 7  0.000
## 3  0.000
## 1  0.000
## Models ranked by AIC(x)</code></pre>

<p>There are two models that are clearly favored over the others with
<span class="math inline">\(\Delta\text{AICs}\)</span> for the model with <em>Elevation</em> and <em>Max.Temp</em> of 0 and for
the model with all three predictors of 1.37. The <span class="math inline">\(\Delta\text{AIC}\)</span> for the
third ranked model (contains just <em>Elevation</em>) is 3.51 suggesting clear support
for the top model over this. The
difference between the second and third ranked models also provides relatively
strong support for the more complex model over the model with just <em>Elevation</em>.
And the mean-only model had a <span class="math inline">\(\Delta\text{AIC}\)</span> of nearly 40 – suggesting
extremely strong support for the top model versus using no predictors. So we
have pretty clear support for models that include the <em>Elevation</em> and <em>Max.Temp</em>
variables (in both top models) and some support for also including the
<em>Min.Temp</em>, but the top model did not require its inclusion.</p>
<p>We could add further explorations of the term-plots and confidence
intervals for
the slopes from the
top or, here, possibly top two models. We would not spend any time with
p-values since we already used the AIC to assess importance of the model
components and they are invalid if we model select prior to reporting them. We
can quickly compare the slopes for variables that are shared in the two models
since they are both quantitative variables using the output. It is interesting
that the <em>Elevation</em> and <em>Max.Temp</em> slopes change little with the inclusion of
<em>Min.Temp</em> in moving from the top to second ranked model (0.02408 to 0.0286 and
1.253 to 1.243).</p>
<p>This was an observational study and so we can’t consider causal
inferences here
as discussed previously. Generally, the use of AICs does not preclude making
causal statements but if you have randomized assignment of levels of an
explanatory variable, it is more philosophically consistent to use hypothesis
testing methods in that setting. If you went to the effort to impose the levels
of a treatment on the subjects, it also makes sense to see if the differences
created are beyond what you might expect by chance if the treatment didn’t
matter.</p>
</div>
<div id="section8-14" class="section level2">
<h2><span class="header-section-number">8.14</span> Case study: Forced expiratory volume model selection using AICs</h2>
<p>Researchers were interested in studying the effects of smoking by children on
their lung development by measuring the forced expiratory volume (<em>FEV</em>,
measured in Liters) in a representative sample of children (<span class="math inline">\(n=654\)</span>) between
the ages of 3 and 19; this data set is available in the <code>FEV</code> data set in the
<code>coneproj</code> package (<span class="citation">Meyer and Liao (<a href="#ref-R-coneproj" role="doc-biblioref">2018</a>)</span>, <span class="citation">Liao and Meyer (<a href="#ref-Liao2014" role="doc-biblioref">2014</a>)</span>).

Measurements on the <em>age</em> (in years)
and <em>height</em> (in inches) as well as the <em>sex</em> and <em>smoking status</em> of the
children were made. We would expect both the <em>age</em> and <em>height</em> to have positive
relationships with <em>FEV</em> (lung capacity) and that smoking might decrease the lung
capacity but also that older children would be more likely to smoke. So the
<em>height</em> and <em>age</em> might be <strong><em>confounded</em></strong> with smoking status and smoking
might diminish lung development for older kids – resulting in a
potential interaction between <em>age</em> and <em>smoking</em>. The <em>sex</em> of the child might
also matter and should be considered or at
least controlled for since the response is a size-based measure. This creates
the potential for including up to four variables (<em>age</em>, <em>height</em>, <em>sex</em>, and
<em>smoking status</em>) and possibly the interaction between <em>age</em> and <em>smoking status</em>.
Initial explorations suggested that modeling the log-FEV would be more successful
than trying to model the responses on the original scale. Figure
<a href="chapter8.html#fig:Figure8-34">8.35</a> shows the suggestion of different slopes for the
smokers than non-smokers and that there aren’t very many smokers under 9 years
old in the data set.</p>
<p>So we will start with a model that contains an <em>age</em> by <em>smoking</em> interaction
and include <em>height</em> and <em>sex</em> as additive terms. We are not sure if any of
these model components will be needed, so the simplest candidate model will be
to remove all the predictors and just have a mean-only model (<code>FEV~1</code>). In
between the mean-only and most complicated model are many different options
where we can drop the interaction or drop the additive terms or drop the terms
involved in the interaction if we don’t need the interaction.</p>

<div class="figure"><span id="fig:Figure8-34"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-34-1.png" alt="Scatterplot of log(FEV) vs Age by smoking status." width="960" />
<p class="caption">
Figure 8.35: Scatterplot of log(FEV) vs Age by smoking status.
</p>
</div>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(coneproj)
<span class="kw">data</span>(FEV)
FEV &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(FEV)
FEV<span class="op">$</span>sex &lt;-<span class="st"> </span><span class="kw">factor</span>(FEV<span class="op">$</span>sex) <span class="co">#Make sex a factor</span>
<span class="kw">levels</span>(FEV<span class="op">$</span>sex) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Female&quot;</span>,<span class="st">&quot;Male&quot;</span>)  <span class="co">#Make sex labels explicit</span>
FEV<span class="op">$</span>smoke &lt;-<span class="st"> </span><span class="kw">factor</span>(FEV<span class="op">$</span>smoke) <span class="co">#Make smoking status a factor</span>
<span class="kw">levels</span>(FEV<span class="op">$</span>smoke) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Nonsmoker&quot;</span>,<span class="st">&quot;Smoker&quot;</span>) <span class="co">#Make smoking status labels explicit</span>
<span class="kw">require</span>(car)
<span class="kw">scatterplot</span>(<span class="kw">log</span>(FEV)<span class="op">~</span>age<span class="op">|</span>smoke, <span class="dt">data=</span>FEV, <span class="dt">smooth=</span>F,
            <span class="dt">main=</span><span class="st">&quot;Plot of log(FEV) vs Age of children by smoking  status&quot;</span>,
            <span class="dt">legend=</span><span class="kw">list</span>(<span class="dt">coords=</span><span class="st">&quot;topleft&quot;</span>, <span class="dt">columns=</span><span class="dv">1</span>, <span class="dt">cex.leg=</span><span class="fl">0.75</span>))</code></pre>
<p>To get the needed results, start with the <strong><em>full model</em></strong> – the most
complicated model you want to consider. It is good to check assumptions before
considering reducing the model as they rarely get better in simpler models and
the <strong>AIC is only appropriate to use if the model assumptions are reasonably
well-met</strong>. As suggested above, our “fullish” model for the <em>log(FEV)</em> values is
specified as <code>log(FEV)~height+age*smoke+sex</code>.</p>
<p>(ref:fig8-35) Diagnostics for the log(FEV) model that includes height, sex,
and an interaction between age and smoking status (the full model).</p>
<pre class="sourceCode r"><code class="sourceCode r">fm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(FEV)<span class="op">~</span>height<span class="op">+</span>age<span class="op">*</span>smoke<span class="op">+</span>sex, <span class="dt">data=</span>FEV)
<span class="kw">summary</span>(fm1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = log(FEV) ~ height + age * smoke + sex, data = FEV)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.62926 -0.08783  0.01136  0.09658  0.40751 
## 
## Coefficients:
##                  Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)     -1.919494   0.080571 -23.824  &lt; 2e-16
## height           0.042066   0.001759  23.911  &lt; 2e-16
## age              0.025368   0.003642   6.966 8.03e-12
## smokeSmoker      0.107884   0.113646   0.949  0.34282
## sexMale          0.030871   0.011764   2.624  0.00889
## age:smokeSmoker -0.011666   0.008465  -1.378  0.16863
## 
## Residual standard error: 0.1454 on 648 degrees of freedom
## Multiple R-squared:  0.8112, Adjusted R-squared:  0.8097 
## F-statistic: 556.8 on 5 and 648 DF,  p-value: &lt; 2.2e-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">oma=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">0</span>))
<span class="kw">plot</span>(fm1, <span class="dt">sub.caption=</span><span class="st">&quot;Diagnostics for full FEV model&quot;</span>)</code></pre>
<div class="figure"><span id="fig:Figure8-35"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-35-1.png" alt="(ref:fig8-35)" width="960" />
<p class="caption">
Figure 8.36: (ref:fig8-35)
</p>
</div>
<p>The diagnostic plots suggest that there are a few outlying points (Figure <a href="chapter8.html#fig:Figure8-35">8.36</a>) but they
are not influential and there is no indication of violations of the normality and constant
variance assumptions. If we select a different
model(s), we would want to check its diagnostics and make sure that the results
do not look noticeably worse than these do.</p>
<p>The <code>AIC</code> function can be used to generate the AIC values for a
single or set
of candidate models. It will also provide the model degrees of freedom used for
each model. For example, suppose that the want to compare <code>fm1</code> to a model
without the interaction term in the model, called <code>fm1R</code>. You need to fit both
models and then apply the <code>AIC</code> function to them with commas between the model
names:</p>
<pre class="sourceCode r"><code class="sourceCode r">fm1R &lt;-<span class="st"> </span><span class="kw">lm</span>(<span class="kw">log</span>(FEV)<span class="op">~</span>height<span class="op">+</span>age<span class="op">+</span>smoke<span class="op">+</span>sex, <span class="dt">data=</span>FEV)
<span class="kw">AIC</span>(fm1, fm1R)</code></pre>
<pre><code>##      df       AIC
## fm1   7 -658.5178
## fm1R  6 -658.6037</code></pre>
<p>These results tells us that the <code>fm1R</code> model (the one without the interaction)
is better on the AIC by 0.09 AIC units. Note that this model does not “fit” as well
as the full model, it is just the top AIC model – the AIC results suggest that
it is slightly closer to the truth than the more complicated model. But this
provides only an assessment of the difference between including or excluding
the interaction between <em>age</em> and <em>smoking</em> in a model with two other predictors.
We are probably also interested in whether the other terms are
needed in the model. The full suite of results from dredge provide model
comparisons that help us to assess the presence/absence of each model component
including the interaction.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MuMIn)
<span class="kw">options</span>(<span class="dt">na.action =</span> <span class="st">&quot;na.fail&quot;</span>) <span class="co">#Must run this code once to use dredge</span>
<span class="kw">dredge</span>(fm1, <span class="dt">rank=</span><span class="st">&quot;AIC&quot;</span>, 
       <span class="dt">extra =</span> <span class="kw">c</span>(<span class="st">&quot;R^2&quot;</span>, <span class="dt">adjRsq=</span><span class="cf">function</span>(x) <span class="kw">summary</span>(x)<span class="op">$</span>adj.r.squared))</code></pre>
<pre><code>## Global model call: lm(formula = log(FEV) ~ height + age * smoke + sex, data = FEV)
## ---
## Model selection table 
##        (Int)     age     hgh sex smk age:smk     R^2  adjRsq df   logLik
## 16 -1.944000 0.02339 0.04280   +   +         0.81060 0.80950  6  335.302
## 32 -1.919000 0.02537 0.04207   +   +       + 0.81120 0.80970  7  336.259
## 8  -1.940000 0.02120 0.04299   +             0.80920 0.80830  5  332.865
## 12 -1.974000 0.02231 0.04371       +         0.80880 0.80790  5  332.163
## 28 -1.955000 0.02388 0.04315       +       + 0.80920 0.80800  6  332.802
## 4  -1.971000 0.01982 0.04399                 0.80710 0.80650  4  329.262
## 7  -2.265000         0.05185   +             0.79640 0.79580  4  311.594
## 3  -2.271000         0.05212                 0.79560 0.79530  3  310.322
## 15 -2.267000         0.05190   +   +         0.79640 0.79550  5  311.602
## 11 -2.277000         0.05222       +         0.79560 0.79500  4  310.378
## 30 -0.067780 0.09493           +   +       + 0.64460 0.64240  6  129.430
## 26 -0.026590 0.09596               +       + 0.62360 0.62190  5  110.667
## 14 -0.015820 0.08963           +   +         0.62110 0.61930  5  108.465
## 6   0.004991 0.08660           +             0.61750 0.61630  4  105.363
## 10  0.022940 0.09077               +         0.60120 0.60000  4   91.790
## 2   0.050600 0.08708                         0.59580 0.59520  3   87.342
## 13  0.822000                   +   +         0.09535 0.09257  4 -176.092
## 9   0.888400                       +         0.05975 0.05831  3 -188.712
## 5   0.857400                   +             0.02878 0.02729  3 -199.310
## 1   0.915400                                 0.00000 0.00000  2 -208.859
##       AIC   delta weight
## 16 -658.6    0.00  0.414
## 32 -658.5    0.09  0.397
## 8  -655.7    2.87  0.099
## 12 -654.3    4.28  0.049
## 28 -653.6    5.00  0.034
## 4  -650.5    8.08  0.007
## 7  -615.2   43.42  0.000
## 3  -614.6   43.96  0.000
## 15 -613.2   45.40  0.000
## 11 -612.8   45.85  0.000
## 30 -246.9  411.74  0.000
## 26 -211.3  447.27  0.000
## 14 -206.9  451.67  0.000
## 6  -202.7  455.88  0.000
## 10 -175.6  483.02  0.000
## 2  -168.7  489.92  0.000
## 13  360.2 1018.79  0.000
## 9   383.4 1042.03  0.000
## 5   404.6 1063.22  0.000
## 1   421.7 1080.32  0.000
## Models ranked by AIC(x)</code></pre>
<p>There is a lot of information in the output and some of the needed information
in the second set of rows, so we will try to point out some useful features to
consider. The left columns describe the models being estimated. For example, the
first row of results is for a model with an intercept (<code>Int</code>), <em>age</em> (<code>age</code>)
, <em>height</em> (<code>hgh</code>), <em>sex</em> (<code>sex</code>), and <em>smoking</em>(<code>smk</code>). For <em>sex</em> and
<em>smoking</em>, there are “<code>+</code>”s in the output row when they are included in that
model but no coefficient since they are categorical variables. There is no
interaction between <em>age</em> and <em>smoking</em> in the top ranked model. The top AIC
model has an <span class="math inline">\(\boldsymbol{R}^2=0.8106\)</span>, adjusted <strong><em>R</em></strong><sup>2</sup> of 0.8095,
<em>model df</em>=6 (from an intercept, four slopes, and the residual variance),
log-likelihood (<code>logLik</code>)=335.302, an AIC=-658.6 and <span class="math inline">\(\Delta\text{AIC}\)</span> of 0.00.
The next best model adds the interaction between <em>age</em> and <em>smoking</em>, resulting
in increases in the <strong><em>R</em></strong><sup>2</sup>, adjusted <strong><em>R</em></strong><sup>2</sup>, and
<em>model df</em>, but increasing the AIC by 0.09 units <span class="math inline">\((\Delta\text{AIC}=0.09)\)</span>.
This suggests that these two models are essentially equivalent on the AIC because
the difference is so small. The simpler model is a little bit better on AIC so
you could focus on it or on the slightly more complicated model – but you should
probably note that the evidence is equivocal for these two models.</p>
<p>The comparison to other potential models shows the strength of evidence in
support of all the other model components. The intercept-only model is again the
last in the list with the least support on AICs with a <span class="math inline">\(\Delta\text{AIC}\)</span>
of 1080.32, suggesting it is not worth considering in comparison with the top
model. <strong>Comparing the mean-only model to our favorite model on AICs is a bit
like the overall <span class="math inline">\(\boldsymbol{F}\)</span>-test we considered in Section</strong>
<a href="chapter8.html#section8-7">8.7</a> <strong>because it compares a model with no predictors to a
complicated model.</strong> Each model with just one predictor included is available
in the table as well, with the top single predictor model based on <em>height</em>
having a <span class="math inline">\(\Delta\text{AIC}\)</span> of 43.96. So we certainly need to
pursue something more complicated than SLR based on the AIC results. Closer to
the top model is the third-ranked model that includes <em>age</em>, <em>height</em>, and <em>sex</em>.
It has a <span class="math inline">\(\Delta\text{AIC}\)</span> of 2.87 so we would say that
these results present marginal support for the top two models over this model. It is the simplest model of the top three but not close enough to
be considered in detail.</p>
<p>This table also provides the opportunity to compare the model selection
results
from the adjusted <strong><em>R</em></strong><sup>2</sup> compared to the AIC. The AIC favors the model
without an interaction between <em>age</em> and <em>smoking</em> whereas the adjusted
<em>R</em><sup>2</sup> favors the most complicated model considered here
that included an <em>age</em> and <em>smoking</em> interaction. The AIC provides units that are
more interpretable than adjusted <em>R</em><sup>2</sup> even though the
scale for the AIC is a bit mysterious as <strong>distances from the unknown true
model</strong>.</p>
<p>The top AIC model (and possibly the other similar models) can then be
explored in more detail. You should not then focus on hypothesis testing in this
model<a href="#fn121" class="footnote-ref" id="fnref121"><sup>121</sup></a>. Confidence
intervals and term-plots are useful for describing the different
model components and making inferences for the estimated sizes of differences
in the population. These results should not be used for deciding if terms are
“significant” when the models are selected using measures like the AIC or
adjusted <em>R</em><sup>2</sup>. But you can discuss the estimated model components to go with how you arrived at having them in the model.</p>
<p>In this situation, the top model is estimated to be</p>
<p><span class="math display">\[\log(\widehat{\text{FEV}})_i = -1.94 + 0.043\cdot\text{Height}_i+ 0.0234\cdot\text{Age}_i
-0.046I_{\text{Smoker},i}+0.0293I_{\text{Male},i}\]</span></p>
<p>based on the estimated coefficients provided below. Using these results and the
term-plots (Figure <a href="chapter8.html#fig:Figure8-36">8.37</a>) we see that in this model there are positive slopes
for <em>Age</em> and <em>Height</em> on <em>log-FEV</em>, a negative coefficient for <em>smoking</em>
(<em>Smoker</em>), and a positive coefficient for <em>sex</em> (<em>Males</em>). We could go further with interpretations
such as for the <em>age</em> term: For a 1 year increase in <em>age</em>, we expect, on
average, a 0.0234 log-liter increase in <em>FEV</em>, after controlling for the
<em>height</em>, <em>smoking status</em>, and <em>sex</em> of the children. We can even interpret
this on the original scale since this was a <em>log(y)</em> response model using the same techniques as in Section
<a href="chapter7.html#section7-6">7.6</a>. If we exponentiate the slope coefficient of the quantitative
variable, <span class="math inline">\(\exp(0.0234)=1.0237\)</span>. This provides the interpretation on the original
<em>FEV</em> scale, for a 1 year increase in <em>age</em>, we expect 2.4% increase in the
median <em>FEV</em>, after controlling for the <em>height</em>, <em>smoking status</em>, and <em>sex</em> of
the children. <strong>The only difference from Section</strong> <a href="chapter7.html#section7-6">7.6</a> <strong>when
working with a log(y) model now is that we have to note that the model used to
generate the slope coefficient had other components and so this estimate is
after adjusting for them.</strong></p>
<p>(ref:fig8-36) Term-plots for the top AIC model for log(FEV) that includes
height, age, smoking status and sex in the model.</p>
<pre class="sourceCode r"><code class="sourceCode r">fm1R<span class="op">$</span>coefficients</code></pre>
<pre><code>## (Intercept)      height         age smokeSmoker     sexMale 
## -1.94399818  0.04279579  0.02338721 -0.04606754  0.02931936</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">confint</span>(fm1R)</code></pre>
<pre><code>##                    2.5 %       97.5 %
## (Intercept) -2.098414941 -1.789581413
## height       0.039498923  0.046092655
## age          0.016812109  0.029962319
## smokeSmoker -0.087127344 -0.005007728
## sexMale      0.006308481  0.052330236</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(<span class="kw">allEffects</span>(fm1R))</code></pre>
<div class="figure"><span id="fig:Figure8-36"></span>
<img src="08-multipleLinearRegression_files/figure-html/Figure8-36-1.png" alt="(ref:fig8-36)" width="672" />
<p class="caption">
Figure 8.37: (ref:fig8-36)
</p>
</div>
<p>Like any statistical method, the AIC works better with larger sample sizes and
when the assumptions are met. It also will detect important variables in models
more easily when the effects of the predictor variables are strong. Along with
the AIC results, it is good to report the coefficients for your top estimated
model(s), confidence intervals for the coefficients and/or term-plots, and
<em>R</em><sup>2</sup>. This provides a useful summary of the reasons for selecting
the model(s), information on the importance of the terms within the model, and
a measure of the variability explained by the model. The <em>R</em><sup>2</sup> is not used to
select the model, but after selection can be a nice summary of model quality.
For <code>fm1R</code> , the <span class="math inline">\(R^2=0.8106\)</span> suggesting that the selected model explains
81% of the variation in log-<em>FEV</em> values.</p>
<p>The AICs are a preferred modeling strategy in some areas such as Ecology. As
with this and many other
methods discussed in this book, it is sometimes as easy to find journal
articles with mistakes in using statistical methods as it is to find papers
doing it correctly. After completing this material, you have the potential to
have the knowledge and experience of two statistics classes and now are better
trained than some researchers that frequently use these methods. This set of tools
can be easily mis-applied. Try to make sure that you are thinking carefully
through your problem before jumping to the statistical results. Make a graph
first, think carefully about your study design and variables collected and what
your models of interest might be, what assumptions might be violated based on
the data collection story, and then start fitting models. Then check your
assumptions and only proceed on with any inference if those conditions are
reasonably well-met. The AIC provides an alternative method for selecting among
different potential models and they do not need to be nested (a requirement of
hypothesis testing methods used to sequentially simplify models). The automated
consideration of all possible models in the <code>dredge</code> function should not be
considered in all situations but can be useful in a preliminary model exploration
study where no clear knowledge exists about useful models to consider. Reporting the summary of AIC results beyond just reporting the top model(s) selected for focused exploration provides the evidence to support that selection.</p>
</div>
<div id="section8-15" class="section level2">
<h2><span class="header-section-number">8.15</span> Chapter summary</h2>
<p>This chapter explored the most complicated models we’re
going to explore. MLR models can incorporate features of SLR and ANOVAs. The
MLR’s used in this chapter highlight the flexibility of the linear modeling
framework to move from two-sample mean models to multi-predictor models with
interactions of categorical and quantitative variables. It is useful to use the
pertinent names for the simpler models, but at this point we could have called
everything we are doing <strong><em>fitting linear models</em></strong>. The power of the linear
model involves being able to add multiple predictor variables to the
model and handle categorical predictors using indicator variables.  All this
power comes with some responsibility in that you need to know what you are
trying to fit and how to interpret the results provided. We introduced each scenario
working from simple to the most complicated version of the models, trying to
motivate when you would encounter them, and the specific details of the
interpretations of each type of model. In Chapter <a href="chapter9.html#chapter9">9</a>, case studies
are used to review the different methods discussed with reminders of how to
identify and interpret the particular methods used.</p>
<p>When you have to make modeling decisions, you should remember the main
priorities in modeling. First, you need
to find a model that can address research questions of interest. Second, find a
model that is trustworthy and has assumptions that are reasonably well met.
Third, report the logic and evidence that was used to identify and support the
model. All too often, researchers present only a final model with little
information on how they arrived at it. You should be reporting the reasons for
decisions made and the evidence supporting them. For example, if you were
considering an interaction model and the interaction was dropped and an additive
model is re-fit and interpreted, the evidence related to the interaction test
should still be reported. Similarly, if a larger MLR is considered and some
variables are removed, the evidence (reason) for those removals should be
provided. Because of multicollinearity in models, you should never remove more
than one quantitative predictor at a time or else you could remove two variables
that are important but were “hiding” when both were included in the model.</p>

</div>
<div id="section8-16" class="section level2">
<h2><span class="header-section-number">8.16</span> Summary of important R code</h2>
<p>There is very little “new” R code in this chapter since all these methods were
either used in the ANOVA or SLR chapters. The models are more complicated but
are built off of methods from previous chapters. In this code, <code>y</code> is a
response variable, <code>x1</code>, <code>x2</code>, …, <code>xK</code> are quantitative
explanatory variables, <code>group</code> is a factor variable and the data are
in <code>DATASETNAME</code>.</p>
<ul>
<li><p><strong>scatterplot(<font color='red'>y</font>~<font color='red'>x1|group</font>,
data=<font color='red'>DATASETNAME</font>, smooth=F)</strong></p>
<ul>
<li><p>Requires the <code>car</code> package.</p></li>
<li><p>Provides a scatterplot with a regression line for each group.
</p></li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> <code>&lt;-</code> lm(<font color='red'>y</font>~
<font color='red'>x1+x2+…+xK</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Estimates an MLR model using least squares with <span class="math inline">\(K\)</span> quantitative
predictors.
</li>
</ul></li>
<li><p>**<font color='red'>MODELNAME</font> <code>&lt;-</code> lm(<font color='red'>y</font>~
<font color='red'>x1*group</font>, data=<font color='red'>DATASETNAME</font>)**</p>
<ul>
<li>Estimates an interaction model between a quantitative and categorical
variable, providing different slopes and intercepts for each group.</li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> <code>&lt;-</code> lm(<font color='red'>y</font>~
<font color='red'>x1+group</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Estimates an additive model with a quantitative and categorical
variable, providing different intercepts for each group.</li>
</ul></li>
<li><p><strong>summary(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides parameter estimates, overall <span class="math inline">\(F\)</span>-test, <strong><em>R</em></strong><sup>2</sup>,
and adjusted <strong><em>R</em></strong><sup>2</sup>.
</li>
</ul></li>
<li><p><strong>par(mfrow=c(2, 2)); plot(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides four regression diagnostic plots in one plot.</li>
</ul></li>
<li><p><strong>confint(<font color='red'>MODELNAME</font>, level=0.95)</strong></p>
<ul>
<li><p>Provides 95% confidence intervals for the regression model coefficients.</p></li>
<li><p>Change <code>level</code> if you want other confidence levels.
</p></li>
</ul></li>
<li><p><strong>plot(allEffects(<font color='red'>MODELNAME</font>))</strong></p>
<ul>
<li><p>Requires the <code>effects</code> package.</p></li>
<li><p>Provides a plot of the estimated regression lines with 95% confidence
interval for the mean.
</p></li>
</ul></li>
<li><p><strong>vif(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li><p>Requires the <code>car</code> package.</p></li>
<li><p>Provides VIFs for an MLR model. Only use in additive models - not meaningful for terms involved in interactions.
</p></li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>, se.fit=T)</strong></p>
<ul>
<li>Provides fitted values for all observed <span class="math inline">\(x\text{&#39;s}\)</span> with SEs for the
mean.
</li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>, newdata=tibble(<font color='red'>x1</font>
= <font color='red'>X1_NEW</font>, <font color='red'>x2</font> = <font color='red'>X2_NEW</font>,
<font color='red'>…</font>, <font color='red'>xK</font> = <font color='red'>XK_NEW</font>,
interval=“confidence”)</strong></p>
<ul>
<li>Provides fitted value for specific values of the quantitative predictors
with CI for the mean.</li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>, newdata=tibble(<font color='red'>x1</font>
= <font color='red'>X1_NEW</font>, <font color='red'>x2</font> = <font color='red'>X2_NEW</font>,
<font color='red'>…</font>, <font color='red'>xK</font> = <font color='red'>XK_NEW</font>,
interval=“prediction”)</strong></p>
<ul>
<li>Provides fitted value for specific values of the quantitative predictors
with PI for a new observation.</li>
</ul></li>
<li><p><strong>Anova(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li><p>Requires the <code>car</code> package.</p></li>
<li><p>Use to generate ANOVA tables and <span class="math inline">\(F\)</span>-tests useful when categorical
variables are included in either the additive or interaction models.
</p></li>
</ul></li>
<li><p><strong>AIC(<font color='red'>MODELNAME_1</font>, <font color='red'>MODELNAME_2</font>)</strong></p>
<ul>
<li>Use to get AIC results for two candidate models called <code>MODELNAME_1</code>
and <code>MODELNAME_2</code>.
</li>
</ul></li>
<li><p><strong>options(na.action = “na.fail”)<br />
dredge(<font color='red'>FULL_MODELNAME</font>, rank=“AIC”)</strong></p>
<ul>
<li><p>Requires the <code>MuMIn</code> package.</p></li>
<li><p>Provides AIC and delta AIC results for all possible simpler models given
a full model called <code>FULL_MODELNAME</code>.
</p></li>
</ul></li>
</ul>
</div>
<div id="section8-17" class="section level2">
<h2><span class="header-section-number">8.17</span> Practice problems</h2>
<p>The original research goal for the treadmill data set used for practice
problems in the last two chapters was to
replace the costly treadmill oxygen test with a cheap to find running time
measurement but there were actually quite a few variables measured while the
run time was found – maybe we can replace the treadmill test result with a
combined prediction built using a few variables using the MLR techniques. The
following code will get us re-started in this situation.</p>
<pre class="sourceCode r"><code class="sourceCode r">treadmill &lt;-<span class="st"> </span><span class="kw">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/treadmill.csv&quot;</span>)
tm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(TreadMillOx<span class="op">~</span>RunTime, <span class="dt">data=</span>treadmill)</code></pre>
<p>8.1. Fit the MLR that also includes the running pulse (<code>RunPulse</code>), the
resting pulse (<code>RestPulse</code>), body weight (<code>BodyWeight</code>), and Age (<code>Age</code>)
of the subjects using the following code. Report and interpret the <em>R</em><sup>2</sup> for
this model.</p>
<p>8.2. Compare <em>R</em><sup>2</sup> and the adjusted <em>R</em><sup>2</sup> to the results for the SLR model
that just had <code>RunTime</code> in the model. What do these results suggest?</p>
<p>8.3. Interpret the estimated <code>RunTime</code> slope coefficients from the SLR model
and this MLR model. Explain the differences in the estimates.</p>
<p>8.4. Find the VIFs for this model and discuss whether there is an issue with
multicollinearity noted in these results.</p>
<p>8.5. Report the value for the overall <span class="math inline">\(F\)</span>-test for the MLR model and interpret
the result.</p>
<p>8.6. Drop the variable with the largest p-value in the MLR model and re-fit
it. Compare the resulting <strong><em>R</em></strong><sup>2</sup> and adjusted <strong><em>R</em></strong><sup>2</sup>
values to the others found previously.</p>
<p>8.7. Use the <code>dredge</code> function as follows to consider some other potential
reduced models and report the top two models according to adjusted
<strong><em>R</em></strong><sup>2</sup> values. What model had the highest <strong><em>R</em></strong><sup>2</sup>?
Also discuss and compare the model selection results provided by the delta AICs
here.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MuMIn)
<span class="kw">options</span>(<span class="dt">na.action =</span> <span class="st">&quot;na.fail&quot;</span>) <span class="co">#Must run this code once to use dredge</span>
<span class="kw">dredge</span>(mlr1, <span class="dt">rank=</span><span class="st">&quot;AIC&quot;</span>, 
       <span class="dt">extra=</span><span class="kw">c</span>(<span class="st">&quot;R^2&quot;</span>, <span class="dt">adjRsq=</span><span class="cf">function</span>(x) <span class="kw">summary</span>(x)<span class="op">$</span>adj.r.squared))</code></pre>
<p>8.8. For one of the models, interpret the <em>Age</em> slope coefficient. Remember that
only male subjects between 38 and 57 participated in this study. Discuss how
this might have impacted the results found as compared to a more general
population that could have been sampled from.</p>
<p>8.9. The following code creates a new three-level variable grouping the ages
into low, middle, and high for those observed. The scatterplot lets you
explore whether the relationship between treadmill oxygen and run time might
differ across the age groups.</p>
<pre class="sourceCode r"><code class="sourceCode r">treadmill<span class="op">$</span>Ageb &lt;-<span class="st"> </span><span class="kw">cut</span>(treadmill<span class="op">$</span>Age, <span class="dt">breaks=</span><span class="kw">c</span>(<span class="dv">37</span>,<span class="fl">44.5</span>,<span class="fl">50.5</span>,<span class="dv">58</span>))
<span class="kw">summary</span>(treadmill<span class="op">$</span>Ageb)
<span class="kw">require</span>(car)
<span class="kw">scatterplot</span>(TreadMillOx<span class="op">~</span>RunTime<span class="op">|</span>Ageb, <span class="dt">data=</span>treadmill, <span class="dt">smooth=</span>F, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p>Based on the plot, do the lines look approximately parallel or not?</p>
<p>8.10. Fit the MLR that contains a <code>RunTime</code> by <code>Ageb</code> interaction – do not
include any other variables. Compare the <strong><em>R</em></strong><sup>2</sup> and adjusted
<strong><em>R</em></strong><sup>2</sup> results to previous models.</p>
<p>8.11. Find and report the results for the <span class="math inline">\(F\)</span>-test that assesses evidence
relative to the need for different slope coefficients.</p>
<p>8.12. Write out the overall estimated model. What level was R using as
baseline? Write out the simplified model for two of the age levels.</p>
<p>8.13. Fit the additive model with <code>RunTime</code> and predict the mean treadmill
oxygen values for subjects with run times of 11 minutes in each of the three
<code>Ageb</code> groups.</p>
<p>8.14. Find the <span class="math inline">\(F\)</span>-test results for the binned age variable in the additive
model. Report and interpret those results.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Akaike1974">
<p>Akaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” <em>IEEE Transactions on Automatic Control</em> 19: 716–23.</p>
</div>
<div id="ref-R-MuMIn">
<p>Barton, Kamil. 2019. <em>MuMIn: Multi-Model Inference</em>. <a href="https://CRAN.R-project.org/package=MuMIn">https://CRAN.R-project.org/package=MuMIn</a>.</p>
</div>
<div id="ref-Burnham2002">
<p>Burnham, Kenneth P., and David R. Anderson. 2002. <em>Model Selection and Multimodel Inference</em>. NY: Springer.</p>
</div>
<div id="ref-R-openintro">
<p>Diez, David M, Christopher D Barr, and Mine Cetinkaya-Rundel. 2017. <em>Openintro: Data Sets and Supplemental Functions from ’Openintro’ Textbooks</em>. <a href="https://CRAN.R-project.org/package=openintro">https://CRAN.R-project.org/package=openintro</a>.</p>
</div>
<div id="ref-Fox2003">
<p>Fox, John. 2003. “Effect Displays in R for Generalised Linear Models.” <em>Journal of Statistical Software</em> 8 (15): 1–27. <a href="http://www.jstatsoft.org/v08/i15/">http://www.jstatsoft.org/v08/i15/</a>.</p>
</div>
<div id="ref-R-heplots">
<p>Fox, John, and Michael Friendly. 2018. <em>Heplots: Visualizing Hypothesis Tests in Multivariate Linear Models</em>. <a href="https://CRAN.R-project.org/package=heplots">https://CRAN.R-project.org/package=heplots</a>.</p>
</div>
<div id="ref-R-carData">
<p>Fox, John, Sanford Weisberg, and Brad Price. 2018b. <em>CarData: Companion to Applied Regression Data Sets</em>. <a href="https://CRAN.R-project.org/package=carData">https://CRAN.R-project.org/package=carData</a>.</p>
</div>
<div id="ref-Liao2014">
<p>Liao, Xiyue, and Mary C. Meyer. 2014. “Coneproj: An R Package for the Primal or Dual Cone Projections with Routines for Constrained Regression.” <em>Journal of Statistical Software</em> 61 (12): 1–22. <a href="http://www.jstatsoft.org/v61/i12/">http://www.jstatsoft.org/v61/i12/</a>.</p>
</div>
<div id="ref-R-smdata">
<p>Merkle, Ed, and Michael Smithson. 2018. <em>Smdata: Data to Accompany Smithson &amp; Merkle, 2013</em>. <a href="https://CRAN.R-project.org/package=smdata">https://CRAN.R-project.org/package=smdata</a>.</p>
</div>
<div id="ref-R-coneproj">
<p>Meyer, Mary C., and Xiyue Liao. 2018. <em>Coneproj: Primal or Dual Cone Projections with Routines for Constrained Regression</em>. <a href="https://CRAN.R-project.org/package=coneproj">https://CRAN.R-project.org/package=coneproj</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="92">
<li id="fn92"><p>If you take advanced applied mathematics courses, you can learn
more about the algorithms being used by <code>lm</code>.
Everyone else only cares
about the algorithms when they don’t work – which is usually due to the
user’s inputs in these models not the algorithm itself.<a href="chapter8.html#fnref92" class="footnote-back">↩</a></p></li>
<li id="fn93"><p>Sometimes the <code>effects</code> plots
ignores the edge explanatory observations with the
default display. Always check the original variable summaries when considering
the range of observed values. The code modifications used to make these plots
are a little annoying and can be avoided if you remember to check the original
summary statistics or scatterplots for variable extrema.<a href="chapter8.html#fnref93" class="footnote-back">↩</a></p></li>
<li id="fn94"><p>We used this same notation in the fitting the additive Two-Way
ANOVA and this is also additive in terms of these variables. Interaction
models are discussed later in the chapter.<a href="chapter8.html#fnref94" class="footnote-back">↩</a></p></li>
<li id="fn95"><p>Imagine
showing up to a ski area expecting a 40 inch base and there only being
11 inches. I’m sure ski areas are always more accurate than this model in their
reporting of amounts of snow on the ground…<a href="chapter8.html#fnref95" class="footnote-back">↩</a></p></li>
<li id="fn96"><p>The site name is redacted to protect
the innocence of the reader. More information on this site, located in
Beaverhead County, is available at
<a href="http://www.wcc.nrcs.usda.gov/nwcc/site?sitenum=355&amp;state=mt" class="uri">http://www.wcc.nrcs.usda.gov/nwcc/site?sitenum=355&amp;state=mt</a>.<a href="chapter8.html#fnref96" class="footnote-back">↩</a></p></li>
<li id="fn97"><p>The <code>seq</code> function has syntax of
<code>seq(from=startingpoint, to=endingpoint, length.out=#ofvalues_between_start_and_end)</code> and the <code>rep</code> function has
syntax of <code>rep(numbertorepeat, #oftimes).</code><a href="chapter8.html#fnref97" class="footnote-back">↩</a></p></li>
<li id="fn98"><p>Also
see Section <a href="chapter8.html#section8-13">8.13</a> for another method of picking among different
models.<a href="chapter8.html#fnref98" class="footnote-back">↩</a></p></li>
<li id="fn99"><p>This section was inspired by a
similar section from <span class="citation">De Veaux, Velleman, and Bock (<a href="#ref-DeVeaux2011" role="doc-biblioref">2011</a>)</span>.<a href="chapter8.html#fnref99" class="footnote-back">↩</a></p></li>
<li id="fn100"><p>There are some social
science models where the model is fit with the mean subtracted from each
predictor so all have mean 0 and the precision of the y-intercept is
interesting. But even in these models, the test for the y-intercept being 0 is
rarely of interest.<a href="chapter8.html#fnref100" class="footnote-back">↩</a></p></li>
<li id="fn101"><p>Either
someone had a weighted GPA with bonus points, or more likely here, there
was a coding error in the data set since only one observation was over 4.0 in
the GPA data. Either way, we could remove it and note that our inferences for
HSGPA do not extend above 4.0.<a href="chapter8.html#fnref101" class="footnote-back">↩</a></p></li>
<li id="fn102"><p>When there
are just two predictors, the VIFs have to be the same since the
proportion of information shared is the same in both directions. With
more than two predictors, each variable can have a different VIF
value.<a href="chapter8.html#fnref102" class="footnote-back">↩</a></p></li>
<li id="fn103"><p>We
are actually making an educated guess about what these codes mean. Other
similar data sets used 1 for males but the documentation on these data is a bit
sparse. We proceed with a small potential that the conclusions regarding
differences in <code>sex</code> are in the wrong direction.<a href="chapter8.html#fnref103" class="footnote-back">↩</a></p></li>
<li id="fn104"><p>Some people also call them <strong><em>dummy variables</em></strong> to reflect that they are stand-ins for dealing with the categorical information.<a href="chapter8.html#fnref104" class="footnote-back">↩</a></p></li>
<li id="fn105"><p>That may not read
how I intended…<a href="chapter8.html#fnref105" class="footnote-back">↩</a></p></li>
<li id="fn106"><p>This is true for
additive uses of indicator variables. In Section <a href="chapter8.html#section8-11">8.11</a>, we
consider interactions between quantitative and categorical variables which has
the effect of changing slopes and intercepts. The simplification ideas to
produce estimated equations for each group are used there but we have to account
for changing slopes by group too.<a href="chapter8.html#fnref106" class="footnote-back">↩</a></p></li>
<li id="fn107"><p>When making the SATM term-plot, the
categorical variable is held at the most
frequently occurring value in the data set. If you drop <code>ci.style="lines"</code>
from the effect plot options, it is best to copy the figures as Bitmaps or
save them as an image or they may (for some reason) lose the shaded bands in some word processing programs.<a href="chapter8.html#fnref107" class="footnote-back">↩</a></p></li>
<li id="fn108"><p>Models like this with a categorical variable and quantitative variable are often called <em>ANCOVA</em> or <em>analysis of covariance</em> models but really are just versions of our linear models we’ve been using throughout this material.<a href="chapter8.html#fnref108" class="footnote-back">↩</a></p></li>
<li id="fn109"><p>Note that we employed some specific options in the <code>legend</code> option to get the legend to fit on this scatterplot better. Usually you can avoid this but the <code>coords</code> option defined a location and the <code>columns</code> option made it a two column legend.<a href="chapter8.html#fnref109" class="footnote-back">↩</a></p></li>
<li id="fn110"><p>Standardizing quantitative predictor variables is popular in social sciences, often where the response variable is also standardized. In those situations, they generate what are called “standardized slopes” (<a href="https://en.wikipedia.org/wiki/Standardized_coefficient" class="uri">https://en.wikipedia.org/wiki/Standardized_coefficient</a>) that estimate the change in SDs in the response for a 1 SD change in the explanatory variable.<a href="chapter8.html#fnref110" class="footnote-back">↩</a></p></li>
<li id="fn111"><p>There is a way to test for a difference in the two lines at a particular <span class="math inline">\(x\)</span> value but it is beyond the scope of this material and uses what are called “contrasts”.<a href="chapter8.html#fnref111" class="footnote-back">↩</a></p></li>
<li id="fn112"><p>This is an example of what is called “step down” testing for model
refinement which is a commonly used technique for arriving at a final model to
describe response variables. Note that each step in the process should be
reported, not just the final model that only has variables with small p-values
remaining in it.<a href="chapter8.html#fnref112" class="footnote-back">↩</a></p></li>
<li id="fn113"><p>We could also use the <code>anova</code> function to do this but using <code>Anova</code>
throughout this material provides the answers we want in the additive model and
it has no impact for the only test of interest in the interaction model since
the interaction is the last component in the model.<a href="chapter8.html#fnref113" class="footnote-back">↩</a></p></li>
<li id="fn114"><p>In most situations, it would be crazy
to assume that the true model for a
process has been obtained so we can never pick the “correct” model. In fact, we
won’t even know if we are picking a “good” model, but just the best from a set
of the candidate models. But we can study the general performance of methods
using simulations where we know the true model and the AIC has some useful
properties in identifying the correct model when it is in the candidate set of
models. No such similar theory exists for the adjusted <strong><em>R</em></strong><sup>2</sup>.<a href="chapter8.html#fnref114" class="footnote-back">↩</a></p></li>
<li id="fn115"><p>Most people now call this
Akaike’s (pronounced <strong>ah-kah-ee-kay</strong>) Information Criterion, but he used the
AIC nomenclature to mean An Information Criterion – he was not so vain as to
name the method after himself in the original paper that proposed it.<a href="chapter8.html#fnref115" class="footnote-back">↩</a></p></li>
<li id="fn116"><p>More details on these components of the methods will be
left for more advanced material - we will focus on an introduction to using the AIC measure here.<a href="chapter8.html#fnref116" class="footnote-back">↩</a></p></li>
<li id="fn117"><p>Reproducibility ideas are used in statistics
first by making data and code used available to others (like all the code and
data in this book) and second by trying to use methods that when others perform
similar studies they will find similar results (from Physics, think of the famous
cold-fusion experiments <a href="http://en.wikipedia.org/wiki/Cold_fusion" class="uri">http://en.wikipedia.org/wiki/Cold_fusion</a>.)<a href="chapter8.html#fnref117" class="footnote-back">↩</a></p></li>
<li id="fn118"><p>Although sometimes excluded, the count of
parameters should include counting the residual variance as a parameter.<a href="chapter8.html#fnref118" class="footnote-back">↩</a></p></li>
<li id="fn119"><p>We put quotes on “full” or sometimes
call it the “fullish” model because we could always add more to the model, like
interactions or other explanatory variables. So we rarely have a completely full model but
we do have our “most complicated that we are considering” model.<a href="chapter8.html#fnref119" class="footnote-back">↩</a></p></li>
<li id="fn120"><p>The options in <code>extra=...</code> are to get extra information displayed that you do not necessarily need. You can simply run <code>dredge(m6,rank="AIC")</code> to get just the AIC results.<a href="chapter8.html#fnref120" class="footnote-back">↩</a></p></li>
<li id="fn121"><p>Hypothesis testing so permeates the use of statistics that even after
using AICs many researchers are pressured to report p-values for model
components.

Some of this could be confusion caused when people first learned
these statistical methods because when we teach you statistics we show you how
to use various methods, one after another, and forget to mention that you should
not use <strong>every</strong> method we taught you in <strong>every</strong> analysis.<a href="chapter8.html#fnref121" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter7.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter9.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
