<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>A Second Semester Statistics Course with R</title>
  <meta name="description" content="A Second Semester Statistics Course with R">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="A Second Semester Statistics Course with R" />
  <meta property="og:type" content="book" />
  
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="A Second Semester Statistics Course with R" />
  
  
  

<meta name="author" content="Mark C Greenwood">



  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="chapter4.html">
<link rel="next" href="chapter6.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a><ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> Chapter summary</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a><ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Histograms, boxplots, and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Beanplots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Chapter summary</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a><ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell-means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for Prisoner Rating data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a><ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a><ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and table plots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a><ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient (Optional section)</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a><ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomizing inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a><ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a><ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">A Second Semester Statistics Course with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter5" class="section level1">
<h1><span class="header-section-number">Chapter 5</span> Chi-square tests</h1>
<div id="section5-1" class="section level2">
<h2><span class="header-section-number">5.1</span> Situation, contingency tables, and table plots</h2>
<p>In this chapter, the focus shifts briefly from analyzing quantitative response variables to methods for handling categorical response variables. This is important because in some situations it is not possible to measure the response variable quantitatively. For example, we will analyze the results from a clinical trial where the results for the subjects were measured as one of three categories: <em>no improvement</em>, <em>some improvement</em>, and <em>marked improvement</em>. While that type of response could be treated as numerical, coded possibly as 1, 2, and 3, it would be difficult to assume that the responses such as those follow a normal distribution since they are <strong><em>discrete</em></strong> (not continuous, measured at whole number values only) and, more importantly, the difference between <em>no improvement</em> and <em>some improvement</em> is not necessarily the same as the difference between <em>some</em> and <em>marked improvement</em>. If it is treated numerically, then the differences between levels are assumed to be the same unless a different coding scheme is used (say 1, 2, and 5). It is better to treat these types of responses as being in one of the three categories and use statistical methods that don’t make unreasonable and arbitrary assumptions about what the numerical coding might mean. The study being performed here involved subjects randomly assigned to either a treatment or a placebo (control) group and we want to address research questions similar to those considered in Chapters <a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a> – assessing differences among two or more groups. With quantitative responses, the differences in the distributions are parameterized via the means of the groups and we used 2-sample mean or ANOVA hypotheses and tests. With categorical responses, the focus is on the probabilities of getting responses in each category and whether they differ among the groups.</p>
<p>We start with some useful summary techniques, both numerical and graphical, applied to some examples of studies these methods can be used to analyze. Graphical techniques provide opportunities for assessing specific patterns in variables, relationships between variables, and for generally understanding the responses obtained. There are many different types of plots and each can enhance certain features of data. We will start with a “fun” display, called a <em>table plot</em>, to help us understand some aspects of the results from a double-blind randomized clinical trial investigating a treatment for rheumatoid arthritis that has the categorical response variable introduced previously. These data are available in the <code>Arthritis</code> data set available in the <code>vcd</code> package <span class="citation">(Meyer, Zeileis, and Hornik <a href="#ref-R-vcd">2017</a>)</span>. There were <span class="math inline">\(n=84\)</span> subjects, with some demographic information recorded along with the <code>Treatment</code> status (<em>Treated</em>, <em>Placebo</em>) and whether the patients’ arthritis symptoms <code>Improved</code> (with levels of <em>None</em>, <em>Some</em>, and <em>Marked</em>).</p>
<p>The <code>tableplot</code> function from the <code>tabplot</code> package <span class="citation">(Tennekes and de Jonge <a href="#ref-R-tabplot">2017</a>)</span> displays bars for each response in a row<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a> based on the category of responses or as a bar with the height corresponding the value of quantitative variables<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a>. It also plots a red cell if the observations were missing on a particular variable. The plot can be obtained simply as <code>tableplot(DATASETNAME)</code>. But when using <code>tableplot</code>, we may not want to display everything in the tibble and often just select some of the variables. We use <code>Treatment</code>, <code>Improved</code>, <code>Sex</code>, and <code>Age</code> in the <code>select=...</code> option with a <code>c()</code> and commas between the names of the variables we want to display as shown below. The first one in the list is also the one that the data are sorted based on.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(vcd)
<span class="kw">data</span>(Arthritis) <span class="co">#Double-blind clinical trial with treatment and control groups</span>
<span class="kw">require</span>(tibble)
Arthritis &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(Arthritis)
<span class="co">#Homogeneity example</span>
<span class="kw">require</span>(tabplot)
<span class="kw">tableplot</span>(Arthritis,<span class="dt">select=</span><span class="kw">c</span>(Treatment,Improved,Sex,Age))</code></pre></div>
<div class="figure"><span id="fig:Figure5-1"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-1-1.png" alt="Table plot of the arthritis data set." width="960" />
<p class="caption">
Figure 5.1: Table plot of the arthritis data set.
</p>
</div>
<p>The first thing we can gather from Figure <a href="chapter5.html#fig:Figure5-1">5.1</a> is that there are no red cells so there were no missing observations in the data set. Missing observations regularly arise in real studies when observations are not obtained for many different reasons and it is always good to check for missing data issues – this plot provides a quick visual method for doing that check. Primarily we are interested in whether the treatment led to a different pattern (or rates) of improvement responses. There seems to be more purple (<em>Marked</em>) improvement responses in the treatment group and more blue (<em>None</em>) responses in the placebo group. This sort of plot also helps us to simultaneously consider the role of other variables in the observed responses. You can see the sex of each subject in the vertical panel for <code>Sex</code> and it seems that there is a relatively balanced mix of males and females in the treatment/placebo groups. Quantitative variables are also displayed with horizontal bars corresponding to the responses (the x-axis provides the units of the responses, here in years). From the panel for <code>Age</code>, we can see that the ages of subjects ranged from the 20s to 70s and that there is no clear difference in the ages between the treated and placebo groups. If, for example, all the male subjects had ended up being randomized into the treatment group, then we might have worried about whether sex and treatment were confounded and whether any differences in the responses might be due to sex instead of the treatment. The random assignment of treatment/placebo to the subjects appears to have been successful here in generating a mix of ages and sexes among the two treatment groups<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a>. The main benefit of this sort of plot is the ability to visualize more than two categorical variables simultaneously. But now we want to focus more directly on the researchers’ main question – does the treatment lead to different improvement outcomes than the placebo?</p>
<p>To directly assess the effects of the treatment, we want to display just the two variables of interest. <strong><em>Stacked bar charts</em></strong> provide a method of displaying the response patterns (in <code>Improved</code>) across the levels of a predictor variable (<code>Treatment</code>) by displaying a bar for each predictor variable level and the proportions of responses in each category of the response in each of those groups. If the placebo is as effective as the treatment, then we would expect similar proportions of responses in each improvement category. A difference in the effectiveness would manifest in different proportions in the different improvement categories between <em>Treated</em> and <em>Placebo</em>. To get information in this direction, we start with obtaining the counts in each combination of categories using the <code>tally</code> function to generate contingency tables. <strong><em>Contingency tables</em></strong> with <strong><em>R</em></strong> rows and <strong><em>C</em></strong> columns (called <strong><em>R by C tables</em></strong>) summarize the counts of observations in each combination of the explanatory and response variables. In these data, there are <span class="math inline">\(R=2\)</span> rows and <span class="math inline">\(C=3\)</span> columns making a <span class="math inline">\(2\times 3\)</span> table – note that you do not count the row and column for the “Totals” in defining the size of the table. In the table, there seems to be many more <em>Marked</em> improvement responses (21 vs 7) and fewer <em>None</em> responses (13 vs 29) in the treated group compared to the placebo group.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(mosaic)
<span class="kw">tally</span>(<span class="op">~</span>Treatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthritis, <span class="dt">margins=</span>T)</code></pre></div>
<pre><code>##          Improved
## Treatment None Some Marked Total
##   Placebo   29    7      7    43
##   Treated   13    7     21    41
##   Total     42   14     28    84</code></pre>
<p>Using the <code>tally</code> function with <code>~x+y</code> provides a contingency table with the <code>x</code> variable on the rows and the <code>y</code> variable on the columns, with <code>margins=T</code> as an option so we can obtain the totals along the rows, columns, and table total of <span class="math inline">\(N=84\)</span>. In general, contingency tables contain the counts <span class="math inline">\(n_{rc}\)</span> in the <span class="math inline">\(r^{th}\)</span> row and <span class="math inline">\(c^{th}\)</span> column where <span class="math inline">\(r=1,\ldots,R\)</span> and <span class="math inline">\(c=1,\ldots,C\)</span>. We can also define the <strong><em>row totals</em></strong> as the sum across the columns of the counts in row <span class="math inline">\(r\)</span> as</p>
<p><span class="math display">\[\mathbf{n_{r\bullet}}=\Sigma^C_{c=1}n_{rc},\]</span></p>
<p>the <strong><em>column totals</em></strong> as the sum across the rows for the counts in column <span class="math inline">\(c\)</span> as</p>
<p><span class="math display">\[\mathbf{n_{\bullet c}}=\Sigma^R_{r=1}n_{rc},\]</span></p>
<p>and the <strong><em>table total</em></strong> as</p>
<p><span class="math display">\[\mathbf{N}=\Sigma^R_{r=1}\mathbf{n_{r\bullet}} = \Sigma^C_{c=1}\mathbf{n_{\bullet c}}
= \Sigma^R_{r=1}\Sigma^C_{c=1}\mathbf{n_{rc}}.\]</span></p>
<p>We’ll need these quantities to do some calculations in a bit. A generic contingency table with added row, column, and table totals just like the previous result from the <code>tally</code> function is provided in Table <a href="chapter5.html#tab:Table5-1">5.1</a>.</p>

<table>
<caption><span id="tab:Table5-1">Table 5.1: </span> General notation for counts in an <em>R</em> by <em>C</em> contingency table.</caption>
<colgroup>
<col width="7%" />
<col width="17%" />
<col width="17%" />
<col width="17%" />
<col width="5%" />
<col width="17%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center"><strong>Response<br />
Level 1</strong></th>
<th align="center"><strong>Response<br />
Level 2</strong></th>
<th align="center"><strong>Response<br />
Level 3</strong></th>
<th align="center"><br />
<strong>…</strong></th>
<th align="center"><strong>Response<br />
Level C</strong></th>
<th align="center"><br />
<strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Group 1</strong></td>
<td align="center"><span class="math inline">\(n_{11}\)</span></td>
<td align="center"><span class="math inline">\(n_{12}\)</span></td>
<td align="center"><span class="math inline">\(n_{13}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{1C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{1 \bullet}}\)</span></td>
</tr>
<tr class="even">
<td align="center"><strong>Group 2</strong></td>
<td align="center"><span class="math inline">\(n_{21}\)</span></td>
<td align="center"><span class="math inline">\(n_{22}\)</span></td>
<td align="center"><span class="math inline">\(n_{23}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{2C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{2 \bullet}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>…</strong></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center"><strong>…</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>Group R</strong></td>
<td align="center"><span class="math inline">\(n_{R1}\)</span></td>
<td align="center"><span class="math inline">\(n_{R2}\)</span></td>
<td align="center"><span class="math inline">\(n_{R3}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(n_{RC}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{R \bullet}}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Totals</strong></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 1}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 2}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 3}}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet C}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{N}\)</span></td>
</tr>
</tbody>
</table>
<p>Comparing counts from the contingency table is useful, but comparing proportions in each category is better, especially when the sample sizes in the levels of the explanatory variable differ. Switching the formula used in the <code>tally</code> function formula to <code>~ y|x</code> and adding the <code>format=&quot;proportion&quot;</code> option provides the proportions in the response categories conditional on the category of the predictor (these are called <strong><em>conditional proportions</em></strong> or the <strong><em>conditional distribution</em></strong> of, here, <em>Improved</em> on <em>Treatment</em>)<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a>. Note that they sum to 1.0 in each level of x, <em>placebo</em> or <em>treated</em>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tally</span>(<span class="op">~</span>Improved<span class="op">|</span>Treatment, <span class="dt">data=</span>Arthritis, <span class="dt">format=</span><span class="st">&quot;proportion&quot;</span>, <span class="dt">margins=</span>T)</code></pre></div>
<pre><code>##         Treatment
## Improved   Placebo   Treated
##   None   0.6744186 0.3170732
##   Some   0.1627907 0.1707317
##   Marked 0.1627907 0.5121951
##   Total  1.0000000 1.0000000</code></pre>
<p>This version of the <code>tally</code> result switches the variables between the rows and columns from the first summary of the data but the single “Total” row makes it clear to read the proportions down the columns in this version of the table. In this application, it shows how the proportions seem to be different among categories of <em>Improvement</em> between the placebo and treatment groups. This matches the previous thoughts on these data, but now a difference of marked improvement of 16% vs 51% is more clearly a big difference. We can also display this result using a <strong><em>stacked bar-chart</em></strong> that displays the same information using the <code>plot</code> function with a <code>y~x</code> formula:</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Improved<span class="op">~</span>Treatment, <span class="dt">data=</span>Arthritis,
     <span class="dt">main=</span><span class="st">&quot;Stacked Bar Chart of Arthritis Data&quot;</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-2"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-2-1.png" alt="Stacked bar chart of Arthritis data. The left bar is for the Placebo group and the right bar is for the Treated group. The width of the bars is based on relative size of each group and the portion of the total height of each shaded area is the proportion of that group in each category. The darkest shading is for “none”, medium shading for “some”, and the lightest shading for “marked”, as labeled on the y-axis." width="480" />
<p class="caption">
Figure 5.2: Stacked bar chart of Arthritis data. The left bar is for the Placebo group and the right bar is for the Treated group. The width of the bars is based on relative size of each group and the portion of the total height of each shaded area is the proportion of that group in each category. The darkest shading is for “none”, medium shading for “some”, and the lightest shading for “marked”, as labeled on the y-axis.
</p>
</div>
<p>The stacked bar-chart in Figure <a href="chapter5.html#fig:Figure5-2">5.2</a> displays the previous conditional proportions for the groups, with the same relatively clear difference between the groups persisting. If you run the <code>plot</code> function with variables that are coded numerically, it will make a very different looking graph (R is smart!) so again be careful that you are instructing R to treat your variables as categorical if they really are categorical. R is powerful but can’t read your mind!</p>
<p>In this chapter, we analyze data collected in two different fashions and modify the hypotheses to reflect the differences in the data collection processes, choosing either between what are called Homogeneity and Independence tests. The previous situation where levels of a treatment are randomly assigned to the subjects in a study describes the situation for what is called a <strong><em>Homogeneity Test</em></strong>. Homogeneity also applies when random samples are taken from each population of interest to generate the observations in each group of the explanatory variable based on the population groups. These sorts of situations resemble many of the examples from Chapter <a href="chapter3.html#chapter3">3</a> where treatments were assigned to subjects. The other situation considered is where a single sample is collected to represent a population and then a contingency table is formed based on responses on two categorical variables. When one sample is collected and analyzed using a contingency table, the appropriate analysis is called an <strong><em>Independence</em></strong> or <strong><em>Association test</em></strong>. In this situation, it is not necessary to have variables that are clearly classified as explanatory or response although it is certainly possible. Data that often align with Independence testing are collected using surveys of subjects randomly selected from a single, large population. An example, analyzed below, involves a survey of voters and whether their party affiliation is related to who they voted for – the republican, democrat, or other candidate. There is clearly an explanatory variable of the <em>Party affiliation</em> but a single large sample was taken from the population of all likely voters so the Independence test needs to be applied. Another example where Independence is appropriate involves a study of student cheating behavior. Again, a single sample was taken from the population of students at a university and this determines that it will be an Independence test. Students responded to questions about lying to get out of turning in a paper and/or taking an exam (<em>none</em>, <em>either</em>, or <em>both</em>) and copying on an exam and/or turning in a paper written by someone else (<em>neither</em>, <em>either</em>, or <em>both</em>). In this situation, it is not clear which variable is response or explanatory (which should explain the other) and it does not matter with the Independence testing framework. Figure <a href="chapter5.html#fig:Figure5-3">5.3</a> contains a diagram of the data collection processes and can help you to identify the appropriate analysis situation.</p>

<div class="figure"><span id="fig:Figure5-3"></span>
<img src="chapter5_files/image027.png" alt="Diagram of the scenarios involved in Homogeneity and Independence tests. Homogeneity testing involves R random samples or subjects assigned to R groups. Independence testing involves a single random sample and measurements on two categorical variables." width="479" />
<p class="caption">
Figure 5.3: Diagram of the scenarios involved in Homogeneity and Independence tests. Homogeneity testing involves R random samples or subjects assigned to R groups. Independence testing involves a single random sample and measurements on two categorical variables.
</p>
</div>
<p>You will discover that the test statistics are the same for both methods, which can create some desire to assume that the differences in the data collection doesn’t matter. In Homogeneity designs, the sample size in each group <span class="math inline">\((\mathbf{n_{1\bullet}},\mathbf{n_{2\bullet},\ldots,\mathbf{n_{R\bullet}}})\)</span> is fixed. In Independence situations, the total sample size <span class="math inline">\(\mathbf{N}\)</span> is fixed but all the <span class="math inline">\(\mathbf{n_{r\bullet}}\text{&#39;s}\)</span> are random. These differences impact the graphs, hypotheses, and conclusions used even though the test statistics and p-values are calculated the same way – so we only need to learn one test statistic to handle the two situations, but we need to make sure we know which we’re doing!</p>
</div>
<div id="section5-2" class="section level2">
<h2><span class="header-section-number">5.2</span> Homogeneity test hypotheses</h2>
<p>If we define some additional notation, we can then define hypotheses that allow us to assess evidence related to whether the treatment “matters” in Homogeneity situations. This situation is similar to what we did in the One-Way ANOVA situation with quantitative responses in Chapter <a href="chapter3.html#chapter3">3</a> but the parameters now relate to proportions in the response variable categories across the groups. First we can define the conditional population proportions in level <span class="math inline">\(c\)</span> (column <span class="math inline">\(c=1,\ldots,C\)</span>) of group <span class="math inline">\(r\)</span> (row <span class="math inline">\(r=1,\ldots,R\)</span>) as <span class="math inline">\(p_{rc}\)</span>. Table <a href="chapter5.html#tab:Table5-2">5.2</a> shows the proportions, noting that the proportions in each row sum to 1 since they are conditional on the group of interest. A <strong><em>transposed</em></strong> (rows and columns flipped) version of this table is produced by the <code>tally</code> function if you use the formula <code>~y|x</code>.</p>

<table>
<caption><span id="tab:Table5-2">Table 5.2: </span> Table of conditional proportions in the Homogeneity testing scenario.</caption>
<colgroup>
<col width="8%" />
<col width="18%" />
<col width="18%" />
<col width="18%" />
<col width="5%" />
<col width="18%" />
<col width="11%" />
</colgroup>
<thead>
<tr class="header">
<th align="center"> </th>
<th align="center"><strong>Response<br />
Level 1</strong></th>
<th align="center"><strong>Response<br />
Level 2</strong></th>
<th align="center"><strong>Response<br />
Level 3</strong></th>
<th align="center"><br />
<strong>…</strong></th>
<th align="center"><strong>Response<br />
Level C</strong></th>
<th align="center"><br />
<strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center"><strong>Group 1</strong></td>
<td align="center"><span class="math inline">\(p_{11}\)</span></td>
<td align="center"><span class="math inline">\(p_{12}\)</span></td>
<td align="center"><span class="math inline">\(p_{13}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{1C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="even">
<td align="center"><strong>Group 2</strong></td>
<td align="center"><span class="math inline">\(p_{21}\)</span></td>
<td align="center"><span class="math inline">\(p_{22}\)</span></td>
<td align="center"><span class="math inline">\(p_{23}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{2C}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>…</strong></td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center">…</td>
<td align="center"><strong>…</strong></td>
</tr>
<tr class="even">
<td align="center"><strong>Group R</strong></td>
<td align="center"><span class="math inline">\(p_{R1}\)</span></td>
<td align="center"><span class="math inline">\(p_{R2}\)</span></td>
<td align="center"><span class="math inline">\(p_{R3}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(p_{RC}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
<tr class="odd">
<td align="center"><strong>Totals</strong></td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet 1}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{n_{\bullet 2}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet 3}}\)</span></td>
<td align="center">…</td>
<td align="center"><span class="math inline">\(\boldsymbol{p_{\bullet C}}\)</span></td>
<td align="center"><span class="math inline">\(\boldsymbol{1.0}\)</span></td>
</tr>
</tbody>
</table>
<p>In the Homogeneity situation, the null hypothesis is that the distributions are the same in all the <span class="math inline">\(R\)</span> populations. This means that the null hypothesis is:</p>

<p><span class="math display">\[\begin{array}{rl}
\mathbf{H_0:}\  &amp; \mathbf{p_{11}=p_{21}=\ldots=p_{R1}} \textbf{ and } \mathbf{p_{12}=p_{22}=\ldots=p_{R2}}  \textbf{ and } \mathbf{p_{13}=p_{23}=\ldots=p_{R3}} \\ 
&amp; \textbf{ and } \mathbf{\ldots} \textbf{ and }\mathbf{p_{1C}=p_{2C}=\ldots=p_{RC}}. \\
\end{array}\]</span></p>
<p>If all the groups are the same, then they all have the same conditional proportions and we can more simply write the null hypothesis as:</p>
<p><span class="math display">\[\mathbf{H_0:(p_{r1},p_{r2},\ldots,p_{rC})=(p_1,p_2,\ldots,p_C)} \textbf{ for all } \mathbf{r}.\]</span></p>
<p>In other words, the pattern of proportions across the columns are <strong>the same for all the</strong> <span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>. The alternative is that there is some difference in the proportions of at least one response category for at least one group. In slightly more gentle and easier to reproduce words, equivalently, we can say:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_0:}\)</span> <strong>The population distributions of the responses for variable</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>are the same across the</strong> <span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>.</li>
</ul>
<p>The alternative hypothesis is then:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_A:}\)</span> <strong>The population distributions of the responses for variable</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>are NOT ALL the same across the</strong> <span class="math inline">\(\mathbf{R}\)</span> <strong>groups</strong>.</li>
</ul>
<p>To make this concrete, consider what the proportions could look like if they satisfied the null hypothesis for the <em>Arthritis</em> example, as displayed in Figure <a href="chapter5.html#fig:Figure5-4">5.4</a>.</p>

<div class="figure"><span id="fig:Figure5-4"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-4-1.png" alt="Plot of one way that the Arthritis proportions could have been if the null hypothesis had been true." width="480" />
<p class="caption">
Figure 5.4: Plot of one way that the Arthritis proportions could have been if the null hypothesis had been true.
</p>
</div>
<p>Note that the proportions in the different response categories do not need to be the same just that the distribution needs to be the same across the groups. The null hypothesis does <em>not</em> require that all three response categories (<em>none</em>, <em>some</em>, <em>marked</em>) be equally likely. It assumes that whatever the distribution of proportions is across these three levels that there is no difference in that distribution between the explanatory variable (here treated/placebo) groups. Figure <a href="chapter5.html#fig:Figure5-4">5.4</a> shows an example of a situation where the null hypothesis is true and the distributions of responses across the groups look the same but the proportions for <em>none</em>, <em>some</em> and <em>marked</em> are not all equally likely. That situation satisfies the null hypothesis. Compare this plot to the one for the real data set in Figure <a href="chapter5.html#fig:Figure5-2">5.2</a>. It looks like there might be some differences in the responses between the treated and placebo groups as that plot looks much different from this one, but we will need a test statistic and a p-value to fully address the evidence relative to the previous null hypothesis.</p>
</div>
<div id="section5-3" class="section level2">
<h2><span class="header-section-number">5.3</span> Independence test hypotheses</h2>
<p>When we take a single random sample of size <span class="math inline">\(N\)</span> and make a contingency table, our inferences relate to whether there is a relationship or <strong><em>association</em></strong> (that they are not independent) between the variables. This is related to whether the distributions of proportions match across rows in the table but is a more general question since we do not need to determine a variable to condition on, one that takes on the role of an explanatory variable, from the two variables of interest. In general, the hypotheses for an Independence test for variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are:</p>
<ul>
<li><p><span class="math inline">\(\mathbf{H_0}\)</span>: <strong>There is no relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>in the population.</strong></p>
<ul>
<li>Or: <span class="math inline">\(H_0\)</span>: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are independent in the population.</li>
</ul></li>
<li><p><span class="math inline">\(\mathbf{H_A}\)</span>: <strong>There is a relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span> <strong>in the population.</strong></p>
<ul>
<li>Or: <span class="math inline">\(H_A\)</span>: <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are dependent in the population.</li>
</ul></li>
</ul>
<p>To illustrate a test of independence, consider an example involving data from a national random sample taken prior to the 2000 US elections from the data set <code>election</code> from the package <code>poLCA</code> (<span class="citation">Linzer and Lewis. (<a href="#ref-R-poLCA">2014</a>)</span>, <span class="citation">Linzer and Lewis (<a href="#ref-Linzer2011">2011</a>)</span>). Each respondent’s democratic-republican partisan identification was collected, provided in the <code>PARTY</code> variable for measurements on a seven-point scale from (1) <em>Strong Democrat</em>, (2) <em>Weak Democrat</em>, (3) <em>Independent-Democrat</em>, (4) <em>Independent-Independent</em>, (5) <em>Independent-Republican</em>, (6) <em>Weak Republican</em>, to (7) <em>Strong Republican</em>. The <code>VOTEF</code> variable that is created below will contain the candidate that the participants voted for (the data set was originally coded with 1, 2, and 3 for the candidates and we replaced those levels with the candidate names). The contingency table shows some expected results, that individuals with strong party affiliations tend to vote for the party nominee with strong support for Gore in the democrats (<code>PARTY</code> = 1 and 2) and strong support for Bush in the republicans (<code>PARTY</code> = 6 and 7). As always, we want to support our explorations with statistical inferences, here with the potential to extend inferences to the overall population of voters. The inferences in an independence test are related to whether there is a relationship between the two variables in the population. A <strong><em>relationship</em></strong> between variables occurs when knowing the level of one variable for a person, say that they voted for Gore, informs the types of responses that you would expect for that person, here that they are likely affiliated with the Democratic Party. When there is no relationship (the null hypothesis here), knowing the level of one variable is not informative about the level of the other variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(poLCA)
<span class="co"># 2000 Survey - use package=&quot;&quot; because other data sets in R have same name</span>
<span class="kw">data</span>(election, <span class="dt">package=</span><span class="st">&quot;poLCA&quot;</span>) 
election &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(election)
<span class="co"># Subset variables and remove missing values</span>
election2 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(election[,<span class="kw">c</span>(<span class="st">&quot;PARTY&quot;</span>,<span class="st">&quot;VOTE3&quot;</span>)])
election2<span class="op">$</span>VOTEF &lt;-<span class="st"> </span><span class="kw">factor</span>(election2<span class="op">$</span>VOTE3)
<span class="kw">levels</span>(election2<span class="op">$</span>VOTEF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Gore&quot;</span>,<span class="st">&quot;Bush&quot;</span>,<span class="st">&quot;Other&quot;</span>)
<span class="kw">levels</span>(election2<span class="op">$</span>VOTEF)</code></pre></div>
<pre><code>## [1] &quot;Gore&quot;  &quot;Bush&quot;  &quot;Other&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">electable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PARTY<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2)
electable</code></pre></div>
<pre><code>##      VOTEF
## PARTY Gore Bush Other
##     1  238    6     2
##     2  151   18     1
##     3  113   31    13
##     4   37   37    11
##     5   21  124    12
##     6   20  121     2
##     7    3  189     1</code></pre>
<p>The hypotheses for an Independence/Association Test here are:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no relationship between party affiliation and voting status in the population.</p>
<ul>
<li>Or: <span class="math inline">\(H_0\)</span>: Party affiliation and voting status are independent in the population.</li>
</ul></li>
<li><p><span class="math inline">\(H_A\)</span>: There is a relationship between party affiliation and voting status in the population.</p>
<ul>
<li>Or: <span class="math inline">\(H_A\)</span>: Party affiliation and voting status are dependent in the population.</li>
</ul></li>
</ul>
<p>You could also write these hypotheses with the variables switched and that is also perfectly acceptable. Because these hypotheses are ambivalent about the choice of a variable as an “x” or a “y”, the summaries of results should be consistent with that idea. We should not calculate conditional proportions or make stacked bar charts since they imply a directional relationship from x to y (or results for y conditional on the levels of x) that might be hard to justify. Our summaries in these situations are the contingency table (<code>tally(~var1+var2, data=DATASETNAME)</code>) and a new graph called a <strong><em>mosaic plot</em></strong> (using the <code>mosaicplot</code> function).</p>
<p>Mosaic plots display a box for each cell count whose area corresponds to the proportion of the total data set that is in that cell <span class="math inline">\((n_{rc}/\mathbf{N})\)</span>. In some cases, the bars can be short or narrow if proportions of the total are small and the labels can be hard to read but the same bars or a single line exist for each category of the variables in all rows and columns. The mosaic plot makes it easy to identify the most common combination of categories. For example, in Figure <a href="chapter5.html#fig:Figure5-5">5.5</a> the <em>Gore</em> and <code>PARTY</code> = 1 (<em>Strong Democrat</em>) box in the top segment under column 1 of the plot has the largest area so is the highest proportion of the total. Similarly, the middle segment on the right for the <code>PARTY</code> category 7s corresponds to the <em>Bush</em> voters who were a 7 (<em>Strong Republican</em>). Knowing that the middle box in each column is for Bush voters is a little difficult as “Other” and “Bush” overlap each other in the y-axis labeling but it is easy enough to sort out the story here if we have briefly explored the contingency table. We can also get information about the variable used to make the columns as the width of the columns is proportional to the number of subjects in each <code>PARTY</code> category in this plot. There were relatively few 4s (<em>Independent-Independent</em> responses) in total in the data set. Also, the <em>Other</em> category was the highest proportion of any vote-getter in the <code>PARTY</code> = 4 column but there were actually slightly more <em>Other</em> votes out of the total in the 3s (<em>Independent-Democrat</em>) party affiliation. Comparing the size of the 4s &amp; <em>Other</em> segment with the 3s &amp; <em>Other</em> segment, one should conclude that the 3s &amp; <em>Other</em> segment is a slightly larger portion of the total data set. There is generally a gradient of decreasing/increasing voting rates for the two primary candidates across the party affiliations, but there are a few exceptions. For example, the proportion of <em>Gore</em> voters goes up slightly between the <code>PARTY</code> affiliations of 5s and 6s – as the voters become more strongly republican. To have evidence of a relationship, there just needs to be a pattern of variation across the plot of some sort but it does not need to follow such an easily described pattern, especially when the categorical variables do not contain natural ordering.</p>
<p>The mosaic plots are best made on the tables created by the <code>tally</code> function from a table that just contains the counts (<strong>no totals</strong>):</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Makes a mosaic plot where areas are related to the proportion of</span>
<span class="co"># the total in the table</span>
<span class="kw">mosaicplot</span>(electable) </code></pre></div>
<div class="figure"><span id="fig:Figure5-5"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-5-1.png" alt="Mosaic plot of the 2000 election data comparing party affiliation and voting results." width="480" />
<p class="caption">
Figure 5.5: Mosaic plot of the 2000 election data comparing party affiliation and voting results.
</p>
</div>
<p>In general, the results here are not too surprising as the respondents became more heavily republican, they voted for Bush and the same pattern occurs as you look at more democratic respondents. As the voters leaned towards being independent, the proportion voting for “Other” increased. So it certainly seems that there is some sort of relationship between party affiliation and voting status. As always, it is good to compare the observed results to what we would expect if the null hypothesis is true. Figure <a href="chapter5.html#fig:Figure5-6">5.6</a> assumes that the null hypothesis is true and shows the variation in the proportions in each category in the columns and variation in the proportions across the rows, but displays no relationship between <code>PARTY</code> and <code>VOTEF</code>. Essentially, the pattern down a column is the same for all the columns or vice-versa for the rows. The way to think of “no relationship” here would involve considering whether knowing the party level could help you predict the voting response and that is not the case in Figure <a href="chapter5.html#fig:Figure5-6">5.6</a> but was in certain places in Figure <a href="chapter5.html#fig:Figure5-5">5.5</a>.</p>

<div class="figure"><span id="fig:Figure5-6"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-6-1.png" alt="Mosaic plot of what the 2000 election data would look like if the null hypothesis of no relationship were true." width="480" />
<p class="caption">
Figure 5.6: Mosaic plot of what the 2000 election data would look like if the null hypothesis of no relationship were true.
</p>
</div>
</div>
<div id="section5-4" class="section level2">
<h2><span class="header-section-number">5.4</span> Models for R by C tables</h2>
<p>This section is very short in this chapter because we really do not use any “models” in this material. There are some complicated statistical models that can be employed in these situations, but they are beyond the scope of this book. What we do have in this situation is our original data summary in the form of a contingency table, graphs of the results like those seen above, a hypothesis test and p-value (presented below), and some post-test plots that we can use to understand the “source” of any evidence we found in the test.</p>
</div>
<div id="section5-5" class="section level2">
<h2><span class="header-section-number">5.5</span> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</h2>
<p>In order to assess the evidence against our null hypotheses of no difference in distributions or no relationship between the variables, we need to define a test statistic and find its distribution under the null hypothesis. The test statistic used with both types of tests is called the <span class="math inline">\(\mathbf{X^2}\)</span> <strong><em>statistic</em></strong> (we want to call the statistic X-squared not Chi-squared). The statistic compares the observed counts in the contingency table to the <strong><em>expected counts</em></strong> under the null hypothesis, with large differences between what we observed and what we expect under the null leading to evidence against the null hypothesis. To help this statistic to follow a named parametric distribution and provide some insights into sources of interesting differences from the null hypothesis, we <strong><em>standardize</em></strong><a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a> the difference between the observed and expected counts by the square-root of the expected count. The <span class="math inline">\(\mathbf{X^2}\)</span> <strong><em>statistic</em></strong> is based on the sum of squared standardized differences,</p>
<p><span class="math display">\[\boldsymbol{X^2 = \Sigma^{RC}_{i=1}\left(\frac{Observed_i-Expected_i}
{\sqrt{Expected_i}}\right)^2},\]</span></p>
<p>which is the sum over all (<span class="math inline">\(R\)</span> times <span class="math inline">\(C\)</span>) cells in the contingency table of the square of the difference between observed and expected cell counts divided by the square root of the expected cell count. To calculate this test statistic, it useful to start with a table of expected cell counts to go with our contingency table of observed counts. The expected cell counts are easiest to understand in the homogeneity situation but are calculated the same in either scenario.</p>
<p>The idea underlying finding the <strong><em>expected cell counts</em></strong> is to find how many observations we would expect in category <span class="math inline">\(c\)</span> given the sample size in that group, <span class="math inline">\(\mathbf{n_{r\bullet}}\)</span>, if the null hypothesis is true. Under the null hypothesis across all <span class="math inline">\(R\)</span> groups the conditional probabilities in each response category must be the same. Consider Figure <a href="chapter5.html#fig:Figure5-7">5.7</a> where, under the null hypothesis, the probability of <em>None</em>, <em>Some</em>, and <em>Marked</em> are the same in both treatment groups. Specifically we have <span class="math inline">\(\text{Pr}(None)=0.5\)</span>, <span class="math inline">\(\text{Pr}(Some)=0.167\)</span>, and <span class="math inline">\(\text{Pr}(Marked)=0.333\)</span>. With <span class="math inline">\(\mathbf{n_{Placebo\bullet}}=43\)</span> and <span class="math inline">\(\text{Pr}(None)=0.50\)</span>, we would expect <span class="math inline">\(43*0.50=21.5\)</span> subjects to be found in the <em>Placebo, None</em> combination if the null hypothesis were true. Similarly, with <span class="math inline">\(\text{Pr}(Some)=0.167\)</span>, we would expect <span class="math inline">\(43*0.167=7.18\)</span> in the <em>Placebo, Some</em> cell. And for the <em>Treated</em> group with <span class="math inline">\(\mathbf{n_{Treated\bullet}}=41\)</span>, the expected count in the <em>Marked</em> improvement group would be <span class="math inline">\(41*0.333=13.65\)</span>. Those conditional probabilities came from aggregating across the rows because, under the null, the row (<em>Treatment</em>) should not matter. So, the conditional probability was actually calculated as <span class="math inline">\(\mathbf{n_{\bullet c}/N}\)</span> = total number of responses in category <span class="math inline">\(c\)</span> divided by the table total. Since each expected cell count was a conditional probability times the number of observations in the row, we can re-write the expected cell count formula for row <span class="math inline">\(r\)</span> and column <span class="math inline">\(c\)</span> as:</p>
<p><span class="math display">\[\mathbf{Expected\ cell\ count_{rc} = \frac{(n_{r\bullet}*n_{\bullet c})}{N}} = 
\frac{(\text{row } r \text{ total }*\text{ column } c \text{ total})}
{\text{table total}}.\]</span></p>
<p>Table <a href="chapter5.html#tab:Table5-3">5.3</a> demonstrates the calculations of the expected cell counts using this formula for all 6 cells in the <span class="math inline">\(2\times 3\)</span> table.</p>

<div class="figure"><span id="fig:Figure5-7"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-7-1.png" alt="Stacked bar chart that could occur if the null hypothesis were true for the Arthritis study." width="480" />
<p class="caption">
Figure 5.7: Stacked bar chart that could occur if the null hypothesis were true for the Arthritis study.
</p>
</div>

<table style="width:100%;">
<caption><span id="tab:Table5-3">Table 5.3: </span> Demonstration of calculation of expected cell counts for Arthritis data.</caption>
<colgroup>
<col width="8%" />
<col width="25%" />
<col width="25%" />
<col width="25%" />
<col width="14%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">    </th>
<th align="left"><strong>None</strong></th>
<th align="left"><strong>Some</strong></th>
<th align="left"><strong>Marked</strong></th>
<th align="left"><strong>Totals</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Placebo</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{None}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*42}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{21.5}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{Some}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*14}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{7.167}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Placebo}\bullet}*n_{\bullet\text{Marked}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{43*28}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{14.33}}}\)</span><br />
</td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\text{Placebo}\bullet}=43}\)</span></td>
</tr>
<tr class="even">
<td align="left"><strong>Treated</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{None}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*42}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{20.5}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{Some}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*14}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{6.83}}}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{\dfrac{n_{\text{Treated}\bullet}*n_{\bullet\text{Marked}}}{N}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\dfrac{41*28}{84}}\)</span><br />
<span class="math inline">\(\boldsymbol{=\color{red}{\mathbf{13.67}}}\)</span><br />
</td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\text{Treated}\bullet}=41}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><strong>Totals</strong></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{None}}=42}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{Some}}=14}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{n_{\bullet\text{Marked}}=28}\)</span></td>
<td align="left"><span class="math inline">\(\boldsymbol{N=84}\)</span></td>
</tr>
</tbody>
</table>

<p>Of course, using R can help us avoid tedium like this… The main engine for results in this chapter is the <code>chisq.test</code> function. It operates on a table of counts that has been produced <strong>without row or column totals</strong>.</p>

<p>For example, <code>Arthtable</code> below contains just the observed cell counts. Applying the <code>chisq.test</code> function to <code>Arthtable</code> provides a variety of useful output. For the moment, we are just going to extract the information in the “<code>expected</code>” attribute of the results from running this function (using <code>chisq.test()$expected)</code>. These are the expected cell counts which match the previous calculations except for some rounding in the hand-calculations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Arthtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>Treatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthritis)
Arthtable</code></pre></div>
<pre><code>##          Improved
## Treatment None Some Marked
##   Placebo   29    7      7
##   Treated   13    7     21</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>expected</code></pre></div>
<pre><code>##          Improved
## Treatment None     Some   Marked
##   Placebo 21.5 7.166667 14.33333
##   Treated 20.5 6.833333 13.66667</code></pre>
<p>With the observed and expected cell counts in hand, we can turn our attention to calculating the test statistic. It is possible to lay out the “contributions” to the <span class="math inline">\(X^2\)</span> statistic in a table format, allowing a simple way to finally calculate the statistic without losing any numbers. For <strong>each cell</strong> we need to find</p>
<p><span class="math display">\[(\text{observed}-\text{expected})/\sqrt{\text{expected}}),\]</span></p>
<p><strong>square them</strong>, and then we need to add them <strong>all</strong> up. In the current example, there are 6 cells to add up (<span class="math inline">\(R=2\)</span> times <span class="math inline">\(C=3\)</span>), shown in Table <a href="chapter5.html#tab:Table5-4">5.4</a>.</p>

<table>
<caption><span id="tab:Table5-4">Table 5.4: </span> <span class="math inline">\(X^2\)</span> contributions for the Arthritis data.</caption>
<colgroup>
<col width="10%" />
<col width="29%" />
<col width="29%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">    </th>
<th align="left"><strong>None</strong></th>
<th align="left"><strong>Some</strong></th>
<th align="left"><strong>Marked</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>Placebo</strong></td>
<td align="left"><span class="math inline">\(\left(\frac{29-21.5}{\sqrt{21.5}}\right)^2=\color{red}{\mathbf{2.616}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-7.167}{\sqrt{7.167}}\right)^2=\color{red}{\mathbf{0.004}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-14.33}{\sqrt{14.33}}\right)^2=\color{red}{\mathbf{3.752}}\)</span></td>
</tr>
<tr class="even">
<td align="left"><strong>Treated</strong></td>
<td align="left"><span class="math inline">\(\left(\frac{13-20.5}{\sqrt{20.5}}\right)^2=\color{red}{\mathbf{2.744}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{7-6.833}{\sqrt{6.833}}\right)^2=\color{red}{\mathbf{0.004}}\)</span></td>
<td align="left"><span class="math inline">\(\left(\frac{21-13.67}{\sqrt{13.67}}\right)^2=\color{red}{\mathbf{3.935}}\)</span></td>
</tr>
</tbody>
</table>
<p>Finally, the <span class="math inline">\(X^2\)</span> statistic here is the sum of these six results <span class="math inline">\(={\color{red}{2.616+0.004+3.752+2.744+0.004+3.935}}=13.055\)</span></p>
<p>Our favorite function in this chapter, <code>chisq.test</code>, does not provide the contributions to the <span class="math inline">\(X^2\)</span> statistic directly. It provides a related quantity called the</p>
<p><span class="math display">\[\textbf{standardized residual}=\left(\frac{\text{Observed}_i -
\text{Expected}_i}{\sqrt{\text{Expected}_i}}\right),\]</span></p>
<p>which, when squared (in R, squaring is accomplished using <code>^2</code>), is the contribution of that particular cell to the <span class="math inline">\(X^2\)</span> statistic that is displayed in Table <a href="chapter5.html#tab:Table5-4">5.4</a>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">(<span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals)<span class="op">^</span><span class="dv">2</span></code></pre></div>
<pre><code>##          Improved
## Treatment        None        Some      Marked
##   Placebo 2.616279070 0.003875969 3.751937984
##   Treated 2.743902439 0.004065041 3.934959350</code></pre>
<p>The most common error made in calculating the <span class="math inline">\(X^2\)</span> statistic by hand involves having observed less than expected and then failing to make the <span class="math inline">\(X^2\)</span> contribution positive for all cells (remember you are <strong>squaring the entire quantity</strong> in the parentheses and so the sign has to go positive!). In R, we can add up the cells using the <code>sum</code> function over the entire table of numbers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sum</span>((<span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals)<span class="op">^</span><span class="dv">2</span>)</code></pre></div>
<pre><code>## [1] 13.05502</code></pre>
<p>Or we can let R do all this hard work for us and get straight to the good stuff:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(Arthtable)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  Arthtable
## X-squared = 13.055, df = 2, p-value = 0.001463</code></pre>
<p>The <code>chisq.test</code> function reports a p-value by default. Before we discover how it got that result, we can rely on our permutation methods to obtain a distribution for the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis. As in Chapters <a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a>, this will allow us to find a p-value while relaxing one of our assumptions<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a>. In the One-WAY ANOVA in Chapter <a href="chapter3.html#chapter3">3</a>, we permuted the grouping variable relative to the responses, mimicking the null hypothesis that the groups are the same and so we can shuffle them around if the null is true. That same technique is useful here. If we randomly permute the grouping variable used to form the rows in the contingency table relative to the responses in the other variable and track the possibilities available for the <span class="math inline">\(X^2\)</span> statistic under permutations, we can find the probability of getting a result as extreme as or more extreme than what we observed assuming the null is true, our p-value. The observed statistic is the <span class="math inline">\(X^2\)</span> calculated using the formula above. Like the <span class="math inline">\(F\)</span>-statistic, it ends up that only results in the right tail of this distribution are desirable for finding evidence against the null hypothesis because all the values have to be positive. You can see this by observing that values of the <span class="math inline">\(X^2\)</span> statistic close to 0 are generated when the observed values are close to the expected values and that sort of result should not be used to find evidence against the null. When the observed and expected values are “far apart”, then we should find evidence against the null. It is helpful to work through some examples to be able to understand how the <span class="math inline">\(X^2\)</span> statistic “measures” differences between observed and expected.</p>
<p>To start, compare the previous observed <span class="math inline">\(X^2\)</span> of 13.055 to the sort of results we obtain in a single permutation of the treated/placebo labels – Figure <a href="chapter5.html#fig:Figure5-8">5.8</a> (top left panel) shows a permuted data set that produced <span class="math inline">\(X^{2*} = 0.62\)</span>. Visually, you can only see minimal differences between the treatment and placebo groups showing up in the stacked bar-chart. Three other permuted data sets are displayed in Figure <a href="chapter5.html#fig:Figure5-8">5.8</a> showing the variability in results in permutations but that none get close to showing the differences in the bars observed in the real data set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Arthperm &lt;-<span class="st"> </span>Arthritis
Arthperm<span class="op">$</span>PermTreatment &lt;-<span class="st"> </span><span class="kw">factor</span>(<span class="kw">shuffle</span>(Arthperm<span class="op">$</span>Treatment))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(Improved<span class="op">~</span>PermTreatment, <span class="dt">data=</span>Arthperm,
     <span class="dt">main=</span><span class="st">&quot;Stacked Bar Chart of Permuted Arthritis Data&quot;</span>)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Arthpermtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PermTreatment<span class="op">+</span>Improved, <span class="dt">data=</span>Arthperm)
Arthpermtable</code></pre></div>
<pre><code>##              Improved
## PermTreatment None Some Marked
##       Placebo   20    7     16
##       Treated   22    7     12</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(Arthpermtable)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  Arthpermtable
## X-squared = 0.6194, df = 2, p-value = 0.7337</code></pre>

<div class="figure"><span id="fig:Figure5-8"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-8-1.png" alt="Stacked bar charts of four permuted Arthritis data sets that produced \(X^2\) between 0.62 and 2.38." width="672" />
<p class="caption">
Figure 5.8: Stacked bar charts of four permuted Arthritis data sets that produced <span class="math inline">\(X^2\)</span> between 0.62 and 2.38.
</p>
</div>
<p>To build the permutation-based null distribution for the <span class="math inline">\(X^2\)</span> statistic, we need to collect up the test statistics (<span class="math inline">\(X^{2*}\)</span>) in many of these permuted results. The code is similar to permutation tests in Chapters <a href="chapter2.html#chapter2">2</a> and <a href="chapter3.html#chapter3">3</a> except that each permutation generates a new contingency table that is summarized and provided to <code>chisq.test</code> to analyze. We again extract the <code>$statistic</code>.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>statistic; Tobs</code></pre></div>
<pre><code>## X-squared 
##  13.05502</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(Treatment)<span class="op">+</span>Improved,
                               <span class="dt">data=</span>Arthritis))<span class="op">$</span>statistic
}
<span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## X-squared 
##     0.004</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">1</span>))
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>,
     <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">0</span>,Tobs<span class="op">+</span><span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-9"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-9-1.png" alt="Permutation distribution for the \(X^2\) statistic for the Arthritis data with an observed \(X^2\) of 13.1 (bold, vertical line)." width="480" />
<p class="caption">
Figure 5.9: Permutation distribution for the <span class="math inline">\(X^2\)</span> statistic for the Arthritis data with an observed <span class="math inline">\(X^2\)</span> of 13.1 (bold, vertical line).
</p>
</div>
<p>For an observed <span class="math inline">\(X^2\)</span> statistic of 13.055, four out of 1,000 permutation results matched or exceeded this value (<code>pdata</code> returned a value of 0.004). This suggests that our observed result is quite extreme relative to the null hypothesis and provides strong evidence against it.</p>

<p><strong>Validity conditions</strong> for a permutation <span class="math inline">\(X^2\)</span> test are:</p>
<ol style="list-style-type: decimal">
<li><p>Independence of observations.</p></li>
<li><p>Both variables are categorical.</p></li>
<li><p>Expected cell counts &gt; 0 (otherwise <span class="math inline">\(X^2\)</span> is not defined).</p></li>
</ol>
<p>For the permutation approach described here to provide valid inferences we need to be working with observations that are independent of one another. One way that a violation of independence can sometimes occur in this situation is when a single subject shows up in the table more than once. For example, if a single individual completes a survey more than once and those results are reported as if they came from <span class="math inline">\(N\)</span> independent individuals. Be careful about this as it is really easy to make tables of poorly collected or non-independent observations and then consider them for these analyses. Poor data still lead to poor conclusions even if you have fancy new statistical tools to use!</p>
</div>
<div id="section5-6" class="section level2">
<h2><span class="header-section-number">5.6</span> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</h2>
<p>When one additional assumption beyond the previous assumptions for the permutation test is met, it is possible to avoid permutations to find the distribution of the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis and get a p-value using what is called the <strong><em>Chi-square or</em></strong> <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-<strong>distribution</strong>. The name of our test statistic, X-squared, is meant to allude to the potential that this will follow a <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution in certain situations but may not do that all the time and we still can use the methods in Section <a href="chapter5.html#section5-5">5.5</a>. Along with the previous assumption regarding independence and all expected cell counts are greater than 0, we make a requirement that <strong><em>N</em></strong> (the total sample size) is “large enough” and this assumption is written in terms of the expected cell counts. If <strong><em>N</em></strong> is large, then all the expected cell counts should also be large because all those observations have to go somewhere. The problems for the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution as an approximation to the distribution of the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis come when expected cell counts are below 5. And the smaller the expected cell counts become, the more problematic the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution is as an approximation of the sampling distribution of the <span class="math inline">\(X^2\)</span> statistic under the null hypothesis. <strong>The standard rule of thumb is that all the expected cell counts need to exceed 5 for the parametric approach to be valid</strong>. When this condition is violated, it is better to use the permutation approach. The <code>chisq.test</code> function will provide a warning message to help you notice this. But it is good practice to always explore the expected cell counts using <code>chisq.test(...)$expected</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>expected</code></pre></div>
<pre><code>##          Improved
## Treatment None     Some   Marked
##   Placebo 21.5 7.166667 14.33333
##   Treated 20.5 6.833333 13.66667</code></pre>
<p>In the Arthritis data set, the sample size was sufficiently large for the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to provide an accurate p-value since the smallest expected cell count is 6.833 (so all expected counts are larger than 5).</p>
<p>The <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution is a right-skewed distribution that starts at 0 as shown in Figure <a href="chapter5.html#fig:Figure5-10">5.10</a>. Its shape changes as a function of its degrees of freedom. In the contingency table analyses, the <strong><em>degrees of freedom</em></strong> for the Chi-squared test are calculated as</p>
<p><span class="math display">\[\textbf{DF} \mathbf{=(R-1)*(C-1)} = (\text{number of rows }-1)*
(\text{number of columns }-1).\]</span></p>
<p>In the <span class="math inline">\(2 \times 3\)</span> table above, the <span class="math inline">\(\text{DF}=(2-1)*(3-1)=2\)</span> leading to a Chi-square distribution with 2 <em>df</em> for the distribution of <span class="math inline">\(X^2\)</span> under the null hypothesis. The p-value is based on the area to the right of the observed <span class="math inline">\(X^2\)</span> value of 13.055 and the <code>pchisq</code> function provides that area as 0.00146. Note that this is very similar to the permutation result found previously for these data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="fl">13.055</span>, <span class="dt">df=</span><span class="dv">2</span>, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 0.001462658</code></pre>
<p>We’ll see more examples of the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distributions in each of the examples that follow.</p>

<div class="figure"><span id="fig:Figure5-10"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-10-1.png" alt="\(\boldsymbol{\chi^2}\)-distribution with two degrees of freedom with 13.1 indicated with a vertical line." width="480" />
<p class="caption">
Figure 5.10: <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with two degrees of freedom with 13.1 indicated with a vertical line.
</p>
</div>
<p>A small side note about sample sizes is warranted here. In contingency tables, especially those based on survey data, it is common to have large overall sample sizes (<span class="math inline">\(N\)</span>). With large sample sizes, it becomes easy to reject the null the hypothesis, even when the “distance” from the null is relatively minor and possibly unimportant. By this we mean that it might be possible to reject the null hypothesis even if the observed proportions are a small practical distance from the situation described in the null. After obtaining a small p-value, we need to consider whether we have obtained <strong><em>practical significance</em></strong> (or maybe better described as <strong><em>practical importance</em></strong>) to accompany our discussion of rejecting the null hypothesis. Whether a result is large enough to be of practical importance can only be judged by knowing something about the situation we are studying and by providing a good summary of our results to allow experts to assess the size and importance of the result. “Statistical significance” (what our tests address) is just a first step in any situation although many researchers are so happy to see small p-values that this is their last step.</p>
<p>If we revisit our observed results, re-plotted in Figure <a href="chapter5.html#fig:Figure5-11">5.11</a> since Figure <a href="chapter5.html#fig:Figure5-2">5.2</a> is many pages earlier, knowing that we have strong evidence against the null hypothesis of no difference between <em>Placebo</em> and <em>Treated</em> groups, what can we say about the effectiveness of the arthritis medication? It seems that there is a real and important increase in the proportion of patients getting improvement (<em>Some</em> or <em>Marked</em>). If the differences “looked” smaller, even with a small p-value you might not recommend someone take the drug…</p>

<div class="figure"><span id="fig:Figure5-11"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-11-1.png" alt="Stacked bar chart of the Arthritis data comparing Treated and Placebo." width="480" />
<p class="caption">
Figure 5.11: Stacked bar chart of the Arthritis data comparing <em>Treated</em> and <em>Placebo</em>.
</p>
</div>
</div>
<div id="section5-7" class="section level2">
<h2><span class="header-section-number">5.7</span> Examining residuals for the source of differences</h2>
<p>Small p-values are generated by large <span class="math inline">\(X^2\)</span> values. If we want to understand the source of a small p-value, we need to understand what made the test statistic large. To get a large <span class="math inline">\(X^2\)</span> value, we either need many small contributions from lots of cells or a few large contributions. In most situations, there are just a few cells that show large deviations between the null hypothesis (expected cell counts) and what was observed (observed cell counts). It is possible to explore the “size” and direction of the differences between observed and expected counts to learn something about the behavior of the relationship between the variables, especially as it relates to evidence against the null hypothesis of no difference or no relationship. The <strong><em>standardized residual</em></strong>,</p>
<p><span class="math display">\[\boldsymbol{\left(\frac{\textbf{Observed}_i - 
\textbf{Expected}_i}{\sqrt{\textbf{Expected}_i}}\right)},\]</span></p>
<p>provides a measure of deviation of the observed from expected which retains the <strong>direction of deviation</strong> (whether observed was <strong>more or less than expected</strong> is interesting for interpretations) for each cell in the table. It is scaled much like a standard normal distribution providing a scale for “large” deviations for absolute values that are over 2 or 3. In other words, values with magnitude over 2 should be your focus in the standardized residuals, noting whether the observed counts were much more or less than expected. On the <span class="math inline">\(X^2\)</span> scale, standardized residuals of 2 or more mean that the cells are contributing 4 or more units to the overall statistic, which is a pretty noticeable bump up in the size of the statistic. A few contributions at 4 or higher and you will likely end up with a small p-value.</p>
<p>There are two ways to explore standardized residuals. First, we can obtain them via the <code>chisq.test</code> and manually identify the “big ones”. Second, we can augment a mosaic plot of the table with the standardized results by turning on the <code>shade=T</code> option and have the plot help us find the big differences. This technique can be applied whether we are performing an Independence or Homogeneity test – both are evaluated with the same <span class="math inline">\(X^2\)</span> statistic so the large standardized residuals are of interest in both situations. Both types of results are shown for the Arthritis data table:</p>

<pre><code>##          Improved
## Treatment        None        Some      Marked
##   Placebo  1.61749160 -0.06225728 -1.93699199
##   Treated -1.65647289  0.06375767  1.98367320</code></pre>
<div class="figure"><span id="fig:Figure5-12"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-12-1.png" alt="Mosaic plot of the Arthritis data with large standardized residuals indicated (actually, there were none that were indicated because all were less than 2)." width="480" />
<p class="caption">
Figure 5.12: Mosaic plot of the Arthritis data with large standardized residuals indicated (actually, there were none that were indicated because all were less than 2).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(Arthtable)<span class="op">$</span>residuals
<span class="kw">mosaicplot</span>(Arthtable, <span class="dt">shade=</span>T)</code></pre></div>
<p>In these data, the standardized residuals are all less than 2 in magnitude so Figure <a href="chapter5.html#fig:Figure5-12">5.12</a> isn’t too helpful but this type of plot is in other examples. The largest contributions to the <span class="math inline">\(X^2\)</span> statistic come from the <em>Placebo</em> and <em>Treated</em> groups in the <em>Marked</em> improvement cells. Those standardized residuals are -1.94 and 1.98 (both really close to 2), showing that <em>placebo</em> group had <strong>noticeably fewer</strong> <em>Marked</em> improvement<br />
<strong>results than expected</strong> and the <em>Treated</em> group <strong>had noticeably more</strong> <em>Marked</em> improvement responses <strong>than expected if the null hypothesis was true</strong>. Similarly but with smaller magnitudes, there were more <em>None</em> results than expected in the <em>Placebo</em> group and fewer <em>None</em> results than expected in the <em>Treated</em> group. The standardized residuals were very small in the two cells for the <em>Some</em> improvement category, showing that the treatment/placebo were similar in this response category and that the results were about what would be expected if the null hypothesis of no difference were true.</p>
</div>
<div id="section5-8" class="section level2">
<h2><span class="header-section-number">5.8</span> General protocol for <span class="math inline">\(X^2\)</span> tests</h2>
<p>In any contingency table situation, there is a general protocol to completing an analysis.</p>
<ol style="list-style-type: decimal">
<li><p>Identify the data collection method and whether the proper analysis is based on the Independence or Homogeneity hypotheses (Section <a href="chapter5.html#section5-1">5.1</a>).</p></li>
<li><p>Make a contingency table and get a general sense of response patterns. Pay attention to “small” counts, especially cells with 0 counts.</p>
<ol style="list-style-type: lower-alpha">
<li>If there are many small count cells, consider combining categories on one or both variables to make a new variable with fewer categories that has larger counts per cell to have more robust inferences (see Section <a href="chapter5.html#section5-10">5.10</a> for a related example).</li>
</ol></li>
<li><p>Make the appropriate graphical display of results and generally describe the pattern of responses.</p>
<ol style="list-style-type: lower-alpha">
<li><p>For Homogeneity, make a stacked bar-chart.</p></li>
<li><p>For Independence, make a mosaic plot.</p></li>
<li><p>Consider a more general exploration using a table plot if other variables were measured to check for confounding and other interesting multi-variable relationships.</p></li>
</ol></li>
<li><p>Conduct the 6+ steps of the appropriate type of hypothesis test.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Use permutations if any expected cell counts are below 5.</p></li>
<li><p>If all expected cell counts greater than 5, either permutation or parametric approaches are acceptable.</p></li>
</ol></li>
<li><p>Explore the standardized residuals for the “source” of any evidence against the null.</p>
<ol style="list-style-type: lower-alpha">
<li>Tie the interpretation of the “large” standardized residuals and their direction (above or below expected under the null) back into the original data display. Work to find a story for the pattern of responses. If little evidence is found against the null, there is not much to do here.</li>
</ol></li>
</ol>
</div>
<div id="section5-9" class="section level2">
<h2><span class="header-section-number">5.9</span> Political party and voting results: Complete analysis</h2>
<p>As introduced in Section <a href="chapter5.html#section5-3">5.3</a>, a national random sample of voters was obtained related to the 2000 Presidential Election with the party affiliations and voting results recorded for each subject. The data are available in <code>election</code> in the <code>poLCA</code> package <span class="citation">(Linzer and Lewis. <a href="#ref-R-poLCA">2014</a>)</span>. It is always good to start with a bit of data exploration with a table plot, displayed in Figure <a href="chapter5.html#fig:Figure5-13">5.13</a>. Many of the lines of code here are just for making sure that R is treating the categorical variables that were coded numerically as categorical variables.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">election<span class="op">$</span>VOTEF &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>VOTE3)
election<span class="op">$</span>PARTY &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>PARTY)
election<span class="op">$</span>EDUC &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>EDUC)
election<span class="op">$</span>SEX &lt;-<span class="st"> </span><span class="kw">factor</span>(election<span class="op">$</span>GENDER)
<span class="kw">levels</span>(election<span class="op">$</span>VOTEF) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Gore&quot;</span>,<span class="st">&quot;Bush&quot;</span>,<span class="st">&quot;Other&quot;</span>)
<span class="kw">tableplot</span>(election, <span class="dt">select=</span><span class="kw">c</span>(VOTEF,PARTY,EDUC,SEX))</code></pre></div>
<div class="figure"><span id="fig:Figure5-13"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-13-1.png" alt="Table plot of vote, party affiliation, education, and sex from election survey data. Note that missing observations are present in three of four variables. Education is coded from 1 to 7 with higher values related to higher educational attainment. Sex code 1 is for male and 2 is for female." width="960" />
<p class="caption">
Figure 5.13: Table plot of vote, party affiliation, education, and sex from election survey data. Note that missing observations are present in three of four variables. Education is coded from 1 to 7 with higher values related to higher educational attainment. Sex code 1 is for male and 2 is for female.
</p>
</div>
<p>In Figure <a href="chapter5.html#fig:Figure5-13">5.13</a>, we can see many missing <code>VOTEF</code> responses but also some missingness in <code>PARTY</code> and <code>EDUC</code> (<em>Education</em>) status. While we don’t know too much about why people didn’t respond on the Vote question – they could have been unwilling to answer it or may not have voted. It looks like those subjects have more of the lower education level responses (more dark green, brown, and purple) than in the responders to this question. There are many “middle” ratings (orange) in the party affiliation responses for the missing <code>VOTEF</code> responses, suggesting that independents were less likely to answer the question in the survey for whatever reason. We want to focus on those that did respond in <code>VOTEF</code>, so will again use <code>na.omit</code> to clean out any subjects with any missing responses on these four variables and remake this plot (Figure <a href="chapter5.html#fig:Figure5-14">5.14</a>). The code also adds the <code>sort</code> option to the <code>tableplot</code> function call that provides an easy way to sort the data set based on other variables. It is interesting, for example, to sort the responses by <em>Education</em> level and explore the differences in other variables. These explorations are omitted here but easily available by changing the sorting column from 1 to <code>sort=3</code>. Figure <a href="chapter5.html#fig:Figure5-14">5.14</a> shows us that there are clear differences in party affiliation based on voting for <em>Bush</em>, <em>Gore</em>, or <em>Other</em>. It is harder to see if there are differences in education level or sex based on the voting status in this plot, but, as noted above, sorting on these other variables can sometimes help to see other relationships between variables.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">election2 &lt;-<span class="st"> </span><span class="kw">na.omit</span>(election[,<span class="kw">c</span>(<span class="st">&quot;VOTEF&quot;</span>,<span class="st">&quot;PARTY&quot;</span>,<span class="st">&quot;EDUC&quot;</span>,<span class="st">&quot;SEX&quot;</span>)])
<span class="kw">tableplot</span>(election2, <span class="dt">select=</span><span class="kw">c</span>(VOTEF,PARTY,EDUC,SEX), <span class="dt">sort=</span><span class="dv">1</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-14"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-14-1.png" alt="Table plot of election data with subjects without any missing responses." width="960" />
<p class="caption">
Figure 5.14: Table plot of election data with subjects without any missing responses.
</p>
</div>
<p>Focusing on the party affiliation and voting results, the appropriate analysis is with an Independence test because a single random sample was obtained from the population. The total sample size for the complete responses was <span class="math inline">\(N=\)</span> 1,149 (out of the original 1,785 subjects). Because this is an Independence test, the mosaic plot is the appropriate display of the results, which was provided in Figure <a href="chapter5.html#fig:Figure5-5">5.5</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">electable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>PARTY<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2)
electable</code></pre></div>
<pre><code>##      VOTEF
## PARTY Gore Bush Other
##     1  238    6     2
##     2  151   18     1
##     3  113   31    13
##     4   37   36    11
##     5   21  124    12
##     6   20  121     2
##     7    3  188     1</code></pre>
<p>There is a potential for bias in some polls because of the methods used to find and contact people. As US residents have transitioned from land-lines to cell phones, the early adopting cell phone users were often excluded from political polling. These policies are being reconsidered to adapt to the decline in residential phone lines and most polling organizations now include cell phone numbers in their list of potential respondents. This study may have some bias regarding who was considered as part of the population of interest and who was actually found that was willing to respond to their questions. We don’t have much information here but biases arising from unobtainable members of populations are a potential issue in many studies, especially when questions tend toward more sensitive topics. We can make inferences here to people that were willing to respond to the request to answer the survey but should be cautious in extending it to all Americans or even voters in the year 2000. When we say “population” below, this nuanced discussion is what we mean. Because the political party is not randomly assigned to the subjects, we cannot make causal inferences for political affiliation causing different voting patterns<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a>.</p>
<ol style="list-style-type: decimal">
<li><p><strong>Hypotheses:</strong></p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no relationship between the party affiliation (7 levels) and voting results (<em>Bush</em>, <em>Gore</em>, <em>Other</em>) in the population.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: There is a relationship between the party affiliation (7 levels) and voting results (<em>Bush</em>, <em>Gore</em>, <em>Other</em>) in the population.</p></li>
</ul></li>
<li><p><strong>Validity conditions:</strong></p>
<ul>
<li><p>Independence:</p>
<ul>
<li>This assumption is presumed to be met since each subject is measured only once in the table. No other information suggests a potential issue since a random sample was taken from presumably a large national population and we have no information that could suggest dependencies among observations.</li>
</ul></li>
<li><p>All expected cell counts larger than 5 to use the parametric <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to find p-values:</p>
<ul>
<li>We need to generate a table of expected cell counts to be able to check this condition:</li>
</ul>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(electable)<span class="op">$</span>expected</code></pre></div>
<pre><code>## Warning in chisq.test(electable): Chi-squared approximation may be
## incorrect</code></pre>
<pre><code>##      VOTEF
## PARTY      Gore      Bush    Other
##     1 124.81984 112.18799 8.992167
##     2  86.25762  77.52829 6.214099
##     3  79.66144  71.59965 5.738903
##     4  42.62141  38.30809 3.070496
##     5  79.66144  71.59965 5.738903
##     6  72.55788  65.21497 5.227154
##     7  97.42037  87.56136 7.018277</code></pre>
<ul>
<li><p>When we request the expected cell counts, R tries to help us with a warning message if the expected cell counts might be small, as in this situation.</p></li>
<li><p>There is one expected cell count below 5 for <code>Party</code> = 4 who voted <em>Other</em> with an expected cell count of 3.102, so the condition is violated and the permutation approach should be used to obtain more trustworthy p-values. The conditions are met for performing a permutation test.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic:</strong></p>
<ul>
<li>This is best performed by the <code>chisq.test</code> function since there are 21 cells and many potential places for a calculation error if performed by hand.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(electable)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  electable
## X-squared = 762.81, df = 12, p-value &lt; 2.2e-16</code></pre>
<ul>
<li>The observed <span class="math inline">\(X^2\)</span> statistic is 762.81.</li>
</ul></li>
<li><p><strong>Find the p-value:</strong></p>
<ul>
<li>The parametric p-value is &lt; 2.2e-16 from the R output which would be reported as &lt; 0.0001. This was based on a <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with <span class="math inline">\((7-1)*(3-1) = 12\)</span> degrees of freedom displayed in Figure <a href="chapter5.html#fig:Figure5-15">5.15</a>. Note that the observed test statistic of 762.81 was off the plot to the right which reflects how little area is to the right of that value in the distribution.</li>
</ul>
<div class="figure"><span id="fig:Figure5-15"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-15-1.png" alt="Plot of $\boldsymbol{\chi^2}$-distribution with 12 degrees of freedom." width="288" />
<p class="caption">
Figure 5.15: Plot of <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with 12 degrees of freedom.
</p>
</div>
<ul>
<li>If you want to repeat this calculation directly you get a similarly tiny value that R reports as 1.5e-155. Again, reporting less than 0.0001 is just fine.</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="fl">762.81</span>, <span class="dt">df=</span><span class="dv">12</span>, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 1.553744e-155</code></pre>
<ul>
<li>But since the expected cell count condition is violated, we should use permutations as implemented in the following code to provide a trustworthy p-value:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(electable)<span class="op">$</span>statistic; Tobs</code></pre></div>
<pre><code>## X-squared 
##  762.8095</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">1000</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, <span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(PARTY)<span class="op">+</span>VOTEF, <span class="dt">data=</span>election2,
                               <span class="dt">margins=</span>F))<span class="op">$</span>statistic
}
<span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## X-squared 
##         0</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-16"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-16-1.png" alt="Permutation distribution of $X^2$ for the election data. Observed value of 763 not displayed." width="480" />
<p class="caption">
Figure 5.16: Permutation distribution of <span class="math inline">\(X^2\)</span> for the election data. Observed value of 763 not displayed.
</p>
</div>
<ul>
<li>The last results tells us that there were no permuted data sets that produced larger <span class="math inline">\(X^2\text{&#39;s}\)</span> than the observed <span class="math inline">\(X^2\)</span> in 1,000 permutations, so we report that the <strong>p-value was less than 0.001</strong> using the permutation approach. The permutation distribution in Figure <a href="chapter5.html#fig:Figure5-16">5.16</a> contains no results over 40, so the observed configuration was really far from the null hypothesis of no relationship between party status and voting.</li>
</ul></li>
<li><p><strong>Make a decision:</strong></p>
<ul>
<li>With a p-value less than 0.001, we can say that there is almost no chance of observing a configuration like ours or more extreme if the null hypothesis is true. So there is very strong evidence against the null hypothesis.</li>
</ul></li>
<li><p><strong>Write a conclusion and scope of inference:</strong></p>
<ul>
<li>There is strong evidence that there is a relationship between party affiliation and voting results in the population. The results are not causal since no random assignment was present but they do apply to the population of voters in the 2000 election that were able to be contacted by those running the poll and who would be willing to answer all the questions and voted.</li>
</ul></li>
</ol>
<p>We can add a little more refinement to the results by exploring the standardized residuals. The numerical results are obtained using:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(electable)<span class="op">$</span>residuals <span class="co">#(Obs - expected)/sqrt(expected)</span></code></pre></div>
<pre><code>##      VOTEF
## PARTY        Gore        Bush       Other
##     1  10.1304439 -10.0254117  -2.3317373
##     2   6.9709179  -6.7607252  -2.0916557
##     3   3.7352759  -4.7980730   3.0310127
##     4  -0.8610559  -0.3729136   4.5252413
##     5  -6.5724708   6.1926811   2.6135809
##     6  -6.1701472   6.9078679  -1.4115200
##     7  -9.5662296  10.7335798  -2.2717310</code></pre>
<p>And visually using:</p>

<div class="figure"><span id="fig:Figure5-17"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-17-1.png" alt="Mosaic plot with shading based on standardized residuals for the election" width="480" />
<p class="caption">
Figure 5.17: Mosaic plot with shading based on standardized residuals for the election
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Adds information on the size of the residuals</span>
<span class="kw">mosaicplot</span>(electable, <span class="dt">shade=</span>T) </code></pre></div>
<p>In this example, the standardized residuals show some clear sources of the differences from the results expected if there were no relationship present. The largest contributions are found in the highest democrat category (<code>PARTY</code> = 1) where the standardized residual for <em>Gore</em> is 10.13 and for <em>Bush</em> is -10.03, showing much higher than expected (under <span class="math inline">\(H_0\)</span>) counts for Gore voters and much lower than expected (under <span class="math inline">\(H_0\)</span>) for Bush. Similar results in the opposite direction are found in the strong republicans (<code>PARTY</code> = 7). Note how the brightest shade of blue in Figure <a href="chapter5.html#fig:Figure5-17">5.17</a> shows up for much higher than expected results and the brighter red for results in the other direction, where observed counts were much lower than expected. When there are many large standardized residuals, it is OK to focus on the largest results but remember that some of the intermediate deviations, or lack thereof, could also be interesting. For example, the Gore voters from <code>PARTY</code> = 3 had a standardized residual of 3.75 but the <code>PARTY</code> = 5 voters for Bush had a standardized residual of 6.17. So maybe Gore didn’t have as strong of support from his center-leaning supporters as Bush was able to obtain from the same voters on the other side of the middle? A political scientist would easily obtain many more (useful) theories, especially once they understood the additional information provided by exploring the standardized residuals.</p>
</div>
<div id="section5-10" class="section level2">
<h2><span class="header-section-number">5.10</span> Is cheating and lying related in students?</h2>
<p>A study of student behavior was performed at a university with a survey of <span class="math inline">\(N=319\)</span> undergraduate students (<code>cheating</code> data set from the <code>poLCA</code> package originally published by <span class="citation">Dayton (<a href="#ref-Dayton1998">1998</a>)</span>). They were asked to answer four questions about their various academic frauds that involved cheating and lying. Specifically, they were asked if they had ever lied to avoid taking an exam (<code>LIEEXAM</code> with 1 for no and 2 for yes), if they had lied to avoid handing in a term paper on time (<code>LIEPAPER</code> with 2 for yes), if they had purchased a term paper to hand in as their own or obtained a copy of an exam prior to taking the exam (<code>FRAUD</code> with 2 for yes), and if they had copied answers during an exam from someone near them (<code>COPYEXAM</code> with 2 for yes). Additionally, their <code>GPA</code>s were obtained and put into categories: (&lt;2.99, 3.0 to 3.25, 3.26 to 3.50, 3.51 to 3.75, and 3.76 to 4.0). These categories were coded from 1 to 5, respectively. Again, the code starts with making sure the variables are treated categorically by applying the <code>factor</code> function.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(poLCA)
<span class="kw">data</span>(cheating) <span class="co">#Survey of students</span>
cheating &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(cheating)
cheating<span class="op">$</span>LIEEXAM &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>LIEEXAM)
cheating<span class="op">$</span>LIEPAPER &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>LIEPAPER)
cheating<span class="op">$</span>FRAUD &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>FRAUD)
cheating<span class="op">$</span>COPYEXAM &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>COPYEXAM)
cheating<span class="op">$</span>GPA &lt;-<span class="st"> </span><span class="kw">factor</span>(cheating<span class="op">$</span>GPA)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">require</span>(tabplot)
<span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>GPA)</code></pre></div>
<div class="figure"><span id="fig:Figure5-18"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-18-1.png" alt="Table plot of initial cheating and lying data set." width="480" />
<p class="caption">
Figure 5.18: Table plot of initial cheating and lying data set.
</p>
</div>
<p>We can explore some interesting questions about the relationships between these variables. The table plot in Figure <a href="chapter5.html#fig:Figure5-18">5.18</a> again helps us to get a general idea of the data set and to assess some complicated aspects of the relationships between variables. For example, the rates of different unethical behaviors seem to decrease with higher GPA students (but do not completely disappear!). This data set also has a few missing GPAs that we would want to carefully consider – which sorts of students might not be willing to reveal their GPAs? It ends up that these students did not <em>admit</em> to any of the unethical behaviors… Note that we used the <code>sort=GPA</code> option in the <code>tableplot</code> function to sort the responses based on <code>GPA</code> to see how <code>GPA</code> might relate to patterns of unethical behavior.</p>
<p>While the relationship between GPA and presence/absence of the different behaviors is of interest, we want to explore the types of behaviors. It is possible to group the lying behaviors as being a different type (less extreme?) of unethical behavior than obtaining an exam prior to taking it, buying a paper, or copying someone else’s answers. We want to explore whether there is some sort of relationship between the lying and copying behaviors – are those that engage in one type of behavior more likely to do the other? Or are they independent of each other? This is a hard story to elicit from the previous plot because there are so many variables involved.</p>
<p>To simplify the results, combining the two groups of variables into the four possible combinations on each has the potential to simplify the results – or at least allow exploration of additional research questions. In the table plot in Figure <a href="chapter5.html#fig:Figure5-19">5.19</a>, you can see the four categories for each, starting with no bad behavior of either type (which is fortunately the most popular response on both variables!). For each variable, there are students who admitted to one of the two violations and some that did both. The <code>liar</code> variable has categories of <em>None</em>, <em>ExamLie</em>, <em>PaperLie</em>, and <em>LieBoth</em>. The <code>copier</code> variable has categories of <em>None</em>, <em>PaperCheat</em>, <em>ExamCheat</em>, and <em>PaperExamCheat</em> (for doing both). The last category for <code>copier</code> (plotted as an orange color) seems to mostly occur at the top of the plot which is where the students who had lied to get out of things reside, so maybe there is a relationship between those two types of behaviors? On the other hand, for the students who have never lied, quite a few had cheated on exams. The contingency table can help us dig further into the hypotheses related to the Chi-square test of Independence that is appropriate in this situation.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cheating<span class="op">$</span>liar &lt;-<span class="st"> </span><span class="kw">interaction</span>(cheating<span class="op">$</span>LIEEXAM, cheating<span class="op">$</span>LIEPAPER)
<span class="kw">levels</span>(cheating<span class="op">$</span>liar) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamLie&quot;</span>,<span class="st">&quot;PaperLie&quot;</span>,<span class="st">&quot;LieBoth&quot;</span>)

cheating<span class="op">$</span>copier &lt;-<span class="st"> </span><span class="kw">interaction</span>(cheating<span class="op">$</span>FRAUD, cheating<span class="op">$</span>COPYEXAM)
<span class="kw">levels</span>(cheating<span class="op">$</span>copier) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;PaperCheat&quot;</span>,<span class="st">&quot;ExamCheat&quot;</span>,<span class="st">&quot;PaperExamCheat&quot;</span>)

cheatlietable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>liar<span class="op">+</span>copier, <span class="dt">data=</span>cheating)
cheatlietable</code></pre></div>
<pre><code>##           copier
## liar       None PaperCheat ExamCheat PaperExamCheat
##   None      207          7        46              5
##   ExamLie    10          1         3              2
##   PaperLie   13          1         4              2
##   LieBoth    11          1         4              2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>liar, <span class="dt">select=</span><span class="kw">c</span>(liar,copier))</code></pre></div>
<div class="figure"><span id="fig:Figure5-19"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-19-1.png" alt="Table plot of new variables liar and copier that allow exploration of relationships between different types of lying and cheating behaviors." width="960" />
<p class="caption">
Figure 5.19: Table plot of new variables liar and copier that allow exploration of relationships between different types of lying and cheating behaviors.
</p>
</div>
<p>Unfortunately for our statistic, there were very few responses in some combinations of categories even with <span class="math inline">\(N=319\)</span>. For example, there was only one response each in the combinations for students that copied on papers and lied to get out of exams, papers, and both. Some other categories were pretty small as well in the groups that only had one behavior present. To get a higher number of counts in the combinations, we combined the single behavior only levels into “<em>either</em>” categories and left the <em>none</em> and <em>both</em> categories for each variable. This creates two new variables called <code>liar2</code> and <code>copier2</code> (table plot in Figure <a href="chapter5.html#fig:Figure5-20">5.20</a>). The code to create these variables and make the plot is below.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#Collapse the middle categories of each variable</span>
cheating<span class="op">$</span>liar2 &lt;-<span class="st"> </span>cheating<span class="op">$</span>liar
<span class="kw">levels</span>(cheating<span class="op">$</span>liar2) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;LieBoth&quot;</span>)
cheating<span class="op">$</span>copier2 &lt;-<span class="st"> </span>cheating<span class="op">$</span>copier
<span class="kw">levels</span>(cheating<span class="op">$</span>copier2) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;None&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;ExamorPaper&quot;</span>,<span class="st">&quot;CopyBoth&quot;</span>)
<span class="kw">tableplot</span>(cheating, <span class="dt">sort=</span>liar2, <span class="dt">select=</span><span class="kw">c</span>(liar2,copier2))</code></pre></div>
<div class="figure"><span id="fig:Figure5-20"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-20-1.png" alt="Table plot of lying and copying variables after combining categories." width="960" />
<p class="caption">
Figure 5.20: Table plot of lying and copying variables after combining categories.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">cheatlietable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>liar2<span class="op">+</span>copier2, <span class="dt">data=</span>cheating)
cheatlietable</code></pre></div>
<pre><code>##              copier2
## liar2         None ExamorPaper CopyBoth
##   None         207          53        5
##   ExamorPaper   23           9        4
##   LieBoth       11           5        2</code></pre>
<p>This <span class="math inline">\(3\times 3\)</span> table is more manageable and has few really small cells so we will proceed with the 6+ steps of hypothesis testing applied to these data using the Independence testing methods (again a single sample was taken from the population):</p>
<ol style="list-style-type: decimal">
<li><p><strong>Hypotheses:</strong></p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: Lying and copying behavior are independent in the population of students at this university.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: Lying and copying behavior are dependent in the population of students at this university.</p></li>
</ul></li>
<li><p><strong>Validity conditions:</strong></p>
<ul>
<li><p>Independence:</p>
<ul>
<li>This assumption is presumed to be met since each subject is measured only once in the table. No other information suggests a potential issue but we don’t have much information on how these subjects were obtained.</li>
</ul></li>
<li><p>All expected cell counts larger than 5 (required to use <span class="math inline">\(\chi^2\)</span>-distribution to find p-values):</p>
<ul>
<li>We need to generate a table of expected cell counts to check this condition:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(cheatlietable)<span class="op">$</span>expected</code></pre></div>
<pre><code>##              copier2
## liar2              None ExamorPaper  CopyBoth
##   None        200.20376   55.658307 9.1379310
##   ExamorPaper  27.19749    7.561129 1.2413793
##   LieBoth      13.59875    3.780564 0.6206897</code></pre>
<ul>
<li><p>When we request the expected cell counts, there is a warning message (not shown).</p></li>
<li><p>There are three expected cell counts below 5, so the condition is violated and a permutation approach should be used to obtain more trustworthy p-values.</p></li>
</ul></li>
</ul></li>
<li><p><strong>Calculate the test statistic:</strong></p>
<ul>
<li>Use <code>chisq.test</code> although this table is small enough to do by hand if you want the practice – see if you can find a similar answer to what the function provides:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(cheatlietable)</code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  cheatlietable
## X-squared = 13.238, df = 4, p-value = 0.01017</code></pre>
<ul>
<li>The <span class="math inline">\(X^2\)</span> statistic is 13.24.</li>
</ul></li>
<li><p><strong>Find the p-value:</strong></p>
<ul>
<li>The parametric p-value is 0.0102 from the R output. This was based on a <span class="math inline">\(\chi^2\)</span>-distribution with <span class="math inline">\((3-1)*(3-1) = 4\)</span> degrees of freedom that is displayed in Figure <a href="chapter5.html#fig:Figure5-21">5.21</a>. Remember that this isn’t quite the right distribution for the test statistic since our expected cell count condition was violated.</li>
</ul>
<div class="figure"><span id="fig:Figure5-21"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-21-1.png" alt="Plot of $\boldsymbol{\chi^2}$-distribution with 4 degrees of freedom." width="336" />
<p class="caption">
Figure 5.21: Plot of <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with 4 degrees of freedom.
</p>
</div>
<ul>
<li>If you want to repeat the p-value calculation directly:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">pchisq</span>(<span class="fl">13.2384</span>, <span class="dt">df=</span><span class="dv">4</span>, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## [1] 0.01016781</code></pre>
<ul>
<li>But since the expected cell condition is violated, we should use permutations as implemented in the following code with the number of permutations increased to 10,000 to help get a better estimate of the p-value since it is possibly close to 0.05:</li>
</ul>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Tobs &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span>liar2<span class="op">+</span>copier2, <span class="dt">data=</span>cheating))<span class="op">$</span>statistic
Tobs</code></pre></div>
<pre><code>## X-squared 
##  13.23844</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
B &lt;-<span class="st"> </span><span class="dv">10000</span> <span class="co"># Now performing 10,000 permutations</span>
Tstar &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>,<span class="dt">nrow=</span>B)
<span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="op">:</span>B)){
  Tstar[b] &lt;-<span class="st"> </span><span class="kw">chisq.test</span>(<span class="kw">tally</span>(<span class="op">~</span><span class="kw">shuffle</span>(liar2)<span class="op">+</span>copier2,
                               <span class="dt">data=</span>cheating))<span class="op">$</span>statistic
}
<span class="kw">pdata</span>(Tstar, Tobs, <span class="dt">lower.tail=</span>F)</code></pre></div>
<pre><code>## X-squared 
##     0.017</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(Tstar)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(<span class="kw">density</span>(Tstar), <span class="dt">main=</span><span class="st">&quot;Density curve of Tstar&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">abline</span>(<span class="dt">v=</span>Tobs, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-22"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-22-1.png" alt="Plot of permutation distributions for cheat/lie results with observed value of 13.24 (bold, vertical line)." width="480" />
<p class="caption">
Figure 5.22: Plot of permutation distributions for cheat/lie results with observed value of 13.24 (bold, vertical line).
</p>
</div>
<ul>
<li>There were 170 of <span class="math inline">\(B\)</span>=10,000 permuted data sets that produced as large or larger <span class="math inline">\(X^{2*}\text{&#39;s}\)</span> than the observed as displayed in Figure <a href="chapter5.html#fig:Figure5-22">5.22</a>, so we report that the p-value was 0.017 using the permutation approach, which was slightly larger than the result provided by the parametric method.</li>
</ul></li>
<li><p><strong>Make a decision:</strong></p>
<ul>
<li>With a p-value of 0.017, we can say that there is a 1.7% chance of observing a configuration like ours or more extreme if the null hypothesis is true. So I would say that there is strong evidence against the null hypothesis but you might rate this as closer to moderate evidence.</li>
</ul></li>
<li><p><strong>Write a conclusion and scope of inference:</strong></p>
<ul>
<li>There is strong evidence to conclude that there is a relationship between lying and copying behavior in the population of students. There is no causal inference possible here since neither variable was randomly assigned (really neither is explanatory or response here either) but we can extend the inferences to the population of students that these were selected from that would be willing to reveal their GPA (see initial discussion related to some differences in students that wouldn’t answer that question).</li>
</ul></li>
</ol>
<p>The standardized residuals can help us more fully understand this result – the mosaic plot only had one cell shaded and so wasn’t needed here.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(cheatlietable)<span class="op">$</span>residuals</code></pre></div>
<pre><code>##              copier2
## liar2               None ExamorPaper   CopyBoth
##   None         0.4803220  -0.3563200 -1.3688609
##   ExamorPaper -0.8048695   0.5232734  2.4759378
##   LieBoth     -0.7047165   0.6271633  1.7507524</code></pre>
<p>There is really only one large standardized residual for the <em>ExamorPaper</em> liars and the <em>CopyBoth</em> copiers, with a much larger observed value than expected of 2.48. The only other medium-sized standardized residuals came from the <em>CopyBoth</em> copiers column with fewer than expected students in the <em>None</em> category and more than expected in the <em>LieBoth</em> type of lying category. So we are seeing more than expected that lied somehow and copied – we can say this suggests that the students who lie tend to copy too!</p>
</div>
<div id="section5-11" class="section level2">
<h2><span class="header-section-number">5.11</span> Analyzing a stratified random sample of California schools</h2>
<p>In recent decades, there has been a push for quantification of school performance and tying financial punishment and rewards to growth in these metrics. One example is the API (Academic Performance Index) in California that is based mainly on student scores on standardized tests. It ranges between 200 and 1000 and year to year changes are of interest to assess “performance” of schools – calculated as one year minus the previous year (negative “growth” is also possible!). Suppose that a researcher is interested in whether the growth metric might differ between different levels of schools. Maybe it is easier or harder for elementary, middle, or high schools to attain growth? The researcher has a list of most of the schools in the state of each level that are using a database that the researcher has access to. In order to assess this question, the researcher takes a stratified random sample<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a>, selecting <span class="math inline">\(n_{\text{elementary}}=100\)</span> schools from the population of 4421 elementary schools, <span class="math inline">\(n_{\text{middle}}=50\)</span> from the population of 1018 middle schools, and <span class="math inline">\(n_{\text{high}}=50\)</span> from the population of 755 high schools. These data are available in the <code>survey</code> package <span class="citation">(Lumley <a href="#ref-R-survey">2018</a>)</span> and the api data object that loads both <code>apipop</code> (population) and <code>apistrat</code> (stratified random sample) data sets. The <code>growth</code> (change!) in API scores for the schools between 1999 and 2000 (taken as the year 2000 score minus 1999 score) is used as the response variable. The boxplot and beanplot of the growth scores are displayed in Figure <a href="chapter5.html#fig:Figure5-23">5.23</a>. They suggest some differences in the growth rates among the different levels. There are also a few schools flagged as being possible outliers.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(survey)
<span class="kw">data</span>(api)
apistrat &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(apistrat)
apipop &lt;-<span class="st"> </span><span class="kw">as.tibble</span>(apipop)
<span class="kw">tally</span>(<span class="op">~</span>stype, <span class="dt">data=</span>apipop) <span class="co">#Population counts</span></code></pre></div>
<pre><code>## stype
##    E    H    M 
## 4421  755 1018</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tally</span>(<span class="op">~</span>stype, <span class="dt">data=</span>apistrat) <span class="co">#Sample counts</span></code></pre></div>
<pre><code>## stype
##   E   H   M 
## 100  50  50</code></pre>

<div class="figure"><span id="fig:Figure5-23"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-23-1.png" alt="Boxplot and beanplot of the API growth scores by level of school in the stype variable (coded E for elementary, M for Middle, and H for High school)." width="576" />
<p class="caption">
Figure 5.23: Boxplot and beanplot of the API growth scores by level of school in the <code>stype</code> variable (coded E for elementary, M for Middle, and H for High school).
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">boxplot</span>(growth<span class="op">~</span>stype, <span class="dt">data=</span>apistrat, <span class="dt">ylab=</span><span class="st">&quot;Growth&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>,<span class="dv">160</span>))
<span class="kw">beanplot</span>(growth<span class="op">~</span>stype, <span class="dt">data=</span>apistrat, <span class="dt">log=</span><span class="st">&quot;&quot;</span>, <span class="dt">col=</span><span class="st">&quot;beige&quot;</span>,
         <span class="dt">method=</span><span class="st">&quot;jitter&quot;</span>, <span class="dt">ylim=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">55</span>,<span class="dv">160</span>))</code></pre></div>
<p>The One-Way ANOVA <span class="math inline">\(F\)</span>-test, provided below, suggests evidence of some difference in the true mean growth scores among the different types of schools (<span class="math inline">\(F(2,197)=23.56,\text{ p-value}&lt;0.0001\)</span>). But the residuals from this model displayed in the QQ-Plot in Figure <a href="chapter5.html#fig:Figure5-24">5.24</a> contain a slightly long right tail, suggesting a right skewed distribution for the residuals. In a high-stakes situation such as this, reporting results with violations of the assumptions probably would not be desirable, so another approach is needed. The permutation methods would be justified here but there is another “simpler” option available using our new Chi-square analysis methods.</p>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lm</span>(growth<span class="op">~</span>stype, <span class="dt">data=</span>apistrat)
<span class="kw">require</span>(car)
<span class="kw">Anova</span>(m1)</code></pre></div>
<pre><code>## Anova Table (Type II tests)
## 
## Response: growth
##           Sum Sq  Df F value    Pr(&gt;F)
## stype      30370   2  23.563 6.685e-10
## Residuals 126957 197</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(m1, <span class="dt">which=</span><span class="dv">2</span>)</code></pre></div>
<div class="figure"><span id="fig:Figure5-24"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-24-1.png" alt="QQ-plot of standardized residuals from the One-Way ANOVA linear model." width="528" />
<p class="caption">
Figure 5.24: QQ-plot of standardized residuals from the One-Way ANOVA linear model.
</p>
</div>
<p>One way to get around the normality assumption is to use a method that does not assume the responses follow a normal distribution. If we <strong><em>bin</em></strong> or cut the quantitative response variable into a set of ordered categories and apply a Chi-square test, we can proceed without concern about the lack of normality in the residuals of the ANOVA model. To create these bins, a simple idea would be to use the quartiles to generate the response variable categories, binning the quantitative responses into groups for the lowest 25%, second 25%, third 25%, and highest 25% by splitting the data at <span class="math inline">\(Q_1\)</span>, the Median, and <span class="math inline">\(Q_3\)</span>. In R, the <code>cut</code> function is available to turn a quantitative variable into a categorical variable. First, we can use the information from <code>favstats</code> to find the cut-points:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">favstats</span>(<span class="op">~</span>growth, <span class="dt">data=</span>apistrat)</code></pre></div>
<pre><code>##  min   Q1 median Q3 max   mean      sd   n missing
##  -47 6.75     25 48 133 27.995 28.1174 200       0</code></pre>
<p>The <code>cut</code> function can provide the binned variable if it is provided with the end-points of the desired intervals to create new categories with those names in a new variable called <code>growthcut</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">apistrat<span class="op">$</span>growthcut &lt;-<span class="st"> </span><span class="kw">cut</span>(apistrat<span class="op">$</span>growth, <span class="dt">breaks=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">47</span>,<span class="fl">6.75</span>,<span class="dv">25</span>,<span class="dv">48</span>,<span class="dv">133</span>),
                          <span class="dt">include.lowest=</span>T)</code></pre></div>

<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tally</span>(<span class="op">~</span>growthcut, <span class="dt">data=</span>apistrat)</code></pre></div>
<pre><code>## growthcut
## [-47,6.75]  (6.75,25]    (25,48]   (48,133] 
##         50         52         49         49</code></pre>
<p>Now that we have a categorical response variable, we need to decide which sort of Chi-squared analysis to perform. The sampling design determines the correct analysis as always in these situations. The stratified random sample involved samples from each of the three populations so a Homogeneity test should be employed. In these situations, the stacked bar chart provides the appropriate summary of the data. It also shows us the labels of the categories that the <code>cut</code> function created in the new <code>growthcut</code> variable:</p>

<div class="figure"><span id="fig:Figure5-25"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-25-1.png" alt="Stacked bar chart of the growth category responses by level of school." width="576" />
<p class="caption">
Figure 5.25: Stacked bar chart of the growth category responses by level of school.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(growthcut<span class="op">~</span>stype, <span class="dt">data=</span>apistrat)</code></pre></div>
<p>Figure <a href="chapter5.html#fig:Figure5-25">5.25</a> suggests that the distributions of growth scores may not be the same across the levels of the schools with many more high growth <em>Elementary</em> schools than in either the <em>Middle</em> or <em>High</em> school groups (the “high” growth category is labeled as (48, 133] providing the interval of growth scores placed in this category). Similarly, the proportion of the low or negative growth (category of (-47.6, 6.75] for “growth” between -47.6 and 6.75) is least frequently occurring in <em>Elementary</em> schools and most frequent in the <em>High</em> schools. Statisticians often work across many disciplines and so may not always have the subject area knowledge to know why these differences exist (just like you might not), but an education researcher could take this sort of information – because it is a useful summary of interesting school-level data – and generate further insights into why growth in the API metric may or may not be a good or fair measure of school performance.</p>
<p>Of course, we want to consider whether these results can extend to the population of all California schools. The homogeneity hypotheses for assessing the growth rate categories across the types of schools would be:</p>
<ul>
<li><p><span class="math inline">\(H_0\)</span>: There is no difference in the distribution of growth categories across the three levels of schools in the population of California schools.</p></li>
<li><p><span class="math inline">\(H_A\)</span>: There is some difference in the distribution of growth categories across the three levels of schools in the population of California schools.</p></li>
</ul>
<p>There might be an issue with the independence assumption in that schools within the same district might be more similar to one another and different between one another. Sometimes districts are accounted for in education research to account for differences in policies and demographics among the districts. We could explore this issue by finding district-level average growth rates and exploring whether those vary systematically but this is beyond the scope of the current exploration.</p>
<p>Checking the expected cell counts gives insight into the assumption for using the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution to find the p-value:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">growthtable &lt;-<span class="st"> </span><span class="kw">tally</span>(<span class="op">~</span>stype<span class="op">+</span>growthcut, <span class="dt">data=</span>apistrat)
growthtable</code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75] (6.75,25] (25,48] (48,133]
##     E         14        22      27       37
##     H         24        18       5        3
##     M         12        12      17        9</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(growthtable)<span class="op">$</span>expected </code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75] (6.75,25] (25,48] (48,133]
##     E       25.0        26   24.50    24.50
##     H       12.5        13   12.25    12.25
##     M       12.5        13   12.25    12.25</code></pre>
<p>The smallest expected count is 12.25, occurring in four different cells, so the assumptions for using the parametric approach are met.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(growthtable) </code></pre></div>
<pre><code>## 
##  Pearson&#39;s Chi-squared test
## 
## data:  growthtable
## X-squared = 38.668, df = 6, p-value = 8.315e-07</code></pre>
<p>The observed test statistic is <span class="math inline">\(X^2=38.67\)</span> and, based on a <span class="math inline">\(\boldsymbol{\chi^2}(6)\)</span> distribution, the p-value is 0.0000008. This p-value suggests that there is very strong evidence of some difference in the distribution of API growth of schools among <em>Elementary</em>, <em>Middle</em> and <em>High School</em> in the population of schools in California between 1999 and 2000. So we can conclude that there is very strong evidence of some difference in the population (California schools) because the schools were randomly selected from all the California schools but because the level of schools, obviously, cannot be randomly assigned, we cannot say that level of school causes these differences.</p>
<p>The standardized residuals can enhance this interpretation, displayed in Figure <a href="chapter5.html#fig:Figure5-26">5.26</a>. The <em>Elementary</em> schools have fewer low/negative growth schools and more high growth schools than expected under the null hypothesis. The <em>High</em> schools have more low growth and fewer higher growth (growth over 25 points) schools than expected if there were no difference in patterns of response across the school levels. The <em>Middle</em> school results were closer to the results expected if there were no differences across the school levels.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">chisq.test</span>(growthtable)<span class="op">$</span>residuals</code></pre></div>
<pre><code>##      growthcut
## stype [-47,6.75]  (6.75,25]    (25,48]   (48,133]
##     E -2.2000000 -0.7844645  0.5050763  2.5253814
##     H  3.2526912  1.3867505 -2.0714286 -2.6428571
##     M -0.1414214 -0.2773501  1.3571429 -0.9285714</code></pre>

<div class="figure"><span id="fig:Figure5-26"></span>
<img src="05-chiSquaredTests_files/figure-html/Figure5-26-1.png" alt="Mosaic plot of the API Growth rate categories versus level of the school with shading for size of standardized residuals." width="576" />
<p class="caption">
Figure 5.26: Mosaic plot of the API Growth rate categories versus level of the school with shading for size of standardized residuals.
</p>
</div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">mosaicplot</span>(growthcut<span class="op">~</span>stype,<span class="dt">data=</span>apistrat,<span class="dt">shade=</span>T)</code></pre></div>
<p>The binning of quantitative variables is not a first step in analyses – the quantitative version is almost always preferable. However, this analysis avoided the violation of the normality assumption that was somewhat problematic for the ANOVA and still provided useful inferences to the differences in the types of schools. When one goes from a quantitative to categorical version of a variable, one loses information (the specific details of the quantitative responses within each level created) and this almost always will result in a loss of statistical power of the procedure. In this situation, the p-value from the ANOVA was of the order <span class="math inline">\(10^{-10}\)</span> while the Chi-square test had a p-value of order <span class="math inline">\(10^{-7}\)</span>. This larger p-value is typical of the loss of power in going to a categorical response when more information was available. In many cases, there are no options but to use contingency table analyses. This example shows that there might be some situations where “going categorical” could be an acceptable method for handing situations where an assumption is violated.</p>

</div>
<div id="section5-12" class="section level2">
<h2><span class="header-section-number">5.12</span> Chapter summary</h2>
<p>Chi-square tests can be generally used to perform two types of tests, the Independence and Homogeneity tests. The appropriate analysis is determined based on the data collection methodology. The parametric Chi-squared distribution for which these tests are named is appropriate when the expected cell counts are large enough (related to having a large enough overall sample). When this condition is violated, the permutation approach can provide valuable inferences in these situations in most situations.</p>
<p>Data displays of the stacked barchart (Homogeneity) and mosaic plots (Independence) provide a visual summary of the results that can also be found in contingency tables. You should have learned how to calculate the <span class="math inline">\(X^2\)</span> (X-squared) test statistic based on first finding the expected cell counts. Under certain assumptions, it will follow a Chi-Squared distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom. When those assumptions are not met, it is better to use a permutation approach to find p-values. Either way, the same statistic is used to test either kind of hypothesis, independence or homogeneity. After assessing evidence against the null hypothesis, it is interesting to see which cells in the table contributed to the deviations from the null hypothesis. The standardized residuals provide that information. Graphing them in a mosaic plot makes for a fun display to identify the large residuals and often allows you to better understand the results. This should tie back into the original data display and contingency table where you identified initial patterns and help to tell the story of the results.</p>

</div>
<div id="section5-13" class="section level2">
<h2><span class="header-section-number">5.13</span> Summary of important R commands</h2>
<p>The main components of R code used in this chapter follow with components to modify in lighter and/or ALL CAPS text where <code>y</code> is a response variable and <code>x</code> is a predictor are easily identified:</p>
<ul>
<li><p><strong><font color='red'>TABLENAME</font> <code>&lt;-</code> tally(~<font color='red'>x</font> + <font color='red'>y</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li><p>This function requires that the <code>mosaic</code> package has been loaded.</p></li>
<li><p>This provides a table of the counts in the variable called <code>TABLENAME</code>.</p></li>
<li><p><code>margins=T</code> is used if want to display row, column, and table totals.</p></li>
</ul></li>
<li><p><strong>plot(<font color='red'>y</font>~ <font color='red'>x</font>, data=<font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Makes a stacked bar chart useful for homogeneity test situations.</li>
</ul></li>
<li><p><strong>mosaicplot(<font color='red'>TABLENAME</font>)</strong></p>
<ul>
<li>Makes a mosaic plot useful for finding patterns in the table in independence test situations.</li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)</strong></p>
<ul>
<li>Provides <span class="math inline">\(X^2\)</span> and p-values based on the <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom.</li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$expected</strong></p>
<ul>
<li>Provides expected cell counts.</li>
</ul></li>
<li><p><strong>pchisq(<font color='red'>X-SQUARED</font>, df=(<font color='red'>R</font> - 1)<code>*</code>(<font color='red'>C</font> - 1), lower.tail=F)</strong></p>
<ul>
<li><p>Provides p-value from <span class="math inline">\(\boldsymbol{\chi^2}\)</span>-distribution with <span class="math inline">\((R-1)(C-1)\)</span> degrees of freedom for observed test statistic.</p></li>
<li><p>See Section <a href="chapter5.html#section5-5">5.5</a> for code related to finding a permutation-based p-value.</p></li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$residuals^2</strong></p>
<ul>
<li>Provides <span class="math inline">\(X^2\)</span> contributions from each cell in table.</li>
</ul></li>
<li><p><strong>chisq.test(<font color='red'>TABLENAME</font>)$residuals</strong></p>
<ul>
<li>Provides standardized residuals.</li>
</ul></li>
<li><p><strong>mosaicplot(<font color='red'>TABLENAME</font>, shade=T)</strong></p>
<ul>
<li>Provides a mosaic plot with shading based on standardized residuals.</li>
</ul></li>
</ul>
</div>
<div id="section5-14" class="section level2">
<h2><span class="header-section-number">5.14</span> Practice problems</h2>
<p>Determine which type of test is appropriate in each situation – <strong><em>Independence</em></strong> or <strong><em>Homogeneity</em></strong>?</p>
<p>5.1. Concerns over diseases being transmitted between birds and humans have led to many areas developing monitoring plans for the birds that are in their regions. The duck pond on campus at MSU-Bozeman is a bit like a night club for the birds that pass through Bozeman.</p>
<ol style="list-style-type: lower-roman">
<li><p>Suppose that a researcher randomly samples 20 ducks at the duck pond on campus on 4 different occasions and records the number ducks that are healthy and number that are sick on each day. The variables in this study are the day of measurement and sick/healthy.</p></li>
<li><p>In another monitoring study, a researcher goes to a wetland area and collects a random sample from all birds present on a single day, classifies them by type of bird (ducks, swans, etc.) and then assesses whether each is sick or healthy. The variables in this study are type of bird and sick/healthy.</p></li>
</ol>
<p>5.2. Psychologists performed an experiment on 48 male bank supervisors attending a management institute to investigate biases against women in personnel decisions. The supervisors were asked to make a decision on whether to promote a hypothetical applicant based on a personnel file. For half of them, the application file described a female candidate; for the others it described a male.</p>
<p>5.3. Researchers collected data on death penalty sentencing in Georgia. For 243 crimes, they categorized the crime by severity from 1 to 6 with Category 1 comprising barroom brawls, liquor-induced arguments, lovers’ quarrels, and similar crimes and Category 6 including the most vicious, cruel, cold-blooded, unprovoked crimes. They also recorded the perpetrator’s race. They wanted to know if there was a relationship between race and type of crime.</p>
<p>5.4. Epidemiologists want to see if Vitamin C helped people with colds. They would like to give some patients Vitamin C and some a placebo then compare the two groups. However, they are worried that the placebo might not be working. Since vitamin C has such a distinct taste, they are worried the participants will know which group they are in. To test if the placebo was working, they collected 200 subjects and randomly assigned half to take a placebo and the other half to take Vitamin C. 30 minutes later, they asked the subjects which supplement they received (hoping that the patients would not know which group they were assigned to).</p>
<p>5.5. Is zodiac sign related to GPA? 300 randomly selected students from MSU were asked their birthday and their current GPA. GPA was then categorized as &lt; 1.50 = F, 1.51-2.50 = D, 2.51 - 3.25 = C, 3.26-3.75 = B, 3.76-4.0 = A and their birthday was used to find their zodiac sign.</p>
<p>5.6. In 1935, the statistician R. A. Fisher famously had a colleague claim that she could distinguish whether milk or tea was added to a cup first. Fisher presented her, in a random order, 4 cups that were filled with milk first and 4 cups that were filled with tea first.</p>
<p>5.7. Researchers wanted to see if people from Rural and Urban areas aged differently. They contacted 200 people from Rural areas and 200 people from Urban areas and asked the participants their age (&lt;40, 41-50, 51-60, &gt;60).</p>
<p>The <a href="https://fivethirtyeight.com/">FiveThirtyEight Blog</a> often shows up with interesting data summaries that have general public appeal. Their staff includes a bunch of quants with various backgrounds. When starting their blog, they had to decide on the data is/are question that we introduced in Section <a href="chapter2.html#section2-1">2.1</a>. To help them think about this, they collected a nationally representative sample that contained three questions about this. Based on their survey, they concluded that</p>
<blockquote>
<p>Relevant to the <a href="http://fivethirtyeight.com/datalab/data-is-vs-data-are/">interests of FiveThirtyEight</a> in particular, we also asked whether people preferred using “data” as a singular or plural noun. To those who prefer the plural, I’ll put this in your terms: The data are pretty conclusive that the vast majority of respondents think we should say “data is.” The singular crowd won by a 58 percentage-point margin, with 79 percent of respondents liking “data is” to 21 percent preferring “data are.” But only half of respondents had put any thought to the usage prior to our survey, so it seems that it’s not a pressing issue for most.</p>
</blockquote>
<p>This came from a survey that contained questions about <em>which is the correct usage</em>, (<code>isare</code>), <em>have you thought about this issue</em> (<code>thoughtabout</code>) with levels Yes/No, and <em>do you care about this issue</em> (<code>careabout</code>) with four levels from <em>Not at all</em> to <em>A lot</em>. The following code loads their data set after missing responses were removed, does a little re-ordering of factor levels to help make the results easier to understand, and makes a table plot to get a general sense of the results including information on the respondents’ gender, age, income, and education.</p>
<pre><code>csd &lt;- read_csv(&quot;http://www.math.montana.edu/courses/s217/documents/csd.csv&quot;)
require(tabplot)
#Need to make it explicit that these are factor variables
csd$careabout &lt;- factor(csd$careabout) 
#Reorders factor levels to be in &quot;correct&quot; order
csd$careabout &lt;- factor(csd$careabout,
                    levels=levels(csd$careabout)[c(1,4,3,2)]) 
csd$Education &lt;- factor(csd$Education)
csd$Education &lt;- factor(csd$Education,
                    levels=levels(csd$Education)[c(4,3,5,1,2)])
csd$Household.Income &lt;- factor(csd$Household.Income)
csd$Household.Income &lt;- factor(csd$Household.Income,
                    levels=levels(csd$Household.Income)[c(1,4,5,6,2,3)])
#Sorts plot by careabout responses
tableplot(csd[,c(&quot;isare&quot;,&quot;careabout&quot;,&quot;thoughtabout&quot;,&quot;Gender&quot;,
                 &quot;Age&quot;,&quot;Household.Income&quot;,&quot;Education&quot;)], sortCol=careabout) </code></pre>
<p>5.8. If we are interested in the variables <code>isare</code> and <code>careabout</code>, what sort of test should we perform?</p>
<p>5.9. Make the appropriate plot of the results for the table relating those two variables relative to your answer to 5.8.</p>
<p>5.10. Generate the contingency table and find the expected cell counts, first “by hand” and then check them using the output. Is the parametric procedure appropriate here? Why or why not?</p>
<p>5.11. Report the value of the test statistic, its distribution under the null, the parametric p-value, and write a decision and conclusion, making sure to address scope of inference.</p>
<p>5.12. Make a mosaic plot with the standardized residuals and discuss the results. Specifically, in what way do the is/are preferences move away from the null hypothesis for people that care more about this?</p>
<table>
<tbody>
<tr class="odd">
<td>We might be fighting a losing battle on this particular word usage, but since we</td>
</tr>
<tr class="even">
<td>are in the group that cares a lot about this, we are going</td>
</tr>
<tr class="odd">
<td>to keep trying…</td>
</tr>
</tbody>
</table>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-R-vcd">
<p>Meyer, David, Achim Zeileis, and Kurt Hornik. 2017. <em>Vcd: Visualizing Categorical Data</em>. <a href="https://CRAN.R-project.org/package=vcd" class="uri">https://CRAN.R-project.org/package=vcd</a>.</p>
</div>
<div id="ref-R-tabplot">
<p>Tennekes, Martijn, and Edwin de Jonge. 2017. <em>Tabplot: Tableplot, a Visualization of Large Datasets</em>. <a href="https://CRAN.R-project.org/package=tabplot" class="uri">https://CRAN.R-project.org/package=tabplot</a>.</p>
</div>
<div id="ref-R-poLCA">
<p>Linzer, Drew, and Jeffrey Lewis. 2014. <em>PoLCA: Polytomous Variable Latent Class Analysis</em>. <a href="https://CRAN.R-project.org/package=poLCA" class="uri">https://CRAN.R-project.org/package=poLCA</a>.</p>
</div>
<div id="ref-Linzer2011">
<p>Linzer, Drew, and Jeffrey Lewis. 2011. “PoLCA: An R Package for Polytomous Variable Latent Class Analysis.” <em>Journal of Statistical Software</em> 42 (10): 1–29.</p>
</div>
<div id="ref-Dayton1998">
<p>Dayton, C. Mitchell. 1998. <em>Latent Class Scaling Analysis</em>. Thousand Oaks, CA: SAGE Publications.</p>
</div>
<div id="ref-R-survey">
<p>Lumley, Thomas. 2018. <em>Survey: Analysis of Complex Survey Samples</em>. <a href="https://CRAN.R-project.org/package=survey" class="uri">https://CRAN.R-project.org/package=survey</a>.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="61">
<li id="fn61"><p>In larger data sets, multiple subjects are displayed in each row as proportions of the rows in each category.<a href="chapter5.html#fnref61">↩</a></p></li>
<li id="fn62"><p>Quantitative variables are displayed with boxplot-like bounds to describe the variability in the variable for that row of responses for larger data sets.<a href="chapter5.html#fnref62">↩</a></p></li>
<li id="fn63"><p>While randomization is typically useful in trying to “equalize” the composition of groups, a possible randomization of subjects to the groups is to put all the males into the treatment group. Sometimes we add additional constraints to randomization of subjects to treatments to guarantee that we don’t get stuck with an unusual and highly unlikely assignment like that. It is important at least to check the demographics of different treatment groups to see if anything odd occurred.<a href="chapter5.html#fnref63">↩</a></p></li>
<li id="fn64"><p>The vertical line, “<code>|</code>”, in <code>~ y|x</code> is available on most keyboards on the same key as “<code>\</code>”. It is the mathematical symbol that means “conditional on” whatever follows.<a href="chapter5.html#fnref64">↩</a></p></li>
<li id="fn65"><p>Standardizing involves dividing by the standard deviation of a quantity so it has a standard deviation 1 regardless of its original variability and that is what is happening here even though it doesn’t look like the standardization you are used to with continuous variables.<a href="chapter5.html#fnref65">↩</a></p></li>
<li id="fn66"><p>Here it allows us to relax a requirement that all the expected cell counts are larger than 5 for the parametric test (Section <a href="chapter5.html#section5-6">5.6</a>).<a href="chapter5.html#fnref66">↩</a></p></li>
<li id="fn67"><p>Independence tests can’t be causal by their construction. Homogeneity tests could be causal or just associational, depending on how the subjects ended up in the groups.<a href="chapter5.html#fnref67">↩</a></p></li>
<li id="fn68"><p>A stratified random sample involves taking a simple random sample from each group or strata of the population. It is useful to make sure that each group is represented at a chosen level (for example the sample proportion of the total size). If a simple random sample of all schools had been taken, it is possible that a level could have no schools selected.<a href="chapter5.html#fnref68">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter4.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter6.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf", "Greenwood_Book.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
