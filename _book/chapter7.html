<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Simple linear regression inference | Intermediate Statistics with R</title>
  <meta name="description" content="Chapter 7 Simple linear regression inference | Intermediate Statistics with R" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Simple linear regression inference | Intermediate Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta name="github-repo" content="gpeterson406/Greenwood_Book" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Simple linear regression inference | Intermediate Statistics with R" />
  
  
  

<meta name="author" content="Mark C Greenwood" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter6.html"/>
<link rel="next" href="chapter8.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
<link rel="stylesheet" href="toc.css" type="text/css" />
<link rel="stylesheet" href="font-awesome.min.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Intermediate Statistics with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Cover</a></li>
<li class="chapter" data-level="" data-path="acknowledgments.html"><a href="acknowledgments.html"><i class="fa fa-check"></i>Acknowledgments</a></li>
<li class="chapter" data-level="1" data-path="chapter1.html"><a href="chapter1.html"><i class="fa fa-check"></i><b>1</b> Preface</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter1.html"><a href="chapter1.html#section1-1"><i class="fa fa-check"></i><b>1.1</b> Overview of methods</a></li>
<li class="chapter" data-level="1.2" data-path="chapter1.html"><a href="chapter1.html#section1-2"><i class="fa fa-check"></i><b>1.2</b> Getting started in R</a></li>
<li class="chapter" data-level="1.3" data-path="chapter1.html"><a href="chapter1.html#section1-3"><i class="fa fa-check"></i><b>1.3</b> Basic summary statistics, histograms, and boxplots using R</a></li>
<li class="chapter" data-level="1.4" data-path="chapter1.html"><a href="chapter1.html#section1-4"><i class="fa fa-check"></i><b>1.4</b> R Markdown</a></li>
<li class="chapter" data-level="1.5" data-path="chapter1.html"><a href="chapter1.html#section1-5"><i class="fa fa-check"></i><b>1.5</b> Grammar of Graphics</a></li>
<li class="chapter" data-level="1.6" data-path="chapter1.html"><a href="chapter1.html#section1-6"><i class="fa fa-check"></i><b>1.6</b> Exiting RStudio</a></li>
<li class="chapter" data-level="1.7" data-path="chapter1.html"><a href="chapter1.html#section1-7"><i class="fa fa-check"></i><b>1.7</b> Chapter summary</a></li>
<li class="chapter" data-level="1.8" data-path="chapter1.html"><a href="chapter1.html#section1-8"><i class="fa fa-check"></i><b>1.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="1.9" data-path="chapter1.html"><a href="chapter1.html#section1-9"><i class="fa fa-check"></i><b>1.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter2.html"><a href="chapter2.html"><i class="fa fa-check"></i><b>2</b> (R)e-Introduction to statistics</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter2.html"><a href="chapter2.html#section2-1"><i class="fa fa-check"></i><b>2.1</b> Data wrangling and density curves</a></li>
<li class="chapter" data-level="2.2" data-path="chapter2.html"><a href="chapter2.html#section2-2"><i class="fa fa-check"></i><b>2.2</b> Pirate-plots</a></li>
<li class="chapter" data-level="2.3" data-path="chapter2.html"><a href="chapter2.html#section2-3"><i class="fa fa-check"></i><b>2.3</b> Models, hypotheses, and permutations for the two sample mean situation</a></li>
<li class="chapter" data-level="2.4" data-path="chapter2.html"><a href="chapter2.html#section2-4"><i class="fa fa-check"></i><b>2.4</b> Permutation testing for the two sample mean situation</a></li>
<li class="chapter" data-level="2.5" data-path="chapter2.html"><a href="chapter2.html#section2-5"><i class="fa fa-check"></i><b>2.5</b> Hypothesis testing (general)</a></li>
<li class="chapter" data-level="2.6" data-path="chapter2.html"><a href="chapter2.html#section2-6"><i class="fa fa-check"></i><b>2.6</b> Connecting randomization (nonparametric) and parametric tests</a></li>
<li class="chapter" data-level="2.7" data-path="chapter2.html"><a href="chapter2.html#section2-7"><i class="fa fa-check"></i><b>2.7</b> Second example of permutation tests</a></li>
<li class="chapter" data-level="2.8" data-path="chapter2.html"><a href="chapter2.html#section2-8"><i class="fa fa-check"></i><b>2.8</b> Reproducibility Crisis: Moving beyond p &lt; 0.05, publication bias, and multiple testing issues</a></li>
<li class="chapter" data-level="2.9" data-path="chapter2.html"><a href="chapter2.html#section2-9"><i class="fa fa-check"></i><b>2.9</b> Confidence intervals and bootstrapping</a></li>
<li class="chapter" data-level="2.10" data-path="chapter2.html"><a href="chapter2.html#section2-10"><i class="fa fa-check"></i><b>2.10</b> Bootstrap confidence intervals for difference in GPAs</a></li>
<li class="chapter" data-level="2.11" data-path="chapter2.html"><a href="chapter2.html#section2-11"><i class="fa fa-check"></i><b>2.11</b> Chapter summary</a></li>
<li class="chapter" data-level="2.12" data-path="chapter2.html"><a href="chapter2.html#section2-12"><i class="fa fa-check"></i><b>2.12</b> Summary of important R code</a></li>
<li class="chapter" data-level="2.13" data-path="chapter2.html"><a href="chapter2.html#section2-13"><i class="fa fa-check"></i><b>2.13</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter3.html"><a href="chapter3.html"><i class="fa fa-check"></i><b>3</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter3.html"><a href="chapter3.html#section3-1"><i class="fa fa-check"></i><b>3.1</b> Situation</a></li>
<li class="chapter" data-level="3.2" data-path="chapter3.html"><a href="chapter3.html#section3-2"><i class="fa fa-check"></i><b>3.2</b> Linear model for One-Way ANOVA (cell means and reference-coding)</a></li>
<li class="chapter" data-level="3.3" data-path="chapter3.html"><a href="chapter3.html#section3-3"><i class="fa fa-check"></i><b>3.3</b> One-Way ANOVA Sums of Squares, Mean Squares, and F-test</a></li>
<li class="chapter" data-level="3.4" data-path="chapter3.html"><a href="chapter3.html#section3-4"><i class="fa fa-check"></i><b>3.4</b> ANOVA model diagnostics including QQ-plots</a></li>
<li class="chapter" data-level="3.5" data-path="chapter3.html"><a href="chapter3.html#section3-5"><i class="fa fa-check"></i><b>3.5</b> Guinea pig tooth growth One-Way ANOVA example</a></li>
<li class="chapter" data-level="3.6" data-path="chapter3.html"><a href="chapter3.html#section3-6"><i class="fa fa-check"></i><b>3.6</b> Multiple (pair-wise) comparisons using Tukey’s HSD and the compact letter display</a></li>
<li class="chapter" data-level="3.7" data-path="chapter3.html"><a href="chapter3.html#section3-7"><i class="fa fa-check"></i><b>3.7</b> Pair-wise comparisons for the Overtake data</a></li>
<li class="chapter" data-level="3.8" data-path="chapter3.html"><a href="chapter3.html#section3-8"><i class="fa fa-check"></i><b>3.8</b> Chapter summary</a></li>
<li class="chapter" data-level="3.9" data-path="chapter3.html"><a href="chapter3.html#section3-9"><i class="fa fa-check"></i><b>3.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="3.10" data-path="chapter3.html"><a href="chapter3.html#section3-10"><i class="fa fa-check"></i><b>3.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="chapter4.html"><a href="chapter4.html"><i class="fa fa-check"></i><b>4</b> Two-Way ANOVA</a>
<ul>
<li class="chapter" data-level="4.1" data-path="chapter4.html"><a href="chapter4.html#section4-1"><i class="fa fa-check"></i><b>4.1</b> Situation</a></li>
<li class="chapter" data-level="4.2" data-path="chapter4.html"><a href="chapter4.html#section4-2"><i class="fa fa-check"></i><b>4.2</b> Designing a two-way experiment and visualizing results</a></li>
<li class="chapter" data-level="4.3" data-path="chapter4.html"><a href="chapter4.html#section4-3"><i class="fa fa-check"></i><b>4.3</b> Two-Way ANOVA models and hypothesis tests</a></li>
<li class="chapter" data-level="4.4" data-path="chapter4.html"><a href="chapter4.html#section4-4"><i class="fa fa-check"></i><b>4.4</b> Guinea pig tooth growth analysis with Two-Way ANOVA</a></li>
<li class="chapter" data-level="4.5" data-path="chapter4.html"><a href="chapter4.html#section4-5"><i class="fa fa-check"></i><b>4.5</b> Observational study example: The Psychology of Debt</a></li>
<li class="chapter" data-level="4.6" data-path="chapter4.html"><a href="chapter4.html#section4-6"><i class="fa fa-check"></i><b>4.6</b> Pushing Two-Way ANOVA to the limit: Un-replicated designs and Estimability</a></li>
<li class="chapter" data-level="4.7" data-path="chapter4.html"><a href="chapter4.html#section4-7"><i class="fa fa-check"></i><b>4.7</b> Chapter summary</a></li>
<li class="chapter" data-level="4.8" data-path="chapter4.html"><a href="chapter4.html#section4-8"><i class="fa fa-check"></i><b>4.8</b> Summary of important R code</a></li>
<li class="chapter" data-level="4.9" data-path="chapter4.html"><a href="chapter4.html#section4-9"><i class="fa fa-check"></i><b>4.9</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="chapter5.html"><a href="chapter5.html"><i class="fa fa-check"></i><b>5</b> Chi-square tests</a>
<ul>
<li class="chapter" data-level="5.1" data-path="chapter5.html"><a href="chapter5.html#section5-1"><i class="fa fa-check"></i><b>5.1</b> Situation, contingency tables, and tableplots</a></li>
<li class="chapter" data-level="5.2" data-path="chapter5.html"><a href="chapter5.html#section5-2"><i class="fa fa-check"></i><b>5.2</b> Homogeneity test hypotheses</a></li>
<li class="chapter" data-level="5.3" data-path="chapter5.html"><a href="chapter5.html#section5-3"><i class="fa fa-check"></i><b>5.3</b> Independence test hypotheses</a></li>
<li class="chapter" data-level="5.4" data-path="chapter5.html"><a href="chapter5.html#section5-4"><i class="fa fa-check"></i><b>5.4</b> Models for R by C tables</a></li>
<li class="chapter" data-level="5.5" data-path="chapter5.html"><a href="chapter5.html#section5-5"><i class="fa fa-check"></i><b>5.5</b> Permutation tests for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.6" data-path="chapter5.html"><a href="chapter5.html#section5-6"><i class="fa fa-check"></i><b>5.6</b> Chi-square distribution for the <span class="math inline">\(X^2\)</span> statistic</a></li>
<li class="chapter" data-level="5.7" data-path="chapter5.html"><a href="chapter5.html#section5-7"><i class="fa fa-check"></i><b>5.7</b> Examining residuals for the source of differences</a></li>
<li class="chapter" data-level="5.8" data-path="chapter5.html"><a href="chapter5.html#section5-8"><i class="fa fa-check"></i><b>5.8</b> General protocol for <span class="math inline">\(X^2\)</span> tests</a></li>
<li class="chapter" data-level="5.9" data-path="chapter5.html"><a href="chapter5.html#section5-9"><i class="fa fa-check"></i><b>5.9</b> Political party and voting results: Complete analysis</a></li>
<li class="chapter" data-level="5.10" data-path="chapter5.html"><a href="chapter5.html#section5-10"><i class="fa fa-check"></i><b>5.10</b> Is cheating and lying related in students?</a></li>
<li class="chapter" data-level="5.11" data-path="chapter5.html"><a href="chapter5.html#section5-11"><i class="fa fa-check"></i><b>5.11</b> Analyzing a stratified random sample of California schools</a></li>
<li class="chapter" data-level="5.12" data-path="chapter5.html"><a href="chapter5.html#section5-12"><i class="fa fa-check"></i><b>5.12</b> Chapter summary</a></li>
<li class="chapter" data-level="5.13" data-path="chapter5.html"><a href="chapter5.html#section5-13"><i class="fa fa-check"></i><b>5.13</b> Summary of important R commands</a></li>
<li class="chapter" data-level="5.14" data-path="chapter5.html"><a href="chapter5.html#section5-14"><i class="fa fa-check"></i><b>5.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="chapter6.html"><a href="chapter6.html"><i class="fa fa-check"></i><b>6</b> Correlation and Simple Linear Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="chapter6.html"><a href="chapter6.html#section6-1"><i class="fa fa-check"></i><b>6.1</b> Relationships between two quantitative variables</a></li>
<li class="chapter" data-level="6.2" data-path="chapter6.html"><a href="chapter6.html#section6-2"><i class="fa fa-check"></i><b>6.2</b> Estimating the correlation coefficient</a></li>
<li class="chapter" data-level="6.3" data-path="chapter6.html"><a href="chapter6.html#section6-3"><i class="fa fa-check"></i><b>6.3</b> Relationships between variables by groups</a></li>
<li class="chapter" data-level="6.4" data-path="chapter6.html"><a href="chapter6.html#section6-4"><i class="fa fa-check"></i><b>6.4</b> Inference for the correlation coefficient</a></li>
<li class="chapter" data-level="6.5" data-path="chapter6.html"><a href="chapter6.html#section6-5"><i class="fa fa-check"></i><b>6.5</b> Are tree diameters related to tree heights?</a></li>
<li class="chapter" data-level="6.6" data-path="chapter6.html"><a href="chapter6.html#section6-6"><i class="fa fa-check"></i><b>6.6</b> Describing relationships with a regression model</a></li>
<li class="chapter" data-level="6.7" data-path="chapter6.html"><a href="chapter6.html#section6-7"><i class="fa fa-check"></i><b>6.7</b> Least Squares Estimation</a></li>
<li class="chapter" data-level="6.8" data-path="chapter6.html"><a href="chapter6.html#section6-8"><i class="fa fa-check"></i><b>6.8</b> Measuring the strength of regressions: R<sup>2</sup></a></li>
<li class="chapter" data-level="6.9" data-path="chapter6.html"><a href="chapter6.html#section6-9"><i class="fa fa-check"></i><b>6.9</b> Outliers: leverage and influence</a></li>
<li class="chapter" data-level="6.10" data-path="chapter6.html"><a href="chapter6.html#section6-10"><i class="fa fa-check"></i><b>6.10</b> Residual diagnostics – setting the stage for inference</a></li>
<li class="chapter" data-level="6.11" data-path="chapter6.html"><a href="chapter6.html#section6-11"><i class="fa fa-check"></i><b>6.11</b> Old Faithful discharge and waiting times</a></li>
<li class="chapter" data-level="6.12" data-path="chapter6.html"><a href="chapter6.html#section6-12"><i class="fa fa-check"></i><b>6.12</b> Chapter summary</a></li>
<li class="chapter" data-level="6.13" data-path="chapter6.html"><a href="chapter6.html#section6-13"><i class="fa fa-check"></i><b>6.13</b> Summary of important R code</a></li>
<li class="chapter" data-level="6.14" data-path="chapter6.html"><a href="chapter6.html#section6-14"><i class="fa fa-check"></i><b>6.14</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="chapter7.html"><a href="chapter7.html"><i class="fa fa-check"></i><b>7</b> Simple linear regression inference</a>
<ul>
<li class="chapter" data-level="7.1" data-path="chapter7.html"><a href="chapter7.html#section7-1"><i class="fa fa-check"></i><b>7.1</b> Model</a></li>
<li class="chapter" data-level="7.2" data-path="chapter7.html"><a href="chapter7.html#section7-2"><i class="fa fa-check"></i><b>7.2</b> Confidence interval and hypothesis tests for the slope and intercept</a></li>
<li class="chapter" data-level="7.3" data-path="chapter7.html"><a href="chapter7.html#section7-3"><i class="fa fa-check"></i><b>7.3</b> Bozeman temperature trend</a></li>
<li class="chapter" data-level="7.4" data-path="chapter7.html"><a href="chapter7.html#section7-4"><i class="fa fa-check"></i><b>7.4</b> Randomization-based inferences for the slope coefficient</a></li>
<li class="chapter" data-level="7.5" data-path="chapter7.html"><a href="chapter7.html#section7-5"><i class="fa fa-check"></i><b>7.5</b> Transformations part I: Linearizing relationships</a></li>
<li class="chapter" data-level="7.6" data-path="chapter7.html"><a href="chapter7.html#section7-6"><i class="fa fa-check"></i><b>7.6</b> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)</a></li>
<li class="chapter" data-level="7.7" data-path="chapter7.html"><a href="chapter7.html#section7-7"><i class="fa fa-check"></i><b>7.7</b> Confidence interval for the mean and prediction intervals for a new observation</a></li>
<li class="chapter" data-level="7.8" data-path="chapter7.html"><a href="chapter7.html#section7-8"><i class="fa fa-check"></i><b>7.8</b> Chapter summary</a></li>
<li class="chapter" data-level="7.9" data-path="chapter7.html"><a href="chapter7.html#section7-9"><i class="fa fa-check"></i><b>7.9</b> Summary of important R code</a></li>
<li class="chapter" data-level="7.10" data-path="chapter7.html"><a href="chapter7.html#section7-10"><i class="fa fa-check"></i><b>7.10</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="chapter8.html"><a href="chapter8.html"><i class="fa fa-check"></i><b>8</b> Multiple linear regression</a>
<ul>
<li class="chapter" data-level="8.1" data-path="chapter8.html"><a href="chapter8.html#section8-1"><i class="fa fa-check"></i><b>8.1</b> Going from SLR to MLR</a></li>
<li class="chapter" data-level="8.2" data-path="chapter8.html"><a href="chapter8.html#section8-2"><i class="fa fa-check"></i><b>8.2</b> Validity conditions in MLR</a></li>
<li class="chapter" data-level="8.3" data-path="chapter8.html"><a href="chapter8.html#section8-3"><i class="fa fa-check"></i><b>8.3</b> Interpretation of MLR terms</a></li>
<li class="chapter" data-level="8.4" data-path="chapter8.html"><a href="chapter8.html#section8-4"><i class="fa fa-check"></i><b>8.4</b> Comparing multiple regression models</a></li>
<li class="chapter" data-level="8.5" data-path="chapter8.html"><a href="chapter8.html#section8-5"><i class="fa fa-check"></i><b>8.5</b> General recommendations for MLR interpretations and VIFs</a></li>
<li class="chapter" data-level="8.6" data-path="chapter8.html"><a href="chapter8.html#section8-6"><i class="fa fa-check"></i><b>8.6</b> MLR inference: Parameter inferences using the t-distribution</a></li>
<li class="chapter" data-level="8.7" data-path="chapter8.html"><a href="chapter8.html#section8-7"><i class="fa fa-check"></i><b>8.7</b> Overall F-test in multiple linear regression</a></li>
<li class="chapter" data-level="8.8" data-path="chapter8.html"><a href="chapter8.html#section8-8"><i class="fa fa-check"></i><b>8.8</b> Case study: First year college GPA and SATs</a></li>
<li class="chapter" data-level="8.9" data-path="chapter8.html"><a href="chapter8.html#section8-9"><i class="fa fa-check"></i><b>8.9</b> Different intercepts for different groups: MLR with indicator variables</a></li>
<li class="chapter" data-level="8.10" data-path="chapter8.html"><a href="chapter8.html#section8-10"><i class="fa fa-check"></i><b>8.10</b> Additive MLR with more than two groups: Headache example</a></li>
<li class="chapter" data-level="8.11" data-path="chapter8.html"><a href="chapter8.html#section8-11"><i class="fa fa-check"></i><b>8.11</b> Different slopes and different intercepts</a></li>
<li class="chapter" data-level="8.12" data-path="chapter8.html"><a href="chapter8.html#section8-12"><i class="fa fa-check"></i><b>8.12</b> F-tests for MLR models with quantitative and categorical variables and interactions</a></li>
<li class="chapter" data-level="8.13" data-path="chapter8.html"><a href="chapter8.html#section8-13"><i class="fa fa-check"></i><b>8.13</b> AICs for model selection</a></li>
<li class="chapter" data-level="8.14" data-path="chapter8.html"><a href="chapter8.html#section8-14"><i class="fa fa-check"></i><b>8.14</b> Case study: Forced expiratory volume model selection using AICs</a></li>
<li class="chapter" data-level="8.15" data-path="chapter8.html"><a href="chapter8.html#section8-15"><i class="fa fa-check"></i><b>8.15</b> Chapter summary</a></li>
<li class="chapter" data-level="8.16" data-path="chapter8.html"><a href="chapter8.html#section8-16"><i class="fa fa-check"></i><b>8.16</b> Summary of important R code</a></li>
<li class="chapter" data-level="8.17" data-path="chapter8.html"><a href="chapter8.html#section8-17"><i class="fa fa-check"></i><b>8.17</b> Practice problems</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="chapter9.html"><a href="chapter9.html"><i class="fa fa-check"></i><b>9</b> Case studies</a>
<ul>
<li class="chapter" data-level="9.1" data-path="chapter9.html"><a href="chapter9.html#section9-1"><i class="fa fa-check"></i><b>9.1</b> Overview of material covered</a></li>
<li class="chapter" data-level="9.2" data-path="chapter9.html"><a href="chapter9.html#section9-2"><i class="fa fa-check"></i><b>9.2</b> The impact of simulated chronic nitrogen deposition on the biomass and N2-fixation activity of two boreal feather moss–cyanobacteria associations</a></li>
<li class="chapter" data-level="9.3" data-path="chapter9.html"><a href="chapter9.html#section9-3"><i class="fa fa-check"></i><b>9.3</b> Ants learn to rely on more informative attributes during decision-making</a></li>
<li class="chapter" data-level="9.4" data-path="chapter9.html"><a href="chapter9.html#section9-4"><i class="fa fa-check"></i><b>9.4</b> Multi-variate models are essential for understanding vertebrate diversification in deep time</a></li>
<li class="chapter" data-level="9.5" data-path="chapter9.html"><a href="chapter9.html#section9-5"><i class="fa fa-check"></i><b>9.5</b> What do didgeridoos really do about sleepiness?</a></li>
<li class="chapter" data-level="9.6" data-path="chapter9.html"><a href="chapter9.html#section9-6"><i class="fa fa-check"></i><b>9.6</b> General summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Intermediate Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter7" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Simple linear regression inference<a href="chapter7.html#chapter7" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="section7-1" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Model<a href="chapter7.html#section7-1" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Chapter <a href="chapter6.html#chapter6">6</a>, we learned how to estimate and interpret
correlations and regression
equations with a single predictor variable (<strong><em>simple linear regression</em></strong>
or SLR).
We carefully explored the variety of things that could go wrong and
how to check for problems in regression situations. In this chapter, that work
provides the basis for performing statistical inference that mainly focuses on
the population slope coefficient based on the sample slope coefficient. As a
reminder, the estimated regression model is <span class="math inline">\(\hat{y}_i = b_0 + b_1x_i\)</span>. The
population regression equation is <span class="math inline">\(y_i = \beta_0 + \beta_1x_i + \varepsilon_i\)</span> where <span class="math inline">\(\beta_0\)</span> is the <strong><em>population</em></strong> (or true) <strong><em>y-intercept</em></strong> and
<span class="math inline">\(\beta_1\)</span> is the <strong><em>population</em></strong> (or true) <strong><em>slope coefficient</em></strong>.
These are population parameters (fixed but typically unknown). This model can
be re-written to think about different components and their roles. The mean of
a random variable is statistically denoted as <span class="math inline">\(E(y_i)\)</span>, the
<strong><em>expected value of</em></strong> <span class="math inline">\(\mathbf{y_i}\)</span>, or as <span class="math inline">\(\mu_{y_i}\)</span> and the mean of the
response variable in a simple linear model is specified by
<span class="math inline">\(E(y_i) = \mu_{y_i} = \beta_0 + \beta_1x_i\)</span>. This uses the true regression line
to define the model for the mean of the responses as a function of the value of
the explanatory variable<a href="#fn119" class="footnote-ref" id="fnref119"><sup>119</sup></a>.</p>
<p>The other part of any statistical model is specifying a model for the
variability around the mean.
There are two aspects to the variability to specify here – the shape of the
distribution and the spread of the distribution. This is where the
normal distribution
and our “normality assumption” re-appears. And for normal distributions, we
need to define a variance parameter, <span class="math inline">\(\sigma^2\)</span>. Combined, the complete
regression model is</p>
<p><span class="math display">\[y_i \sim N(\mu_{y_i},\sigma^2), \text{ with }
\mu_{y_i} = \beta_0 + \beta_1x_i,\]</span></p>
<p>which can be read as “y follows a normal distribution
with mean mu-y and variance sigma-squared” and that “mu-y is equal to beta-0 plus beta-1 times x”. This also implies that the random
variability around the true mean, the errors, follow a normal distribution with
mean 0 and that same variance, <span class="math inline">\(\varepsilon_i \sim N(0,\sigma^2)\)</span>. The true
deviations (<span class="math inline">\(\varepsilon_i\)</span>) are once again estimated by the
residuals, <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span> = observed response – predicted
response.
We can use the residuals to estimate <span class="math inline">\(\sigma\)</span>, which is also
called the <strong><em>residual standard error</em></strong>,
<span class="math inline">\(\hat{\sigma} = \sqrt{\Sigma e^2_i / (n-2)}\)</span>. We will find this quantity
near the end
of the regression output as discussed below so the formula is not heavily used
here. This provides us with the three parameters that are estimated as part of
our SLR model: <span class="math inline">\(\beta_0, \beta_1,\text{ and } \sigma\)</span>.</p>
<div style="page-break-after: always;"></div>
<p>These definitions also formalize the assumptions implicit in the
regression model: </p>
<ol style="list-style-type: decimal">
<li><p>The errors follow a normal distribution (<strong><em>Normality assumption</em></strong>).</p></li>
<li><p>The errors have the same variance (<strong><em>Constant variance assumption</em></strong>).</p></li>
<li><p>The observations are independent (<strong><em>Independence assumption</em></strong>).
</p></li>
<li><p>The model for the mean is “correct” (<strong><em>Linearity, No Influential points,
Only one group</em></strong>).</p></li>
</ol>
<p>The diagnostics described at the end of Chapter <a href="chapter6.html#chapter6">6</a> provide
techniques for checking these assumptions – at least not having clear issues with those assumptions is fundamental to having a regression line that we trust and
inferences from it that we also can trust.</p>
<p>To make this clearer, suppose that in the <em>Beers</em> and <em>BAC</em> study that
they had randomly assigned 20 students to consume each number of beers.
We would expect some
variation in the <em>BAC</em> for each group of 20 at each level of <em>Beers</em> but
that each group of observations will be centered at the true mean <em>BAC</em>
for each number of <em>Beers</em>. The regression model assumes that the <em>BAC</em>
values are normally distributed around the mean for each <em>Beer</em> level,
<span class="math inline">\(\text{BAC}_i \sim N(\beta_0 + \beta_1\text{ Beers}_i,\sigma^2)\)</span>, with
the mean defined by the regression equation. We actually do not need to
obtain more than one observation at each <span class="math inline">\(x\)</span> value to
make this assumption or assess it, but the plots below show you what this could
look like. The sketch in Figure <a href="chapter7.html#fig:Figure7-1">7.1</a> attempts to show
the idea of normal
distributions that are centered at the true regression line, all with the same
shape and variance that is an assumption of the regression model. Figure
<a href="chapter7.html#fig:Figure7-2">7.2</a> contains simulated realizations from a normal
distribution of 20 subjects at each <em>Beer</em> level around the assumed true
regression line with two different residual SEs of 0.02 and 0.06. The original
BAC model has a residual SE of 0.02 but had many fewer observations at each
<em>Beer</em> value.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-1"></span>
<img src="chapter7_files/image029small.png" alt="Sketch of assumed normal distributions for the responses centered at the regression line." width="75%" />
<p class="caption">
Figure 7.1: Sketch of assumed normal distributions for the responses centered at the regression line.
</p>
</div>
<div class="sourceCode" id="cb600"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb600-1"><a href="chapter7.html#cb600-1" aria-hidden="true" tabindex="-1"></a>BB <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/beersbac.csv&quot;</span>)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-2"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-2-1.png" alt="Simulated data for Beers and BAC assuming two different residual standard errors (0.02 and 0.06)." width="75%" />
<p class="caption">
Figure 7.2: Simulated data for Beers and BAC assuming two different residual standard errors (0.02 and 0.06).
</p>
</div>
<p>Along with getting the idea that regression models define normal
distributions in the y-direction that are
centered at the regression line, you can also get a sense of how variable
samples from a normal distribution can appear. Each distribution of 20 subjects
at each <span class="math inline">\(x\)</span> value came from a normal distribution but there are some of those
distributions that might appear to generate small outliers and have slightly
different variances. This can help us to remember to not be too particular when
assessing assumptions and allow for some variability in spreads and a few
observations from the tails of the distribution to occasionally arise.
</p>
<p>In sampling from the population, we expect some amount of variability
of each estimator around its
true value. This variability leads to the potential variability in estimated
regression lines (think of a suite of potential estimated regression lines that
would be created by different random samples from the same population).
Figure <a href="chapter7.html#fig:Figure7-3">7.3</a> contains the true regression line (bold, red)
and realizations of the
estimated regression line in simulated data based on results similar to the
real data set. This variability due to random sampling is something that needs to be
properly accounted for to use the <strong>single</strong> estimated regression line to
make inferences about the true line and parameters based on the sample-based
estimates.
The next sections develop those inferential tools.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-3"></span>
<img src="chapter7_files/image034.png" alt="Variability in realized regression lines based on sampling variation. Light grey lines are simulated realizations assuming the bold (red) line is the true SLR model and variability is similar to the original BAC data set. Simulated observations from the estimated models using the simulate function as was used in Chapter 2 were used to create this plot. " width="75%" />
<p class="caption">
Figure 7.3: Variability in realized regression lines based on sampling variation. Light grey lines are simulated realizations assuming the bold (red) line is the true SLR model and variability is similar to the original BAC data set. Simulated observations from the estimated models using the <code>simulate</code> function as was used in Chapter <a href="chapter2.html#chapter2">2</a> were used to create this plot. 
</p>
</div>
<!-- \newpage -->
</div>
<div id="section7-2" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Confidence interval and hypothesis tests for the slope and intercept<a href="chapter7.html#section7-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our inference techniques will resemble previous material with an
interest in forming confidence intervals and
doing hypothesis testing, although the interpretation of confidence intervals
for slope coefficients take some extra care. Remember that the general form of
any parametric confidence interval is</p>
<p><span class="math display">\[\text{estimate} \mp t^*\text{SE}_{estimate},\]</span></p>
<p>so we need to obtain the appropriate standard error for regression model
coefficients and the degrees of freedom to define the <span class="math inline">\(t\)</span>-distribution
to look up <span class="math inline">\(t^*\)</span> multiplier.
We will find the <span class="math inline">\(\text{SE}_{b_0}\)</span> and <span class="math inline">\(\text{SE}_{b_1}\)</span>
in the model summary. The degrees of freedom for the <span class="math inline">\(t\)</span>-distribution
in simple linear regression are <span class="math inline">\(\mathbf{df = n-2}\)</span>.
Putting this
together, the confidence interval for the true y-intercept, <span class="math inline">\(\beta_0\)</span>, is
<span class="math inline">\(\mathbf{b_0 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_0}}\)</span> although this
confidence interval is rarely of interest. The confidence interval
that is almost always of interest is for the true slope coefficient,
<span class="math inline">\(\beta_1\)</span>, that is <span class="math inline">\(\mathbf{b_1 \mp t^*_{n-2}}\textbf{SE}_{\mathbf{b_1}}\)</span>.
The slope confidence interval is used to do two
things: (1) inference for the amount of change in the mean of <span class="math inline">\(y\)</span> for a unit
change in <span class="math inline">\(x\)</span> in the population and (2) to potentially do hypothesis testing by
checking whether 0 is in the CI or not. The sketch in Figure <a href="chapter7.html#fig:Figure7-4">7.4</a>
illustrates the roles of the
CI for the slope in terms of determining where the population slope, <span class="math inline">\(\beta_1\)</span>, coefficient
might be – centered at the sample slope coefficient – our best guess for the
true slope. This sketch also informs an <strong><em>interpretation of the slope coefficient
confidence interval</em></strong>:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-4"></span>
<img src="chapter7_files/image045_resized.png" alt="Graphic illustrating the confidence interval for a slope coefficient for a 1 unit increase in \(x\)." width="75%" />
<p class="caption">
Figure 7.4: Graphic illustrating the confidence interval for a slope coefficient for a 1 unit increase in <span class="math inline">\(x\)</span>.
</p>
</div>
<blockquote>
<p>For a 1 <strong>[<em>units of X</em>]</strong> increase in <strong>X</strong>, we are ___ % confident
that the <strong>true change in the mean of</strong> <strong><em>Y</em></strong> will be between <strong>LL</strong>
and <strong>UL</strong> <strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p></p>
<p>In this interpretation, LL and UL are the calculated lower and upper
limits of the confidence interval. This builds
on our previous interpretation of the slope coefficient, adding in the
information about pinning down the true change (population change) in the mean
of the response variable for a difference of 1 unit in the <span class="math inline">\(x\)</span>-direction. The interpretation of the y-intercept CI is:
</p>
<blockquote>
<p>For an <strong><em>x</em></strong> of 0 <strong>[<em>units of X</em>]</strong>, we are 95% confident that
the true mean of <strong><em>Y</em></strong> will be between <strong>LL</strong> and <strong>UL</strong>
<strong>[<em>units of Y</em>]</strong>.</p>
</blockquote>
<p>This is really only interesting if the value of <span class="math inline">\(x = 0\)</span> is interesting –
we’ll see a method for generating CIs
for the true mean at potentially more interesting values of <span class="math inline">\(x\)</span> in
Section <a href="chapter7.html#section7-7">7.7</a>. To trust the results from these
confidence intervals, it is critical that any issues with the regression validity conditions are minor. </p>
<p>The only hypothesis test of interest in this situation is for the slope
coefficient.
To develop the hypotheses of interest in SLR, note the effect
of having <span class="math inline">\(\beta_1 = 0\)</span> in the mean of the regression equation,
<span class="math inline">\(\mu_{y_i} = \beta_0 + \beta_1x_i = \beta_0 + 0x_i = \beta_0\)</span>.
This is the “intercept-only” or “mean-only” model that suggests that
the mean of <span class="math inline">\(y\)</span> does not vary with different values of <span class="math inline">\(x\)</span> as it is always
<span class="math inline">\(\beta_0\)</span>. We saw this model in the ANOVA material as the reduced model when the
null hypothesis of no difference in the true means across the groups was true.
Here, this is the same as saying that there is no linear relationship between <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span>, or that <span class="math inline">\(x\)</span> is of no use in predicting <span class="math inline">\(y\)</span>, or that we make the same
prediction for <span class="math inline">\(y\)</span> for every value of <span class="math inline">\(x\)</span>. Thus</p>
<p><span class="math display">\[\boldsymbol{H_0: \beta_1 = 0}\]</span></p>
<p>is a test for <strong>no linear relationship between</strong> <span class="math inline">\(\mathbf{x}\)</span> <strong>and</strong> <span class="math inline">\(\mathbf{y}\)</span>
<strong>in the population</strong>. The alternative of <span class="math inline">\(\boldsymbol{H_A: \beta_1\ne 0}\)</span>,
that there is <strong>some</strong> linear relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in the population,
is our main test of interest in these situations. It is also possible to test
greater than or less than alternatives in certain situations.</p>
<p>Test statistics for regression coefficients are developed, if we can trust our assumptions, using the <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2\)</span> degrees of freedom.
The <span class="math inline">\(t\)</span>-test statistic is generally</p>
<p><span class="math display">\[t = \frac{b_i}{\text{SE}_{b_i}}\]</span></p>
<p>with the main interest in the test for <span class="math inline">\(\beta_1\)</span> based on <span class="math inline">\(b_1\)</span> initially.
The p-value would be calculated using the two-tailed area from the
<span class="math inline">\(t_{n-2}\)</span> distribution calculated using the <code>pt</code> function.
The p-value
to test these hypotheses is also provided
in the model summary as we will see below. </p>
<p>The greater than or
less than alternatives can have interesting interpretations in certain
situations. For example, the greater than alternative
<span class="math inline">\(\left(\boldsymbol{H_A: \beta_1 &gt; 0}\right)\)</span> tests an alternative of a
positive linear relationship, with the p-value extracted just from the
right tail of the same <span class="math inline">\(t\)</span>-distribution. This could be
used when a researcher would only find a result “interesting” if a positive
relationship is detected, such as in the study of tree height and tree diameter
where a researcher might be justified in deciding to test only for a positive
linear relationship. Similarly, the left-tailed alternative is also possible,
<span class="math inline">\(\boldsymbol{H_A: \beta_1 &lt; 0}\)</span>. To get one-tailed p-values from two-tailed
results (the default), first check that the observed test statistic is
in the direction of the alternative (<span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span>
for <span class="math inline">\(H_A:\beta_1&lt;0\)</span>).
<strong>If these conditions are met, then the p-value for
the one-sided test from the two-sided version is found by dividing the
reported p-value by 2</strong>. If <span class="math inline">\(t&gt;0\)</span> for <span class="math inline">\(H_A:\beta_1&gt;0\)</span> or <span class="math inline">\(t&lt;0\)</span>
for <span class="math inline">\(H_A:\beta_1&lt;0\)</span> are not met, then the p-value would be greater than
0.5 and it would be easiest to look it up directly using <code>pt</code> using the tail area direction in the direction of the alternative.
</p>
<p>We can revisit a couple of examples for a last time with these ideas in
hand to complete the analyses.</p>
<blockquote>
<p>For the <em>Beers, BAC</em> data, the 95% confidence for the true slope
coefficient, <span class="math inline">\(\beta_1\)</span>, is</p>
</blockquote>
<p><span class="math display">\[\begin{array}{rl}
\boldsymbol{b_1 \mp t^*_{n-2}} \textbf{SE}_{\boldsymbol{b_1}}
&amp; \boldsymbol{= 0.01796 \mp 2.144787 * 0.002402} \\
&amp; \boldsymbol{= 0.01796 \mp 0.00515} \\
&amp; \boldsymbol{\rightarrow (0.0128, 0.0231).}
\end{array}\]</span></p>
<p>You can find the components of this
calculation in the model summary and from <code>qt(0.975, df = n-2)</code> which was
2.145 for the <span class="math inline">\(t^*\)</span>-multiplier. Be careful not to use the <span class="math inline">\(t\)</span>-value of
7.48 in the model summary to
make confidence intervals – that is the test statistic used below. The related
calculations are shown at the bottom of the following code:</p>
<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb601-1"><a href="chapter7.html#cb601-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> Beers, <span class="at">data =</span> BB)</span>
<span id="cb601-2"><a href="chapter7.html#cb601-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = BAC ~ Beers, data = BB)
## 
## Residuals:
##       Min        1Q    Median        3Q       Max 
## -0.027118 -0.017350  0.001773  0.008623  0.041027 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -0.012701   0.012638  -1.005    0.332
## Beers        0.017964   0.002402   7.480 2.97e-06
## 
## Residual standard error: 0.02044 on 14 degrees of freedom
## Multiple R-squared:  0.7998, Adjusted R-squared:  0.7855 
## F-statistic: 55.94 on 1 and 14 DF,  p-value: 2.969e-06</code></pre>
<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb603-1"><a href="chapter7.html#cb603-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">14</span>) <span class="co">#t* multiplier for 95% CI</span></span></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb605-1"><a href="chapter7.html#cb605-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.017964</span> <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">14</span>)<span class="sc">*</span><span class="fl">0.002402</span></span></code></pre></div>
<pre><code>## [1] 0.01281222 0.02311578</code></pre>
<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb607-1"><a href="chapter7.html#cb607-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">14</span>)<span class="sc">*</span><span class="fl">0.002402</span></span></code></pre></div>
<pre><code>## [1] 0.005151778</code></pre>
<p>We can also get the confidence interval
directly from the <code>confint</code> function run on our regression model,
saving some calculation
effort and providing both the CI for the y-intercept and the slope coefficient. </p>
<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb609-1"><a href="chapter7.html#cb609-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m1)</span></code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept) -0.03980535 0.01440414
## Beers        0.01281262 0.02311490</code></pre>
<p>We interpret the 95% CI for the slope coefficient as follows: For a
1 <strong>beer</strong> increase in number of beers consumed, we are 95% confident that
the <strong>true</strong> change in the <strong>mean</strong> <em>BAC</em> will be between 0.0128 and 0.0231
g/dL. While the estimated slope is our best guess of the impacts of an extra
beer consumed based on our sample, this CI provides information about the likely
range of potential impacts on the mean in the population. It also could be used
to test the two-sided hypothesis test and would suggest strong evidence against
the null hypothesis since the confidence interval does not contain 0, but its main use is to quantify where we think the true slope coefficient resides.</p>
<p>The width of the CI, interpreted loosely as the precision of the estimated slope, is
impacted by the variability of
the observations around the estimated regression line, the overall sample size,
and the positioning of the <span class="math inline">\(x\)</span>-observations. Basically all those aspects relate
to how “clearly” known the regression line is and that determines the estimated
precision in the slope. For example, the more variability around the line that
is present, the more uncertainty there is about the correct line to use
(Least Squares (LS) can
still find an estimated line but there are other lines that might be “close” to
its optimizing choice). Similarly, more observations help us get a better
estimate of the mean – an idea that permeates all statistical methods. Finally,
the location of <span class="math inline">\(x\)</span>-values can impact the precision in a slope coefficient. We’ll
revisit this in the context of <strong><em>multicollinearity</em></strong> in the next chapter,
and often we have no control of <span class="math inline">\(x\)</span>-values, but just note that different
patterns of <span class="math inline">\(x\)</span>-values can lead to different precision of estimated slope
coefficients<a href="#fn120" class="footnote-ref" id="fnref120"><sup>120</sup></a>.</p>
<p>For hypothesis testing, we will almost always stick with two-sided tests
in regression modeling as it is a more conservative approach and does not require
us to have an expectation of a direction for relationships <em>a priori</em>. In this
example, the null hypothesis for the slope coefficient is that there is no
linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The
alternative hypothesis is that there is
some linear relationship between <em>Beers</em> and <em>BAC</em> in the population. The
test statistic is <span class="math inline">\(t = 0.01796/0.002402 = 7.48\)</span> which, if model assumptions hold,
follows a <span class="math inline">\(t(14)\)</span> distribution under the null hypothesis. The model summary provides the calculation
of the test statistic
and the two-sided test p-value of <span class="math inline">\(2.97\text{e-6} = 0.00000297\)</span>. So we
would just report
“p-value &lt; 0.0001”. This suggests that there is very strong evidence against the null hypothesis of no linear
relationship between <em>Beers</em> and <em>BAC</em> in the population, so we would conclude that there is a linear relationship between them. Because of the
random assignment, we can also say that drinking beers causes changes in BAC
but, because the sample was made up of volunteers, we cannot infer that these results
would hold in the general population of OSU students or more generally.</p>
<p>There are also results for the y-intercept in the output. The 95% CI is
from -0.0398 to 0.0144, that the true mean <em>BAC</em> for a 0 beer consuming
subject is between
-0.0398 to 0.01445. This is really not a big surprise but possibly is
comforting to know that these results would not show much evidence against the null hypothesis
that the true mean <em>BAC</em> for 0 <em>Beers</em> is 0. Finding little evidence of a
difference from 0 makes sense and makes the estimated y-intercept of -0.013 not
so problematic. In other situations, the results for the y-intercept may be
more illogical but this will often be because the y-intercept is extrapolating
far beyond the scope of observations. The y-intercept’s main function in
regression models is to be at the right level for the slope to “work” to make a
line that describes the responses and thus is usually of lesser interest even though it plays an important role in the model.</p>
<p>As a second example, we can revisit modeling the <em>Hematocrit</em> of female
Australian athletes as a function of <em>body fat %</em>. The sample size is <span class="math inline">\(n = 99\)</span>
so the <em>df</em> are 97 in any <span class="math inline">\(t\)</span>-distributions. In Chapter <a href="chapter6.html#chapter6">6</a>, the
relationship between <em>Hematocrit</em> and <em>body fat %</em> for females appeared to
be a weak negative linear
association. The 95% confidence interval for the slope is -0.186 to 0.0155. For
a 1% increase in body fat %, we are 95% confident that the change in the true
mean Hematocrit is between -0.186 and 0.0155% of blood. This suggests that we
would find little evidence against the null hypothesis of no linear relationship because this CI contains 0. In fact the p-value is 0.0965 which is larger than 0.05
and so provides a consistent conclusion with using the 95% confidence interval
to perform a hypothesis test. Either way, we would conclude that there is not strong evidence against the null hypothesis but there is some evidence against it with a p-value of that size since more extreme results are somewhat common but still fairly rare if we assume the null is true. If you think p-values around 0.10 provide moderate evidence, you might have a different opinion about
the evidence against the null hypothesis here. For this reason, we sometimes
interpret this sort of marginal result as having some or marginal evidence against the null
but certainly would never say that this presents strong evidence.
</p>
<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb611-1"><a href="chapter7.html#cb611-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(alr4)</span>
<span id="cb611-2"><a href="chapter7.html#cb611-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ais)</span>
<span id="cb611-3"><a href="chapter7.html#cb611-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tibble)</span>
<span id="cb611-4"><a href="chapter7.html#cb611-4" aria-hidden="true" tabindex="-1"></a>ais <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ais)</span>
<span id="cb611-5"><a href="chapter7.html#cb611-5" aria-hidden="true" tabindex="-1"></a>aisR <span class="ot">&lt;-</span> ais <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="sc">-</span><span class="dv">56</span>, <span class="sc">-</span><span class="dv">166</span>) <span class="co">#Removes observations in rows 56 and 166</span></span>
<span id="cb611-6"><a href="chapter7.html#cb611-6" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Hc <span class="sc">~</span> Bfat, <span class="at">data =</span> aisR <span class="sc">%&gt;%</span> <span class="fu">filter</span>(Sex <span class="sc">==</span> <span class="dv">1</span>)) <span class="co">#Results for Females </span></span>
<span id="cb611-7"><a href="chapter7.html#cb611-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb611-8"><a href="chapter7.html#cb611-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hc ~ Bfat, data = aisR %&gt;% filter(Sex == 1))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.2399 -2.2132 -0.1061  1.8917  6.6453 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 42.01378    0.93269  45.046   &lt;2e-16
## Bfat        -0.08504    0.05067  -1.678   0.0965
## 
## Residual standard error: 2.598 on 97 degrees of freedom
## Multiple R-squared:  0.02822,    Adjusted R-squared:  0.0182 
## F-statistic: 2.816 on 1 and 97 DF,  p-value: 0.09653</code></pre>
<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb613-1"><a href="chapter7.html#cb613-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m2)</span></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept) 40.1626516 43.86490713
## Bfat        -0.1856071  0.01553165</code></pre>
<p>One more worked example is provided from the Montana fire data. In this
example pay particular attention
to how we are handling the units of the response variable, log-hectares, and to
the changes to doing inferences with a 99% confidence
level CI, and where you can find the needed results in the following output:</p>
<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb615-1"><a href="chapter7.html#cb615-1" aria-hidden="true" tabindex="-1"></a>mtfires <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/climateR2.csv&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb616"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb616-1"><a href="chapter7.html#cb616-1" aria-hidden="true" tabindex="-1"></a>mtfires <span class="ot">&lt;-</span> mtfires <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">loghectares =</span> <span class="fu">log</span>(hectares))</span>
<span id="cb616-2"><a href="chapter7.html#cb616-2" aria-hidden="true" tabindex="-1"></a>fire1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(loghectares <span class="sc">~</span> Temperature, <span class="at">data =</span> mtfires)</span>
<span id="cb616-3"><a href="chapter7.html#cb616-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fire1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = loghectares ~ Temperature, data = mtfires)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0822 -0.9549  0.1210  1.0007  2.4728 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -69.7845    12.3132  -5.667 1.26e-05
## Temperature   1.3884     0.2165   6.412 2.35e-06
## 
## Residual standard error: 1.476 on 21 degrees of freedom
## Multiple R-squared:  0.6619, Adjusted R-squared:  0.6458 
## F-statistic: 41.12 on 1 and 21 DF,  p-value: 2.347e-06</code></pre>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb618-1"><a href="chapter7.html#cb618-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(fire1, <span class="at">level =</span> <span class="fl">0.99</span>)</span></code></pre></div>
<pre><code>##                    0.5 %     99.5 %
## (Intercept) -104.6477287 -34.921286
## Temperature    0.7753784   2.001499</code></pre>
<div class="sourceCode" id="cb620"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb620-1"><a href="chapter7.html#cb620-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.995</span>, <span class="at">df =</span> <span class="dv">21</span>)</span></code></pre></div>
<pre><code>## [1] 2.83136</code></pre>
<ul>
<li><p>Based on the estimated regression model, we can say that if the
average temperature is 0, we expect that, on average, the log-area
burned would be -69.8 log-hectares.</p></li>
<li><p>From the regression model summary, <span class="math inline">\(b_1 = 1.39\)</span> with
<span class="math inline">\(\text{SE}_{b_1} = 0.2165\)</span> and <span class="math inline">\(\mathbf{t = 6.41}\)</span>.</p></li>
<li><p>There were <span class="math inline">\(n = 23\)</span> measurements taken, so <span class="math inline">\(\mathbf{df = n-2 = 23-3 = 21}\)</span>.</p></li>
<li><p>Suppose that we want to test for a linear relationship between
temperature and log-hectares burned:</p>
<p><span class="math display">\[H_0: \beta_1 = 0\]</span></p>
<ul>
<li>In words, the true slope coefficient between <em>Temperature</em> and
<em>log-area burned</em> is 0 OR there is no linear relationship between
<em>Temperature</em> and <em>log-area burned</em> in the population.</li>
</ul>
<p><span class="math display">\[H_A: \beta_1\ne 0\]</span></p>
<ul>
<li>In words, the alternative states that the true slope coefficient
between <em>Temperature</em> and <em>log-area burned</em> is not 0 OR there is a
linear relationship between <em>Temperature</em> and <em>log-area burned</em> in
the population.</li>
</ul></li>
</ul>
<p>Test statistic: <span class="math inline">\(t = 1.39/0.217 = 6.41\)</span></p>
<ul>
<li>Assuming the null hypothesis to be true (no linear relationship), the
<span class="math inline">\(t\)</span>-statistic follows a <span class="math inline">\(t\)</span>-distribution with <span class="math inline">\(n-2 = 23-2 = 21\)</span> degrees of
freedom (or simply <span class="math inline">\(t_{21}\)</span>).</li>
</ul>
<p>p-value:</p>
<ul>
<li><p>From the model summary, the <strong>p-value is</strong> <span class="math inline">\(\mathbf{2.35*10^{-6}}\)</span></p>
<ul>
<li>Interpretation: There is less than a 0.01% chance that we would
observe slope coefficient like we did or something more extreme
(greater than 1.39 log(hectares)/<span class="math inline">\(^\circ F\)</span>) if there were in fact
no linear relationship between temperature (<span class="math inline">\(^\circ F\)</span>) and log-area
burned (log-hectares) in the population.</li>
</ul></li>
</ul>
<p>Conclusion: There is very strong evidence against the null hypothesis of no
linear relationship, so we would conclude that there
is, in fact, a linear relationship between Temperature and log(Hectares)
burned.</p>
<p>Scope of Inference: Since we have a time series of results, our inferences pertain to the
results we could have observed for these years but not for years we did not
observe – so just for the true slope for this sample of years. Because we can’t
randomly assign the amount of area burned, we cannot make causal inferences –
there are many reasons why both the average temperature and area burned would
vary together that would not involve a direct connection between them.</p>
<p><span class="math display">\[\text{99}\% \text{ CI for } \beta_1: \boldsymbol{b_1 \mp
t^*_{n-2}}\textbf{SE}_{\boldsymbol{b_1}} \rightarrow 1.39 \mp 2.831\bullet 0.217
\rightarrow (0.78, 2.00)\]</span></p>
<p>Interpretation of 99% CI for slope coefficient:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that the
change in the true mean log-area burned is between 0.78 and 2.00 log(Hectares).</li>
</ul>
<p>Another way to interpret this is:</p>
<ul>
<li>For a 1 degree F increase in <em>Temperature</em>, we are 99% confident that
the mean Area Burned will change by between 0.78 and 2.00 log(Hectares)
<strong>in the population</strong>.</li>
</ul>
<p>Also <span class="math inline">\(R^2\)</span> is 66.2%, which tells us that <em>Temperature</em> explains 66.2%
of the variation in <em>log(Hectares) burned</em>. Or that the linear regression
model built using <em>Temperature</em> explains 66.2% of the variation in yearly
<em>log(Hectares) burned</em> so this model explains quite a bit but not all the variation in the responses.</p>
</div>
<div id="section7-3" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Bozeman temperature trend<a href="chapter7.html#section7-3" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For a new example, consider the yearly average maximum temperatures in Bozeman, MT.
For over 100 years, daily measurements have been taken of the minimum and
maximum temperatures at hundreds of weather stations across the US. In early
years, this involved manual recording of the temperatures and resetting the
thermometer to track the extremes for the following day. More recently, these
measures have been replaced by digital temperature recording devices that
continue to track this sort of information with much less human effort and,
possibly, errors. This sort of information is often aggregated to monthly or
yearly averages to be able to see “on average” changes from month-to-month or
year-to-year as opposed to the day-to-day variation in the temperature<a href="#fn121" class="footnote-ref" id="fnref121"><sup>121</sup></a>. Often the local information is aggregated further to
provide regional, hemispheric, or even global average temperatures. Climate
change research involves attempting to quantify the changes over time in these
and other long-term temperature or temperature proxies.</p>
<p>These data were extracted from the National Oceanic and Atmospheric
Administration’s National Centers for Environmental Information’s database
(<a href="http://www.ncdc.noaa.gov/cdo-web/" class="uri">http://www.ncdc.noaa.gov/cdo-web/</a>) and we will focus on the yearly
average of the monthly averages of the daily maximum temperature in Bozeman in degrees
F from 1901 to 2014. We can call
them yearly average maximum temperatures but note that it was a little more
complicated than that to arrive at the response variable we are analyzing.</p>
<div class="sourceCode" id="cb622"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb622-1"><a href="chapter7.html#cb622-1" aria-hidden="true" tabindex="-1"></a>bozemantemps <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/BozemanMeanMax.csv&quot;</span>)</span></code></pre></div>

<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb623-1"><a href="chapter7.html#cb623-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(bozemantemps)</span></code></pre></div>
<pre><code>##     meanmax           Year     
##  Min.   :49.75   Min.   :1901  
##  1st Qu.:53.97   1st Qu.:1930  
##  Median :55.43   Median :1959  
##  Mean   :55.34   Mean   :1958  
##  3rd Qu.:57.02   3rd Qu.:1986  
##  Max.   :60.05   Max.   :2014</code></pre>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb625-1"><a href="chapter7.html#cb625-1" aria-hidden="true" tabindex="-1"></a><span class="fu">length</span>(bozemantemps<span class="sc">$</span>Year) <span class="co">#Some years are missing (1905, 1906, 1948, 1950, 1995)</span></span></code></pre></div>
<pre><code>## [1] 109</code></pre>
<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb627-1"><a href="chapter7.html#cb627-1" aria-hidden="true" tabindex="-1"></a>bozemantemps <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Year, <span class="at">y =</span> meanmax)) <span class="sc">+</span></span>
<span id="cb627-2"><a href="chapter7.html#cb627-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb627-3"><a href="chapter7.html#cb627-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb627-4"><a href="chapter7.html#cb627-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>, <span class="at">se =</span> F) <span class="sc">+</span> <span class="co">#Add smoothing line</span></span>
<span id="cb627-5"><a href="chapter7.html#cb627-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb627-6"><a href="chapter7.html#cb627-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Scatterplot of Bozeman Yearly Average Max Temperatures&quot;</span>,</span>
<span id="cb627-7"><a href="chapter7.html#cb627-7" aria-hidden="true" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">&quot;Mean Maximum Temperature (degrees F)&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-5"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-5-1.png" alt="Scatterplot of average yearly maximum temperatures in Bozeman from 1900 to 2014 with SLR (solid) and smoothing (dashed) lines." width="75%" />
<p class="caption">
Figure 7.5: Scatterplot of average yearly maximum temperatures in Bozeman from 1900 to 2014 with SLR (solid) and smoothing (dashed) lines.
</p>
</div>
<p>The scatterplot in Figure <a href="chapter7.html#fig:Figure7-5">7.5</a> shows the results between
1901 and 2014 based on a sample of <span class="math inline">\(n = 109\)</span> years because four years had too
many missing months to fairly include in the responses. Missing values occur
for many reasons and in
this case were likely just machine or human error<a href="#fn122" class="footnote-ref" id="fnref122"><sup>122</sup></a>.
These are time series data and in time series analysis we assume that the
population of interest for inference is all possible realizations from the
underlying process over this time frame even though we only ever get to observe
one realization. In terms of climate change research, we would want to (a)
assess evidence for a trend over time (hopefully assessing whether any observed
trend is clearly different from a result that could have been observed by
chance if there really is no change over time in the true process) and (b)
quantify the size of the change over time along with the uncertainty in that
estimate relative to the underlying true mean change over time. The hypothesis
test for the slope answers (a) and the confidence interval for the slope
addresses (b). We also should be concerned about problematic (influential)
points, changing variance, and potential nonlinearity in the trend over time
causing problems for the SLR inferences. The scatterplot suggests that there is
a moderate or strong positive linear relationship between <em>temperatures</em> and
<em>year</em>. Both looking at the points and at the smoothing line does not suggest a clear curve in these responses over time and the variability seems similar across the years. There appears to be one potential large outlier
in the late 1930s.</p>
<p>We’ll perform all 6+ steps of the hypothesis test for the slope coefficient
and use the confidence
interval interpretation to discuss the size of the change.
First, we need to select our hypotheses (the 2-sided test
would be a <strong><em>conservative</em></strong> choice and no one that does climate change
research wants to be accused of taking a <strong><em>liberal</em></strong> approach in their analyses<a href="#fn123" class="footnote-ref" id="fnref123"><sup>123</sup></a>) and our test statistic, <span class="math inline">\(t = \frac{b_1}{\text{SE}_{b_1}}\)</span>. The scatterplot is the perfect tool to illustrate the situation.</p>
<ol style="list-style-type: decimal">
<li><strong>Hypotheses for the slope coefficient test:</strong></li>
</ol>
<p><span class="math display">\[H_0: \beta_1 = 0 \text{ vs } H_A: \beta_1 \ne 0\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li><strong>Validity conditions:</strong></li>
</ol>
<ul>
<li><p><strong>Quantitative variables condition</strong></p>
<ul>
<li>Both <code>Year</code> and yearly average <code>Temperature</code> are
quantitative variables so are suitable for an SLR analysis.</li>
</ul></li>
<li><p><strong>Independence of observations</strong></p>
<ul>
<li>There may be a lack of independence among years since a warm
year might be followed by another warmer than average year. It
would take more sophisticated models to
account for this and the standard error on the slope coefficient could
either get larger or smaller depending on the type of
<strong><em>autocorrelation</em></strong> (correlation between neighboring time points or
correlation with oneself at some time
lag) present. This creates a caveat on these results but this model is
often the first one researchers fit in these situations and often is
reasonably correct even in the presence of some autocorrelation. </li>
</ul></li>
</ul>
<p>To assess the remaining conditions, we need to fit the regression model and use
the diagnostic plots in Figure <a href="chapter7.html#fig:Figure7-6">7.6</a> to aid our assessment:</p>

<div class="sourceCode" id="cb628"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb628-1"><a href="chapter7.html#cb628-1" aria-hidden="true" tabindex="-1"></a>temp1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year, <span class="at">data =</span> bozemantemps)</span>
<span id="cb628-2"><a href="chapter7.html#cb628-2" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb628-3"><a href="chapter7.html#cb628-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(temp1, <span class="at">add.smooth =</span> F, <span class="at">pch =</span> <span class="dv">16</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-6"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-6-1.png" alt="Diagnostic plots of the Bozeman yearly temperature simple linear regression model." width="75%" />
<p class="caption">
Figure 7.6: Diagnostic plots of the Bozeman yearly temperature simple linear regression model.
</p>
</div>
<ul>
<li><p><strong>Linearity of relationship</strong></p>
<ul>
<li><p>Examine the Residuals vs Fitted plot:
</p>
<ul>
<li>There does not appear to be a clear curve remaining in
the residuals so we should be able to proceed without worrying too much
about missed nonlinearity.</li>
</ul></li>
<li><p>Compare the smoothing line to the regression line in Figure <a href="chapter7.html#fig:Figure7-5">7.5</a>:</p>
<ul>
<li>There does not appear to be a big difference between the straight line
and the smoothing line.</li>
</ul></li>
</ul></li>
<li><p><strong>Equal (constant) variance</strong></p>
<ul>
<li>Examining the Residuals vs Fitted and the “Scale-Location” plots
provide little to no evidence of changing variance.
The variability does decrease slightly in the middle fitted
values but those changes are really minor and present no real evidence of
changing variability.</li>
</ul></li>
<li><p><strong>Normality of residuals</strong></p>
<ul>
<li>Examining the Normal QQ-plot for violations of the normality
assumption shows only one real problem in the outlier from the
32<sup>nd</sup> observation in the data set (the temperature observed in 1934)
which was identified as a large outlier when examining the original scatterplot.
We should be careful about inferences that assume normality and contain this
point in the analysis. We might consider running the analysis with and
without that point to see how much it impacts the results just to be sure
it isn’t creating evidence of a trend because of a violation of the
normality assumption. The next check reassures us that re-running the
model without this point would only result in slightly changing the SEs
and not the slopes.</li>
</ul></li>
<li><p><strong>No influential points:</strong></p>
<ul>
<li><p>There are no influential points displayed in the Residuals vs
Leverage plot since the Cook’s D contours are not displayed.
</p>
<ul>
<li>Note: by default this plot contains a smoothing line that is
relatively meaningless, so ignore it
if is displayed. We suppressed it using the <code>add.smooth = F</code>
option in <code>plot(temp1)</code> but if you forget to do that, just
ignore the smoothers in the diagnostic plots especially in
the Residuals vs Leverage plot. </li>
</ul></li>
<li><p>These results tells us that the outlier was not influential. If
you look back at the scatterplot, it was
located near the middle of the observed <span class="math inline">\(x\text{&#39;s}\)</span> so its
potential leverage is low. You can find its leverage based on the
plot to be around 0.12 when there are observations in the data set
with leverages over 0.3. The high leverage points occur at the
beginning and the end of the record because they are at the edges of
the observed <span class="math inline">\(x\text{&#39;s}\)</span> and most of these points follow the overall
pattern fairly well.</p></li>
</ul></li>
</ul>
<p>So the main issues are with the assumption of independence
of observations and one non-influential outlier that might be compromising our
normality assumption a bit.</p>
<ol start="3" style="list-style-type: decimal">
<li><p><strong>Calculate the test statistic and p-value:</strong></p>
<ul>
<li><span class="math inline">\(t = 0.05244/0.00476 = 11.02\)</span></li>
</ul>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb629-1"><a href="chapter7.html#cb629-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(temp1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmax ~ Year, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3779 -0.9300  0.1078  1.1960  5.8698 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -47.35123    9.32184   -5.08 1.61e-06
## Year          0.05244    0.00476   11.02  &lt; 2e-16
## 
## Residual standard error: 1.624 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li><p>From the model summary: p-value &lt; 2e-16 or just &lt; 0.0001</p></li>
<li><p>The test statistic is assumed to follow a <span class="math inline">\(t\)</span>-distribution with
<span class="math inline">\(n-2 = 109-2 = 107\)</span> degrees of freedom. The p-value can also be calculated as:</p></li>
</ul>
<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb631-1"><a href="chapter7.html#cb631-1" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">*</span><span class="fu">pt</span>(<span class="fl">11.02</span>, <span class="at">df =</span> <span class="dv">107</span>, <span class="at">lower.tail =</span> F)</span></code></pre></div>
<pre><code>## [1] 2.498481e-19</code></pre>
<ul>
<li>Which is then reported as &lt; 0.0001, which means that the chances of
observing a slope coefficient as extreme or more extreme than 0.052 if the
null hypothesis of no linear relationship is true is less than 0.01%.</li>
</ul></li>
<li><p><strong>Write a conclusion:</strong></p>
<ul>
<li>There is very strong evidence (<span class="math inline">\(t_{107} = 11.02\)</span>, p-value &lt; 0.0001) against
the null hypothesis of no linear relationship
between <em>Year</em> and yearly mean <em>Temperature</em> so we can conclude that
there is, in fact, some linear relationship between <em>Year</em> and yearly mean
maximum <em>Temperature</em> in Bozeman.</li>
</ul></li>
</ol>
<!-- \newpage -->
<ol start="5" style="list-style-type: decimal">
<li><p><strong>Size:</strong></p>
<ul>
<li>For a one year increase in Year, we estimate that, on average, the yearly
average maximum temperature will change by 0.0524 <span class="math inline">\(^\circ F\)</span> (95% CI: 0.043
to 0.062). This suggests a modest but noticeable change in the mean
temperature in Bozeman and the confidence suggests minimal variation around
this estimate, going from 0.04 to 0.06 <span class="math inline">\(^\circ F\)</span>. The “size” of this change
is discussed more in Section <a href="chapter7.html#section7-5">7.5</a>.</li>
</ul>
<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb633-1"><a href="chapter7.html#cb633-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(temp1)</span></code></pre></div>
<pre><code>##                    2.5 %       97.5 %
## (Intercept) -65.83068375 -28.87177785
## Year          0.04300681   0.06187746</code></pre></li>
<li><p><strong>Scope of inference:</strong></p>
<ul>
<li>We can conclude that this detected trend pertains to the Bozeman
area in the years 1901 to 2014 but not outside of either this area or time
frame. We cannot say that time caused the observed changes since it was not
randomly assigned and we cannot attribute the changes to any other factors
because we did not consider them. But knowing that there was a trend toward
increasing temperatures is an intriguing first step in a more complete
analysis of changing climate in the area.</li>
</ul></li>
</ol>
<p>It is also good to report the percentage of variation that the model
explains: <em>Year</em> explains 54.91% of the variation in yearly average maximum
<em>Temperature</em>. If the coefficient of determination value had been very
small, we might discount the previous result. Since it is moderately large with
over 50% of the variation in the response explained, that suggests that just by
using a linear trend over time we can account for quite a bit of the variation
in yearly average maximum temperatures in Bozeman. Note that the percentage of
variation explained would get much worse if we tried to analyze the monthly or
original daily maximum temperature data even though we might find about the same
estimated mean change over time.</p>
<p>Interpreting a confidence interval provides more useful information than the
hypothesis test here –
instead of just assessing evidence against the null hypothesis, we can actually
provide our best guess at the true change in the mean of <span class="math inline">\(y\)</span> for a change in <span class="math inline">\(x\)</span>.
Here, the 95% CI is (0.043, 0.062). This tells us that for a 1 year increase in
<em>Year</em>, we are 95% confident that the change in the true mean of the yearly
average maximum <em>Temperatures</em> in Bozeman is between 0.043 and 0.062 <span class="math inline">\(^\circ F\)</span>.</p>
<p>Sometimes the scale of the <span class="math inline">\(x\)</span>-variable makes interpretation a little difficult,
so we can re-scale it
to make the resulting slope coefficient more interpretable without changing how the model fits the responses. One option is to re-scale the variable and
re-fit the regression model and the other (easier) option is to re-scale our
interpretation. The idea here is that a 100-year change might be easier and
more meaningful scale to interpret than a single year change. If we have a
slope in the model of 0.052 (for a 1 year change), we can also say that a 100
year change in the mean is estimated to be 0.052*100 = 0.52<span class="math inline">\(^\circ F\)</span>.
Similarly, the 95% CI for the
population mean 100-year change would be from
0.43<span class="math inline">\(^\circ F\)</span> to 0.62<span class="math inline">\(^\circ F\)</span>. In 2007, the IPCC
(Intergovernmental Panel on Climate Change;
<a href="http://www.ipcc.ch/publications_and_data/ar4/wg1/en/tssts-3-1-1.html" class="uri">http://www.ipcc.ch/publications_and_data/ar4/wg1/en/tssts-3-1-1.html</a>)
estimated the global temperature change from 1906 to 2005 to be
0.74<span class="math inline">\(^\circ C\)</span> per decade or, scaled up, 7.4<span class="math inline">\(^\circ C\)</span> per century
(1.33<span class="math inline">\(^\circ F\)</span>). There are many reasons why our
local temperature trend might differ, including that our analysis was of
average maximum temperatures and the IPCC was considering the average
temperature (which was not measured locally or in most places in a good way
until digital instrumentation was installed) and that local trends are likely
to vary around the global average change based on localized environmental
conditions.</p>
<p>One issue that arises in studies of climate change is that researchers
often consider these sorts
of tests at many locations and on many response variables (if I did the maximum
temperature, why not also do the same analysis of the minimum temperature time
series as well? And if I did the analysis for Bozeman, what about Butte and
Helena and…?). Remember our discussion of multiple testing issues? This issue can arise when regression
modeling is repeated in many similar data sets, say different sites or
different response variables or both, in one study. In <span class="citation">(<a href="#ref-Moore2007" role="doc-biblioref"><strong>Moore2007?</strong></a>)</span>, we considered the impacts on the assessment of evidence of trends
of earlier spring onset timing in the Mountain West when the number of tests
across many sites is accounted for. We found that the evidence for time trends
decreases substantially but does not disappear. In a related study, <span class="citation">(<a href="#ref-Greenwood2011" role="doc-biblioref"><strong>Greenwood2011?</strong></a>)</span>
found evidence for regional trends to earlier spring
onset using more sophisticated statistical models. The main point here is to
<strong>be careful when using simple statistical methods repeatedly if you are
not accounting for the number of tests performed</strong>. </p>
<p>Along with the confidence interval, we can also plot the estimated model
(Figure <a href="chapter7.html#fig:Figure7-7">7.7</a>) using a term-plot from the <code>effects</code> package
(Fox, 2003). This is the same function we used for visualizing results
in the ANOVA models and in its basic application you just need
<code>plot(allEffects(MODELNAME))</code> although we from time to time, we will add some options.
In
regression models, we get to see the regression line along with bounds for
95% confidence intervals for the mean at every value of <span class="math inline">\(x\)</span> that was observed
(explained in the next section). Note that there is also a rugplot on the <span class="math inline">\(x\)</span>-axis
showing you where values of the explanatory variable were obtained, which is
useful to understanding how much information is available for different aspects
of the line. Here it provides gaps for missing years of observations as sort of
broken teeth in a comb. Also not used here, we can also turn on the <code>residuals = T</code> option, which in SLR just plots the original points and adds a smoothing line to this plot to reinforce the previous assessment of assumptions.</p>

<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb635-1"><a href="chapter7.html#cb635-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(effects)</span>
<span id="cb635-2"><a href="chapter7.html#cb635-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">allEffects</span>(temp1, <span class="at">xlevels =</span> <span class="fu">list</span>(<span class="at">Year =</span> bozemantemps<span class="sc">$</span>Year)),</span>
<span id="cb635-3"><a href="chapter7.html#cb635-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">grid =</span> T)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-7"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-7-1.png" alt="Term-plot for the Bozeman mean yearly maximum temperature linear regression model with 95% confidence interval bands for the mean in each year." width="75%" />
<p class="caption">
Figure 7.7: Term-plot for the Bozeman mean yearly maximum temperature linear regression model with 95% confidence interval bands for the mean in each year.
</p>
</div>
<p>If we extended the plot for the model to <code>Year</code> = 0, we could see the
reason that the y-intercept in this model is -47.4<span class="math inline">\(^\circ F\)</span>. This is
obviously a large extrapolation for these data and provides a silly result.
However, in paleoclimate data that goes back
thousands of years using tree rings, ice cores, or sea sediments, the estimated
mean in year 0 might be interesting and within the scope of observed values or
it might not. For example, in <span class="citation">(<a href="#ref-Santibanez2018" role="doc-biblioref"><strong>Santibanez2018?</strong></a>)</span>, the data were a time series from
27,000 to about 9,000 years before present extracted from Antarctic ice cores.
It all depends on the application.</p>
<p>To make the y-intercept more interesting for our data set, we can re-scale the <span class="math inline">\(x\text{&#39;s}\)</span> using <code>mutate</code> before we fit the
model to have the first year in the data set (1901) be “0”. This is
accomplished by calculating <span class="math inline">\(\text{Year2} = \text{Year}-1901\)</span>.</p>
<div class="sourceCode" id="cb636"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb636-1"><a href="chapter7.html#cb636-1" aria-hidden="true" tabindex="-1"></a>bozemantemps <span class="ot">&lt;-</span> bozemantemps <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">Year2 =</span> Year <span class="sc">-</span> <span class="dv">1901</span>)</span>
<span id="cb636-2"><a href="chapter7.html#cb636-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(bozemantemps<span class="sc">$</span>Year2)</span></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    0.00   29.00   58.00   57.27   85.00  113.00</code></pre>
<p>The new estimated regression equation is
<span class="math inline">\(\widehat{\text{Temp}}_i = 52.34 + 0.052\cdot\text{Year2}_i\)</span>. The slope and
its test statistic are the same as in the previous model. The y-intercept
has changed dramatically with a 95% CI from 51.72<span class="math inline">\(^\circ F\)</span> to 52.96<span class="math inline">\(^\circ F\)</span>
for <code>Year2</code> = 0. But we know that <code>Year2</code> has a 0 value for 1901 because
of our subtraction. That means that this CI is
for the true mean in 1901 and is now at least somewhat interesting. If you
revisit Figure <a href="chapter7.html#fig:Figure7-7">7.7</a> you will actually see that the displayed
confidence intervals
provide upper and lower bounds that match this result for 1901 – the
y-intercept CI matches the 95% CI for the true mean in the first year of the data set.</p>
<div class="sourceCode" id="cb638"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb638-1"><a href="chapter7.html#cb638-1" aria-hidden="true" tabindex="-1"></a>temp2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year2, <span class="at">data =</span> bozemantemps)</span>
<span id="cb638-2"><a href="chapter7.html#cb638-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(temp2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmax ~ Year2, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3779 -0.9300  0.1078  1.1960  5.8698 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 52.34126    0.31383  166.78   &lt;2e-16
## Year2        0.05244    0.00476   11.02   &lt;2e-16
## 
## Residual standard error: 1.624 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb640"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb640-1"><a href="chapter7.html#cb640-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(temp2)</span></code></pre></div>
<pre><code>##                   2.5 %      97.5 %
## (Intercept) 51.71913822 52.96339150
## Year2        0.04300681  0.06187746</code></pre>
<p>Ideally, we want to find a regression model that does not violate any
assumptions, has a high <span class="math inline">\(\mathbf{R^2}\)</span> value, and a slope coefficient
with a small p-value. If any of these are not the case, then we are
not completely satisfied with the regression and <strong>should be suspicious
of any inference we perform</strong>. We can sometimes resolve some of the
systematic issues noted above using <strong><em>transformations</em></strong>, discussed in
Sections <a href="chapter7.html#section7-5">7.5</a> and <a href="chapter7.html#section7-6">7.6</a>.
</p>
</div>
<div id="section7-4" class="section level2 hasAnchor" number="7.4">
<h2><span class="header-section-number">7.4</span> Randomization-based inferences for the slope coefficient<a href="chapter7.html#section7-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Exploring permutation testing in SLR provides an opportunity to gauge the observed
relationship against the sorts of relationships we would expect to see if there
was no linear relationship between the variables. If the relationship is linear
(not curvilinear) and the null hypothesis of <span class="math inline">\(\beta_1 = 0\)</span> is true, then any
configuration of the responses relative to the
predictor variable’s values is as good as any other.
Consider the four scatterplots of
the Bozeman temperature data versus <code>Year</code> and permuted versions of <code>Year</code>
in Figure <a href="chapter7.html#fig:Figure7-8">7.8</a>. First, think about which of the panels you
think present the most
evidence of a linear relationship between <code>Year</code> and <code>Temperature</code>?</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-8"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-8-1.png" alt="Plot of the Temperature responses versus four versions of Year, three of which are permutations of the Year variable relative to the Temperatures." width="75%" />
<p class="caption">
Figure 7.8: Plot of the <code>Temperature</code> responses versus four versions of <code>Year</code>, three of which are permutations of the Year variable relative to the Temperatures.
</p>
</div>
<p>Hopefully you can see that panel (c) contains the most clear linear
relationship among the choices. The plot in
panel (c) is actually the real data set and pretty clearly presents itself as
“different” from the other results. When we have small p-values, the real data
set will be clearly different from the permuted results because it will be
almost impossible to find a permuted data set that can attain as large a slope
coefficient as was observed in the real data set<a href="#fn124" class="footnote-ref" id="fnref124"><sup>124</sup></a>. This result ties back into our original interests
in this climate change research
situation – does our result look like it is different from what could have been
observed just by chance if there were no linear relationship between <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span>? It seems unlikely…</p>
<p>Repeating this permutation process and tracking the estimated slope
coefficients, as <span class="math inline">\(T^*\)</span>, provides another method to obtain a p-value in
SLR applications. This could also be performed on the <span class="math inline">\(t\)</span>-statistic for
the slope coefficient and would provide the same p-values but the sampling
distribution would have a
different <span class="math inline">\(x\)</span>-axis scaling. In this situation, the observed slope of 0.052 is
really far from any possible values that can be obtained using permutations as
shown in Figure <a href="chapter7.html#fig:Figure7-9">7.9</a>. The p-value would be reported as
<span class="math inline">\(&lt;0.001\)</span> for the two-sided permutation test.</p>
<!-- \newpage -->
<div class="sourceCode" id="cb642"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb642-1"><a href="chapter7.html#cb642-1" aria-hidden="true" tabindex="-1"></a>Tobs <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year, <span class="at">data =</span> bozemantemps)<span class="sc">$</span>coef[<span class="dv">2</span>]</span>
<span id="cb642-2"><a href="chapter7.html#cb642-2" aria-hidden="true" tabindex="-1"></a>Tobs</span></code></pre></div>
<pre><code>##       Year 
## 0.05244213</code></pre>
<div class="sourceCode" id="cb644"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb644-1"><a href="chapter7.html#cb644-1" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb644-2"><a href="chapter7.html#cb644-2" aria-hidden="true" tabindex="-1"></a>Tstar <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> B)</span>
<span id="cb644-3"><a href="chapter7.html#cb644-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb644-4"><a href="chapter7.html#cb644-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="sc">:</span>B)){</span>
<span id="cb644-5"><a href="chapter7.html#cb644-5" aria-hidden="true" tabindex="-1"></a>  Tstar[b] <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> <span class="fu">shuffle</span>(Year), <span class="at">data =</span> bozemantemps)<span class="sc">$</span>coef[<span class="dv">2</span>]</span>
<span id="cb644-6"><a href="chapter7.html#cb644-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb644-7"><a href="chapter7.html#cb644-7" aria-hidden="true" tabindex="-1"></a><span class="fu">pdata</span>(<span class="fu">abs</span>(Tstar), <span class="fu">abs</span>(Tobs), <span class="at">lower.tail =</span> F)[[<span class="dv">1</span>]]</span></code></pre></div>
<pre><code>## [1] 0</code></pre>

<div class="sourceCode" id="cb646"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb646-1"><a href="chapter7.html#cb646-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(Tstar) <span class="sc">%&gt;%</span>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Tstar)) <span class="sc">+</span> </span>
<span id="cb646-2"><a href="chapter7.html#cb646-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..ncount..), <span class="at">bins =</span> <span class="dv">20</span>, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>) <span class="sc">+</span> </span>
<span id="cb646-3"><a href="chapter7.html#cb646-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..scaled..)) <span class="sc">+</span></span>
<span id="cb646-4"><a href="chapter7.html#cb646-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb646-5"><a href="chapter7.html#cb646-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb646-6"><a href="chapter7.html#cb646-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span>Tobs, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb646-7"><a href="chapter7.html#cb646-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_bin</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..ncount.., <span class="at">label =</span> ..count..), <span class="at">bins =</span> <span class="dv">20</span>,</span>
<span id="cb646-8"><a href="chapter7.html#cb646-8" aria-hidden="true" tabindex="-1"></a>           <span class="at">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.75</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-9"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-9-1.png" alt="Permutation distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines at \(\pm b_1 = 0.56\) based on the observed estimated slope." width="75%" />
<p class="caption">
Figure 7.9: Permutation distribution of the slope coefficient in the Bozeman temperature linear regression model with bold vertical lines at <span class="math inline">\(\pm b_1 = 0.56\)</span> based on the observed estimated slope.
</p>
</div>
<p>One other interesting aspect of exploring the permuted data sets as in
Figure <a href="chapter7.html#fig:Figure7-8">7.8</a> is that the outlier
in the late 1930s “disappears” in the permuted data sets because there were
many other observations that were that warm, just none that happened around
that time of the century in the real data set. This reinforces the evidence for
changes over time that seem to be present in these data – old unusual years
don’t look unusual in more recent years (which is a pretty concerning result).</p>
<p>The permutation approach can be useful in situations where the normality assumption
is compromised, but there are no influential points. In these situations, we
might find more trustworthy p-values using permutations but only if we are
working with an initial estimated regression equation that we generally trust.
I personally like the permutation approach as a way of explaining what a p-value
is actually measuring – the chance of seeing something like what we saw, or
more extreme, if the null is true.
And the previous scatterplots show what the
“by chance” versions of this relationship might look like.</p>
<p>In a similar situation where we want to focus on confidence intervals for slope
coefficients but are not completely comfortable with the normality assumption,
it is also possible to generate bootstrap confidence intervals by sampling with
replacement from the data set. This idea was introduced in
Sections <a href="chapter2.html#section2-8">2.8</a> and <a href="chapter2.html#section2-9">2.9</a>. This provides a 95%
bootstrap confidence interval from 0.0433 to 0.061,
which almost exactly matches the parametric <span class="math inline">\(t\)</span>-based confidence interval. The
bootstrap distributions are very symmetric (Figure <a href="chapter7.html#fig:Figure7-10">7.10</a>).
The interpretation is
the same and this result reinforces the other assessments that the parametric
approach is not unreasonable here except possibly for the independence assumption. These randomization approaches provide no robustness against violations of the independence assumption. </p>

<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb647-1"><a href="chapter7.html#cb647-1" aria-hidden="true" tabindex="-1"></a>Tobs <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year, <span class="at">data =</span> bozemantemps)<span class="sc">$</span>coef[<span class="dv">2</span>]</span>
<span id="cb647-2"><a href="chapter7.html#cb647-2" aria-hidden="true" tabindex="-1"></a>Tobs</span></code></pre></div>
<pre><code>##       Year 
## 0.05244213</code></pre>
<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb649-1"><a href="chapter7.html#cb649-1" aria-hidden="true" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb649-2"><a href="chapter7.html#cb649-2" aria-hidden="true" tabindex="-1"></a>Tstar <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> B)</span>
<span id="cb649-3"><a href="chapter7.html#cb649-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> (b <span class="cf">in</span> (<span class="dv">1</span><span class="sc">:</span>B)){</span>
<span id="cb649-4"><a href="chapter7.html#cb649-4" aria-hidden="true" tabindex="-1"></a>  Tstar[b] <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year, <span class="at">data =</span> <span class="fu">resample</span>(bozemantemps))<span class="sc">$</span>coef[<span class="dv">2</span>]</span>
<span id="cb649-5"><a href="chapter7.html#cb649-5" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb649-6"><a href="chapter7.html#cb649-6" aria-hidden="true" tabindex="-1"></a>quantiles <span class="ot">&lt;-</span> <span class="fu">qdata</span>(Tstar, <span class="fu">c</span>(<span class="fl">0.025</span>, <span class="fl">0.975</span>))</span>
<span id="cb649-7"><a href="chapter7.html#cb649-7" aria-hidden="true" tabindex="-1"></a>quantiles</span></code></pre></div>
<pre><code>##       2.5%      97.5% 
## 0.04326952 0.06131044</code></pre>
<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb651-1"><a href="chapter7.html#cb651-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tibble</span>(Tstar) <span class="sc">%&gt;%</span>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Tstar)) <span class="sc">+</span> </span>
<span id="cb651-2"><a href="chapter7.html#cb651-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..ncount..), <span class="at">bins =</span> <span class="dv">15</span>, <span class="at">col =</span> <span class="dv">1</span>, <span class="at">fill =</span> <span class="st">&quot;skyblue&quot;</span>, <span class="at">center =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb651-3"><a href="chapter7.html#cb651-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..scaled..)) <span class="sc">+</span></span>
<span id="cb651-4"><a href="chapter7.html#cb651-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb651-5"><a href="chapter7.html#cb651-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;Density&quot;</span>) <span class="sc">+</span></span>
<span id="cb651-6"><a href="chapter7.html#cb651-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> quantiles, <span class="at">col =</span> <span class="st">&quot;blue&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>, <span class="at">lty =</span> <span class="dv">3</span>) <span class="sc">+</span>    </span>
<span id="cb651-7"><a href="chapter7.html#cb651-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> Tobs, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb651-8"><a href="chapter7.html#cb651-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">stat_bin</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..ncount.., <span class="at">label =</span> ..count..), <span class="at">bins =</span> <span class="dv">15</span>,</span>
<span id="cb651-9"><a href="chapter7.html#cb651-9" aria-hidden="true" tabindex="-1"></a>           <span class="at">geom =</span> <span class="st">&quot;text&quot;</span>, <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.75</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-10"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-10-1.png" alt="Bootstrap distribution of the slope coefficient in the Bozeman temperature linear regression model with bold dashed vertical lines delineating the 95% confidence interval and the bold solid line the observed slope of 0.052." width="75%" />
<p class="caption">
Figure 7.10: Bootstrap distribution of the slope coefficient in the Bozeman temperature linear regression model with bold dashed vertical lines delineating the 95% confidence interval and the bold solid line the observed slope of 0.052.
</p>
</div>
<!-- \newpage -->
</div>
<div id="section7-5" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Transformations part I: Linearizing relationships<a href="chapter7.html#section7-5" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When the influential point, linearity, constant variance and/or normality
assumptions are clearly violated, we cannot trust any of the inferences generated by the
regression model. The violations occur on gradients from minor to really major
problems. As we have seen from the examples in the previous chapters, it has
been hard to find data sets that were free of all issues. Furthermore, it may
seem hopeless to be able to make successful inferences in some of these
situations with the previous tools. There are three potential solutions to violations
of the validity conditions: </p>
<ol style="list-style-type: decimal">
<li><p>Consider removing an offending point or two and see if this improves the results,
presenting results both with and without those points to describe their impact<a href="#fn125" class="footnote-ref" id="fnref125"><sup>125</sup></a>,</p></li>
<li><p>Try to transform the response, explanatory, or both variables and see if you can
force the data set to meet our SLR assumptions after transformation (the focus
of this and the next section), or </p></li>
<li><p>Consider more advanced statistical models that can account for these issues
(the focus of subsequent statistics courses, if you continue on further).</p></li>
</ol>
<p><strong><em>Transformations</em></strong> involve applying a function to
one or both variables.
After applying this transformation, one hopes to have
alleviated whatever issues encouraged its consideration. <strong><em>Linear transformation
functions</em></strong>, of the form <span class="math inline">\(z_{\text{new}} = a*x+b\)</span>, will never help us to fix
assumptions in regression situations; linear transformations change the scaling
of the variables but not their shape or the relationship between two variables.
For example, in the Bozeman Temperature data example, we subtracted 1901 from
the <code>Year</code> variable to have <code>Year2</code> start at 0 and go up to 113. We could
also apply a linear transformation to change Temperature from being measured in
<span class="math inline">\(^\circ F\)</span> to <span class="math inline">\(^\circ C\)</span> using <span class="math inline">\(^\circ C = [^\circ F - 32] *(5/9)\)</span>. The
scatterplots on both the original and transformed scales are provided in
Figure <a href="chapter7.html#fig:Figure7-11">7.11</a>. All the
coefficients in the regression model and the labels on the axes change, but the
“picture” is still the same. Additionally, all the inferences remain the same –
p-values are unchanged by linear transformations. So linear transformations
can be “fun” but really are only useful if they make the coefficients easier to
interpret.
Here if you like temperature changes in <span class="math inline">\(^\circ C\)</span> for a 1 year increase, the
slope coefficient is 0.029 and if you like the original change in <span class="math inline">\(^\circ F\)</span> for
a 1 year increase, the slope coefficient is 0.052. More useful than this is the switch into units of 100 years (so each year increase would just be 0.1 instead of 1), so that the slope is the temperature change over 100 years.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-11"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-11-1.png" alt="Scatterplots of Temperature (\(^\circ F\)) versus Year (left) and Temperature (\(^\circ C\)) vs Years since 1901 (right)." width="75%" />
<p class="caption">
Figure 7.11: Scatterplots of <em>Temperature</em> (<span class="math inline">\(^\circ F\)</span>) versus <em>Year</em> (left) and <em>Temperature</em> (<span class="math inline">\(^\circ C\)</span>) vs <em>Years</em> since 1901 (right).
</p>
</div>
<!-- \vspace{11pt} -->
<div class="sourceCode" id="cb652"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb652-1"><a href="chapter7.html#cb652-1" aria-hidden="true" tabindex="-1"></a>bozemantemps <span class="ot">&lt;-</span> bozemantemps <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">meanmaxC =</span> (meanmax <span class="sc">-</span> <span class="dv">32</span>)<span class="sc">*</span>(<span class="dv">5</span><span class="sc">/</span><span class="dv">9</span>))</span>
<span id="cb652-2"><a href="chapter7.html#cb652-2" aria-hidden="true" tabindex="-1"></a>temp3 <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmaxC <span class="sc">~</span> Year2, <span class="at">data =</span> bozemantemps)</span>
<span id="cb652-3"><a href="chapter7.html#cb652-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(temp1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmax ~ Year, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.3779 -0.9300  0.1078  1.1960  5.8698 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) -47.35123    9.32184   -5.08 1.61e-06
## Year          0.05244    0.00476   11.02  &lt; 2e-16
## 
## Residual standard error: 1.624 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<!-- \newpage -->
<div class="sourceCode" id="cb654"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb654-1"><a href="chapter7.html#cb654-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(temp3)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = meanmaxC ~ Year2, data = bozemantemps)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.8766 -0.5167  0.0599  0.6644  3.2610 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 11.300703   0.174349   64.82   &lt;2e-16
## Year2        0.029135   0.002644   11.02   &lt;2e-16
## 
## Residual standard error: 0.9022 on 107 degrees of freedom
## Multiple R-squared:  0.5315, Adjusted R-squared:  0.5271 
## F-statistic: 121.4 on 1 and 107 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong><em>Nonlinear transformation functions</em></strong>
are where we apply something more complicated than this
shift and scaling, something like <span class="math inline">\(y_{\text{new}} = f(y)\)</span>, where <span class="math inline">\(f(\cdot)\)</span> could be
a log or power of the original variable <span class="math inline">\(y\)</span>. When we apply these sorts of
transformations,
interesting things can happen to our linear models and their problems.
Some
examples of transformations that are at least occasionally used for transforming
the response variable are provided in Table <a href="chapter7.html#tab:Table7-1">7.1</a>, ranging from taking
<span class="math inline">\(y\)</span> to different powers from <span class="math inline">\(y^{-2}\)</span> to <span class="math inline">\(y^2\)</span>. Typical transformations used in statistical
modeling exist along a gradient of powers of the response variable, defined as <span class="math inline">\(y^{\lambda}\)</span>
with <span class="math inline">\(\boldsymbol{\lambda}\)</span> being the power of the transformation of the response
variable and <span class="math inline">\(\lambda = 0\)</span> implying a log-transformation. Except for <span class="math inline">\(\lambda = 1\)</span>, the
transformations are all nonlinear functions of <span class="math inline">\(y\)</span>.</p>

<table>
<caption><span id="tab:Table7-1">Table 7.1: </span> Ladder of powers of transformations that are often used in statistical modeling.</caption>
<colgroup>
<col width="13%" />
<col width="34%" />
<col width="52%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"><strong>Power</strong></th>
<th align="left"><strong>Formula</strong></th>
<th align="left"><strong>Usage</strong>      </th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">2</td>
<td align="left"><span class="math inline">\(y^2\)</span></td>
<td align="left">seldom used</td>
</tr>
<tr class="even">
<td align="left">1</td>
<td align="left"><span class="math inline">\(y^1=y\)</span></td>
<td align="left">no change</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(1/2\)</span></td>
<td align="left"><span class="math inline">\(y^{0.5}=\sqrt{y}\)</span></td>
<td align="left">counts and area responses</td>
</tr>
<tr class="even">
<td align="left">0</td>
<td align="left"><span class="math inline">\(\log(y)\)</span> natural log of <span class="math inline">\(y\)</span></td>
<td align="left">counts, normality, curves,
non-constant variance</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-1/2\)</span></td>
<td align="left"><span class="math inline">\(y^{-0.5}=1/\sqrt{y}\)</span></td>
<td align="left">uncommon</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(-1\)</span></td>
<td align="left"><span class="math inline">\(y^{-1}=1/y\)</span></td>
<td align="left">for ratios</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(-2\)</span></td>
<td align="left"><span class="math inline">\(y^{-2}=1/y^2\)</span></td>
<td align="left">seldom used</td>
</tr>
</tbody>
</table>
<p>There are even more transformations possible, for example <span class="math inline">\(y^{0.33}\)</span> is sometimes useful
for variables
involved in measuring the volume of something. And we can also consider
applying any of these transformations to the explanatory variable, and consider
using them on both the response and explanatory variables at the same time. The
most common application of these ideas is to transform the response variable using
the log-transformation, at least as a starting point. In fact, the
log-transformation is so commonly used (or maybe overused), that we will just focus
on its use. It is so commonplace in some fields that some researchers
log-transform their data prior to even plotting it. In other
situations, such as when measuring acidity (pH), noise (decibels), or
earthquake size (Richter scale), the measurements are already on logarithmic
scales.</p>
<p>Actually, we have already analyzed data that benefited from a
<strong><em>log-transformation</em></strong> – the <em>log-area burned</em> vs. <em>summer temperature</em>
data for Montana. Figure <a href="chapter7.html#fig:Figure7-12">7.12</a> compares the
relationship between these variables on the original hectares scale and the
log-hectares scale.</p>
<div class="sourceCode" id="cb656"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb656-1"><a href="chapter7.html#cb656-1" aria-hidden="true" tabindex="-1"></a>p <span class="ot">&lt;-</span> mtfires <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Temperature, <span class="at">y =</span> hectares)) <span class="sc">+</span></span>
<span id="cb656-2"><a href="chapter7.html#cb656-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb656-3"><a href="chapter7.html#cb656-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;(a)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Hectares&quot;</span>) <span class="sc">+</span></span>
<span id="cb656-4"><a href="chapter7.html#cb656-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb656-5"><a href="chapter7.html#cb656-5" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb656-6"><a href="chapter7.html#cb656-6" aria-hidden="true" tabindex="-1"></a>plog <span class="ot">&lt;-</span> mtfires <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Temperature, <span class="at">y =</span> loghectares)) <span class="sc">+</span></span>
<span id="cb656-7"><a href="chapter7.html#cb656-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb656-8"><a href="chapter7.html#cb656-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;(b)&quot;</span>, <span class="at">y =</span> <span class="st">&quot;log-Hectares&quot;</span>) <span class="sc">+</span></span>
<span id="cb656-9"><a href="chapter7.html#cb656-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb656-10"><a href="chapter7.html#cb656-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb656-11"><a href="chapter7.html#cb656-11" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p, plog, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-12"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-12-1.png" alt="Scatterplots of Hectares (a) and log-Hectares (b) vs Temperature." width="75%" />
<p class="caption">
Figure 7.12: Scatterplots of Hectares (a) and log-Hectares (b) vs Temperature.
</p>
</div>
<p>Figure <a href="chapter7.html#fig:Figure7-12">7.12</a>(a) displays a relationship
that would be hard fit using SLR – it has a curve and the
variance is increasing with increasing temperatures. With a
log-transformation of <em>Hectares</em>, the relationship appears to be relatively
linear and have constant variance (in (b)). We considered regression models for this
situation previously. This shows at least one situation where a
log-transformation of a response variable can linearize a relationship and
reduce non-constant variance.</p>
<p>This transformation does not always work to “fix” curvilinear
relationships, in fact
in some situations it can make the relationship more nonlinear. For example,
reconsider the relationship between tree diameter and tree height, which
contained some curvature that we could not account for in an SLR.
Figure <a href="chapter7.html#fig:Figure7-13">7.13</a> shows
the original version of the variables and Figure <a href="chapter7.html#fig:Figure7-14">7.14</a>
shows the same
information with the response variable (height) log-transformed.
</p>
<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb657-1"><a href="chapter7.html#cb657-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(spuRs)</span>
<span id="cb657-2"><a href="chapter7.html#cb657-2" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(ufc)</span>
<span id="cb657-3"><a href="chapter7.html#cb657-3" aria-hidden="true" tabindex="-1"></a>ufc <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(ufc)</span>
<span id="cb657-4"><a href="chapter7.html#cb657-4" aria-hidden="true" tabindex="-1"></a>ufc <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="sc">-</span><span class="dv">168</span>) <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> dbh.cm, <span class="at">y =</span> height.m)) <span class="sc">+</span></span>
<span id="cb657-5"><a href="chapter7.html#cb657-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb657-6"><a href="chapter7.html#cb657-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb657-7"><a href="chapter7.html#cb657-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">se =</span> F, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb657-8"><a href="chapter7.html#cb657-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb657-9"><a href="chapter7.html#cb657-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;Tree height vs tree diameter&quot;</span>)</span>
<span id="cb657-10"><a href="chapter7.html#cb657-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb657-11"><a href="chapter7.html#cb657-11" aria-hidden="true" tabindex="-1"></a>ufc <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="sc">-</span><span class="dv">168</span>) <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> dbh.cm, <span class="at">y =</span> <span class="fu">log</span>(height.m))) <span class="sc">+</span></span>
<span id="cb657-12"><a href="chapter7.html#cb657-12" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb657-13"><a href="chapter7.html#cb657-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb657-14"><a href="chapter7.html#cb657-14" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">se =</span> F, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb657-15"><a href="chapter7.html#cb657-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb657-16"><a href="chapter7.html#cb657-16" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;log-tree height vs tree diameter&quot;</span>)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-13"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-13-1.png" alt="Scatterplot of tree height versus tree diameter." width="75%" />
<p class="caption">
Figure 7.13: Scatterplot of tree height versus tree diameter.
</p>
</div>
<p>Figure <a href="chapter7.html#fig:Figure7-14">7.14</a> with the log-transformed height response seems
to show a more nonlinear relationship and may even have more
issues with non-constant variance. This example shows that log-transforming
the response variable cannot fix all problems, even though I’ve seen some
researchers assume it can. It is OK to try a transformation but remember to
always plot the results to make sure it actually helped and did not make the
situation worse.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-14"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-14-1.png" alt="Scatterplot of log(tree height) versus tree diameter." width="75%" />
<p class="caption">
Figure 7.14: Scatterplot of log(tree height) versus tree diameter.
</p>
</div>
<p>All is not lost in this situation, we can consider two other potential
uses of the log-transformation
and see if they can “fix” the relationship up a bit. One option is to apply the
transformation to the explanatory variable (<code>y ~ log(x)</code>), displayed in
Figure <a href="chapter7.html#fig:Figure7-15">7.15</a>. If the distribution of the explanatory
variable is right skewed (see the boxplot on
the <span class="math inline">\(x\)</span>-axis), then consider log-transforming the explanatory variable. This will
often reduce the leverage of those most extreme observations which can be
useful. In this situation, it also seems to have been quite successful at
linearizing the relationship, leaving some minor
non-constant variance, but providing a big improvement from the relationship on
the original scale.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-15"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-15-1.png" alt="Scatterplot of tree height versus log(tree diameter)." width="75%" />
<p class="caption">
Figure 7.15: Scatterplot of tree height versus log(tree diameter).
</p>
</div>
<p>The other option, especially when everything else fails, is to apply the
log-transformation to
both the explanatory and response variables (<code>log(y) ~ log(x)</code>), as
displayed in Figure <a href="chapter7.html#fig:Figure7-16">7.16</a>. For this example, the
transformation seems to be better than the first two options
(none and only <span class="math inline">\(\log(y)\)</span>), but demonstrates some decreasing variability with
larger <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. It has also created a new and different curve in the
relationship (see the smoothing (dashed) line start below the SLR line, then go
above it, and the finish below it). In the end, we might prefer to fit an SLR
model to the tree <em>height</em> vs <em>log(diameter)</em> versions of the variables
(Figure <a href="chapter7.html#fig:Figure7-15">7.15</a>).</p>

<div class="sourceCode" id="cb658"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb658-1"><a href="chapter7.html#cb658-1" aria-hidden="true" tabindex="-1"></a>ufc <span class="sc">%&gt;%</span> <span class="fu">slice</span>(<span class="sc">-</span><span class="dv">168</span>) <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">log</span>(dbh.cm), <span class="at">y =</span> <span class="fu">log</span>(height.m))) <span class="sc">+</span></span>
<span id="cb658-2"><a href="chapter7.html#cb658-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb658-3"><a href="chapter7.html#cb658-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb658-4"><a href="chapter7.html#cb658-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">col =</span> <span class="st">&quot;red&quot;</span>, <span class="at">lwd =</span> <span class="dv">1</span>, <span class="at">se =</span> F, <span class="at">lty =</span> <span class="dv">2</span>) <span class="sc">+</span></span>
<span id="cb658-5"><a href="chapter7.html#cb658-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb658-6"><a href="chapter7.html#cb658-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">&quot;log-tree height vs log-tree diameter&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-16"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-16-1.png" alt="Scatterplot of log(tree height) versus log(tree diameter)." width="75%" />
<p class="caption">
Figure 7.16: Scatterplot of log(tree height) versus log(tree diameter).
</p>
</div>
<p>Economists also like to use <span class="math inline">\(\log(y) \sim \log(x)\)</span> transformations. The
log-log transformation
tends to linearize certain relationships and has specific interpretations in
terms of Economics theory. The log-log transformation shows up in many
different disciplines as a way of obtaining a linear relationship on
the log-log scale,
but different fields discuss it differently. The following example shows a
situation where transformations of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are required and
this double transformation seems to be quite successful in what
looks like an initially hopeless situation for our linear modeling approach.</p>
<p>Data were collected in 1988 on the rates of infant mortality
(infant deaths per 1,000
live births) and gross domestic product (GDP) per capita (in 1998 US dollars)
from <span class="math inline">\(n = 207\)</span> countries. These data are available from the <code>carData</code> package
(<span class="citation">Fox, Weisberg, and Price (<a href="#ref-R-carData" role="doc-biblioref">2022b</a>)</span>, <span class="citation">(<a href="#ref-Fox2003" role="doc-biblioref"><strong>Fox2003?</strong></a>)</span>) in a data set called <code>UN</code>.
The four panels in
Figure <a href="chapter7.html#fig:Figure7-17">7.17</a> show the original relationship and the impacts of
log-transforming one or both variables.
The only scatterplot that could potentially be modeled using SLR is the lower
right panel (d) that shows the relationship between <em>log(infant mortality)</em>
and <em>log(GDP)</em>. In the next section, we will fit models to some of these
relationships and use our diagnostic plots to help us assess “success” of
the transformations.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-17"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-17-1.png" alt="Scatterplots of Infant Mortality vs GDP under four different combinations of log-transformations." width="75%" />
<p class="caption">
Figure 7.17: Scatterplots of Infant Mortality vs GDP under four different combinations of log-transformations.
</p>
</div>
<p>Almost all nonlinear transformations assume that the variables are strictly
greater than 0. For example, consider what happens when we apply the
<code>log</code> function to 0 or a negative value in R:</p>
<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb659-1"><a href="chapter7.html#cb659-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(<span class="sc">-</span><span class="dv">1</span>)</span></code></pre></div>
<pre><code>## [1] NaN</code></pre>
<div class="sourceCode" id="cb661"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb661-1"><a href="chapter7.html#cb661-1" aria-hidden="true" tabindex="-1"></a><span class="fu">log</span>(<span class="dv">0</span>)</span></code></pre></div>
<pre><code>## [1] -Inf</code></pre>
<p>So be careful to think about the domain of the transformation function
before using transformations.
For
example, when using the log-transformation make sure that the data values are
non-zero and positive or you will get some surprises when you go to fit your
regression model to a data set with NaNs (not a number) and/or
<span class="math inline">\(-\infty\text{&#39;s}\)</span> in it. When using fractional powers (square-roots or similar), just having non-negative values are required and so 0 is acceptable.</p>
<p>Sometimes the log-transformations will not be entirely successful. If the
relationship is <strong><em>monotonic</em></strong> (strictly positive or strictly negative
but not both), then possibly another stop
on the ladder of transformations in Table <a href="chapter7.html#tab:Table7-1">7.1</a> might work.
If the relationship
is not monotonic, then it may be better to consider a more complex regression
model that can accommodate the shape in the relationship or to bin the predictor, response, or both into categories so you can use ANOVA or Chi-square methods and avoid at least the linearity assumption.</p>
<p>Finally, remember that <code>log</code> in statistics and especially in R means the
<strong><em>natural log</em></strong> (<em>ln</em> or log base <em>e</em> as you might see it elsewhere). In
these situations, applying the <code>log10</code> function (which provides log base 10)
to the variables would lead to very similar results, but readers may assume
you used <em>ln</em> if you don’t state that you used <span class="math inline">\(log_{10}\)</span>. The main thing
to remember to do is to be clear when communicating the version you are
using. As an example, I was working with researchers on a study
<span class="citation">(<a href="#ref-Dieser2010" role="doc-biblioref"><strong>Dieser2010?</strong></a>)</span> related to impacts of environmental
stresses on bacterial survival. The response variable was log-transformed
counts and involved smoothed regression lines fit on this scale. I was using
natural logs to fit the models and then shared the fitted values from the
models and my collaborators back-transformed the results assuming that I had
used <span class="math inline">\(log_{10}\)</span>. We quickly resolved our differences once we discovered them but
this serves as a reminder at how important communication is in group projects
– we both said we were working with log-transformations and didn’t know that we
defaulted to different bases.</p>
<table>
<tbody>
<tr class="odd">
<td align="center">Generally, in statistics, it’s safe to assume that everything is log base <em>e</em> unless otherwise specified.</td>
</tr>
</tbody>
</table>
</div>
<div id="section7-6" class="section level2 hasAnchor" number="7.6">
<h2><span class="header-section-number">7.6</span> Transformations part II: Impacts on SLR interpretations: log(y), log(x), &amp; both log(y) &amp; log(x)<a href="chapter7.html#section7-6" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The previous attempts to linearize relationships imply a desire to be
able to fit SLR models.
The <em>log</em>-transformations, when successful,
provide the potential to validly apply our SLR model. There are then two options
for interpretations: you can either interpret the model on the
transformed scale or you can translate the SLR model on the transformed scale
back to the original scale of the variables. It ends up that
<em>log</em>-transformations have special interpretations on the original scales
depending on whether the <em>log</em> was applied to the response variable, the
explanatory variable, or both.</p>
<!-- \newpage -->
<p><strong>Scenario 1: log(y) vs x model:</strong></p>
<p>First consider the <span class="math inline">\(\log(y) \sim x\)</span> situations where the estimated model
is of the form <span class="math inline">\(\widehat{\log(y)} = b_0 + b_1x\)</span>. When only the response is
<em>log</em>-transformed, some people call this a <strong><em>semi-log model</em></strong>.
But many
researchers will use this model without any special considerations, as
long as it provides
a situation where the SLR assumptions are reasonably well-satisfied. To
understand the properties and eventually the interpretation of
transformed-variables models, we need to try to “reverse” our transformation.
If we exponentiate<a href="#fn126" class="footnote-ref" id="fnref126"><sup>126</sup></a> both sides of <span class="math inline">\(\log(y) = b_0 + b_1x\)</span>, we get:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-18"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-18-1.png" alt="Plot of the estimated SLR (a) and implied model for the median on the original Hectares scale (b) for the area burned vs temperature data." width="75%" />
<p class="caption">
Figure 7.18: Plot of the estimated SLR (a) and implied model for the median on the original Hectares scale (b) for the area burned vs temperature data.
</p>
</div>
<ul>
<li><p><span class="math inline">\(\exp(\log(y)) = \exp(b_0 + b_1x)\)</span>, <em>which is</em></p></li>
<li><p><span class="math inline">\(y = \exp(b_0 + b_1x)\)</span>, <em>which can be re-written as</em></p></li>
<li><p><span class="math inline">\(y = \exp(b_0)\exp(b_1x)\)</span>. <em>This is based on the rules for</em> <code>exp()</code> <em>where</em>
<span class="math inline">\(\exp(a+b) = \exp(a)\exp(b)\)</span>.</p></li>
<li><p>Now consider what happens if we increase <span class="math inline">\(x\)</span> by 1 unit, going from <span class="math inline">\(x\)</span>
to <span class="math inline">\(x+1\)</span>, providing a new predicted <span class="math inline">\(y\)</span> that we can call <span class="math inline">\(y^*\)</span>:
<span class="math inline">\(y^* = \exp(b_0)\exp[b_1(x+1)]\)</span>:</p></li>
<li><p><span class="math inline">\(y^* = {\color{red}{\underline{\boldsymbol{\exp(b_0)\exp(b_1x)}}}}\exp(b_1)\)</span>.
<em>Now note that the underlined, bold component was the y-value for</em> <span class="math inline">\(x\)</span>.</p></li>
<li><p><span class="math inline">\(y^* = {\color{red}{\boldsymbol{y}}}\exp(b_1)\)</span>. <em>Found by replacing</em>
<span class="math inline">\(\color{red}{\mathbf{\exp(b_0)\exp(b_1x)}}\)</span> <em>with</em> <span class="math inline">\(\color{red}{\mathbf{y}}\)</span>,
<em>the value for</em> <span class="math inline">\(x\)</span>.</p></li>
</ul>
<p>So the difference in fitted values between <span class="math inline">\(x\)</span> and <span class="math inline">\(x+1\)</span> is to multiply the
result for <span class="math inline">\(x\)</span> (that was predicting <span class="math inline">\(\color{red}{\mathbf{y}}\)</span>) by
<span class="math inline">\(\exp(b_1)\)</span> to get to the predicted result for <span class="math inline">\(x+1\)</span> (called <span class="math inline">\(y^*\)</span>).
We can then use this result to form our <span class="math inline">\(\mathit{\boldsymbol{\log(y)\sim x}}\)</span>
<strong><em>slope interpretation</em></strong>:
for a 1 unit increase in <span class="math inline">\(x\)</span>, we observe a multiplicative change of
<span class="math inline">\(\mathbf{exp(b_1)}\)</span> in the response. When we compute a mean on logged variables
that are symmetrically distributed (this should occur if our transformation was
successful) and then exponentiate the results, the proper interpretation is
that the changes are happening in the <strong>median</strong> of the original responses.
This is the only time in the course that we will switch our inferences to
medians instead of means, and we don’t do this because
we want to, we do it because it is result of modeling on the <span class="math inline">\(\log(y)\)</span> scale,
if successful.</p>
<p>So there are a couple of ways to interpret these results in general:</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>log-scale interpretation of log(y) only model</em></strong>: for a 1 unit
increase in <span class="math inline">\(x\)</span>, we estimate a <span class="math inline">\(b_1\)</span> unit change in the mean of <span class="math inline">\(\log(y)\)</span> or</p></li>
<li><p><strong><em>original scale interpretation of log(y) only model</em></strong>: for a 1 unit
increase in <span class="math inline">\(x\)</span>, we estimate a <span class="math inline">\(exp(b_1)\)</span> times change in the median of <span class="math inline">\(y\)</span>.</p></li>
</ol>
<p>When we are working with regression equations, slopes can either be positive or
negative and our interpretations change based on this result to either result
in growth (<span class="math inline">\(b_1&gt;0\)</span>) or decay (<span class="math inline">\(b_1&lt;0\)</span>) in the responses as the explanatory
variable is increased. As an example, consider <span class="math inline">\(b_1 = 0.4\)</span> and
<span class="math inline">\(\exp(b_1) = \exp(0.4) = 1.492\)</span>. There are a couple of ways to interpret
this on the original scale of the response variable <span class="math inline">\(y\)</span>:</p>
<p>For <span class="math inline">\(\mathbf{b_1&gt;0}\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>For a 1 unit increase in <span class="math inline">\(x\)</span>, the median of <span class="math inline">\(y\)</span> is estimated to change by 1.492 times.</p></li>
<li><p>We can convert this into a <strong>percentage increase</strong> by subtracting 1
from <span class="math inline">\(\exp(0.4)\)</span>, <span class="math inline">\(1.492-1.0 = 0.492\)</span> and multiplying the result by 100,
<span class="math inline">\(0.492*100 = 49.2\%\)</span>. This is interpreted as: For a 1 unit increase in <span class="math inline">\(x\)</span>,
the median of <span class="math inline">\(y\)</span> is estimated to increase by 49.2%.</p></li>
</ol>
<div class="sourceCode" id="cb663"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb663-1"><a href="chapter7.html#cb663-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fl">0.4</span>)</span></code></pre></div>
<pre><code>## [1] 1.491825</code></pre>
<p>For <span class="math inline">\(\mathbf{b_1&lt;0}\)</span>, the change on the <em>log</em>-scale is negative and that
implies on the original scale that the curve decays to 0. For example,
consider <span class="math inline">\(b_1 = -0.3\)</span> and <span class="math inline">\(\exp(-0.3) = 0.741\)</span>. Again,
there are two versions of the interpretation possible:</p>
<ol style="list-style-type: decimal">
<li><p>For a 1 unit increase in <span class="math inline">\(x\)</span>, the median of <span class="math inline">\(y\)</span> is estimated to change by 0.741 times.</p></li>
<li><p>For negative slope coefficients, the percentage decrease is calculated
as <span class="math inline">\((1-\exp(b_1))*100\%\)</span>. For <span class="math inline">\(\exp(-0.3) = 0.741\)</span>, this is
<span class="math inline">\((1-0.741)*100 = 25.9\%\)</span>. This is interpreted as: For a 1 unit increase
in <span class="math inline">\(x\)</span>, the median of <span class="math inline">\(y\)</span> is estimated to decrease by 25.9%.</p></li>
</ol>
<p>We suspect that you will typically prefer interpretation #1 for both
directions but it is important to be able
think about the results in terms of <strong><em>% change of the medians</em></strong> to make
the scale of change more understandable. Some examples will help us see how
these ideas can be used in applications.</p>
<p>For the area burned data set, the estimated regression model is
<span class="math inline">\(\log(\widehat{\text{hectares}}) = -69.8+1.39\cdot\text{ Temp}\)</span>. On the
original scale, this implies that the model is
<span class="math inline">\(\widehat{\text{hectares}} = \exp(-69.8)\exp(1.39\text{ Temp})\)</span>.
Figure <a href="chapter7.html#fig:Figure7-18">7.18</a> provides the <span class="math inline">\(\log(y)\)</span> scale version of
the model and the model transformed to the
original scale of measurement. On the log-hectares scale, the interpretation of
the slope is: For a 1<span class="math inline">\(^\circ F\)</span> increase in summer temperature, we
estimate a 1.39 log-hectares/1<span class="math inline">\(^\circ F\)</span> change, on average, in the log-area
burned. On the original scale: A 1<span class="math inline">\(^\circ F\)</span> increase in temperature is
related to an estimated multiplicative change in the median number of hectares burned of
<span class="math inline">\(\exp(1.39) = 4.01\)</span> times higher areas. That seems like a big rate of growth but the curve does
grow rapidly as shown in panel (b), especially for values over
58<span class="math inline">\(^\circ F\)</span> where the area burned is starting to be really large. You can think
of the multiplicative change here in the following way: the median
number of hectares burned is 4 times higher at 58<span class="math inline">\(^\circ F\)</span> than at
57<span class="math inline">\(^\circ F\)</span> and the median area burned is 4 times larger at 59<span class="math inline">\(^\circ F\)</span>
than at 58<span class="math inline">\(^\circ F\)</span>… This can also be interpreted on a % change scale:
A 1<span class="math inline">\(^\circ F\)</span> increase in temperature is related
to an estimated <span class="math inline">\((4.01-1)*100 = 301\%\)</span> increase in the median number of hectares burned.</p>

<p><strong>Scenario 2: y vs log(x) model:</strong></p>
<p>When only the explanatory variable is log-transformed, it has a
different sort of
impact on the regression model interpretation.
Effectively we move the
percentage change onto the <span class="math inline">\(x\)</span>-scale and modify the first part of our slope
interpretation when we consider the results on the original scale for <span class="math inline">\(x\)</span>. Once again,
we will consider the mathematics underlying the changes in the model and then
work on applying it to real situations. When the explanatory variable is
logged, the estimated regression model is
<span class="math inline">\(\color{red}{\boldsymbol{y = b_0+b_1\log(x)}}\)</span>. This models the relationship
between <span class="math inline">\(y\)</span> and <span class="math inline">\(x\)</span> in terms of multiplicative changes in <span class="math inline">\(x\)</span> having an
effect on the average <span class="math inline">\(y\)</span>.</p>
<p>To develop an interpretation on the <span class="math inline">\(x\)</span>-scale
(not <span class="math inline">\(\log(x)\)</span>), consider the impact of doubling <span class="math inline">\(x\)</span>. This change will
take us from the point (<span class="math inline">\(x,\color{red}{\boldsymbol{y = b_0+b_1\log(x)}}\)</span>)
to the point <span class="math inline">\((2x,\boldsymbol{y^* = b_0+b_1\log(2x)})\)</span>. Now the impact of
doubling <span class="math inline">\(x\)</span> can
be simplified using the rules for logs to be:</p>
<ul>
<li><p><span class="math inline">\(\boldsymbol{y^* = b_0+b_1\log(2x)}\)</span>,</p></li>
<li><p><span class="math inline">\(\boldsymbol{y^*} = {\color{red}{\underline{\boldsymbol{b_0+b_1\log(x)}}}} + b_1\log(2)\)</span>.   <em>Based on the rules for logs:</em> <span class="math inline">\(log(2x) = log(x)+log(2)\)</span>.</p></li>
<li><p><span class="math inline">\(y^* = {\color{red}{\boldsymbol{y}}}+b_1\log(2)\)</span></p></li>
<li><p>So if we double <span class="math inline">\(x\)</span>, we change the <strong>mean</strong> of <span class="math inline">\(y\)</span> by <span class="math inline">\(b_1\log(2)\)</span>.</p></li>
</ul>
<p>As before, there are couple of ways to interpret these sorts of results,
</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>log-scale interpretation of log(x) only model</em></strong>: for a 1 log-unit
increase in <span class="math inline">\(x\)</span>, we estimate a <span class="math inline">\(b_1\)</span> unit change in the mean of <span class="math inline">\(y\)</span> or</p></li>
<li><p><strong><em>original scale interpretation of log(x) only model</em></strong>: for a
doubling of <span class="math inline">\(x\)</span>, we estimate a <span class="math inline">\(b_1\log(2)\)</span> change in the mean of <span class="math inline">\(y\)</span>.
Note that both interpretations are for the mean of the <span class="math inline">\(y\text{&#39;s}\)</span>
since we haven’t changed the <span class="math inline">\(y\sim\)</span> part of the model.</p></li>
</ol>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-19"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-19-1.png" alt="Plot of the observations and estimated SLR model (mortality ~ log(GDP)) (top) and implied model (bottom) for the infant mortality data." width="75%" />
<p class="caption">
Figure 7.19: Plot of the observations and estimated SLR model (mortality ~ log(GDP)) (top) and implied model (bottom) for the infant mortality data.
</p>
</div>
<p>While it is not a perfect model (no model is), let’s consider the model
for <em>infant mortality</em> <span class="math inline">\(\sim\)</span> <em>log(GDP)</em> in order to practice the
interpretation using this type of model. This model was estimated to be
<span class="math inline">\(\widehat{\text{infantmortality}} = 155.77-14.86\cdot\log(\text{GDP})\)</span>. The first
(simplest) interpretation of the slope coefficient is: For a 1 log-dollar
increase in GDP per capita, we estimate infant
mortality to change, on average, by -14.86 deaths/1000 live births. The
second interpretation is on the original GDP scale: For a doubling of GDP, we
estimate infant mortality to change, on average, by <span class="math inline">\(-14.86\log(2) = -10.3\)</span>
deaths/1000 live births. Or, the mean infant mortality is reduced by
10.3 deaths per 1000 live births for each doubling of
GDP. Both versions of the model are displayed in Figure <a href="chapter7.html#fig:Figure7-19">7.19</a>
– one on the scale
the SLR model was fit (panel a) and the other on the original <span class="math inline">\(x\)</span>-scale (panel b)
that matches these last interpretations.</p>
<div class="sourceCode" id="cb665"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb665-1"><a href="chapter7.html#cb665-1" aria-hidden="true" tabindex="-1"></a>ID1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(infantMortality <span class="sc">~</span> <span class="fu">log</span>(ppgdp), <span class="at">data =</span> UN)</span>
<span id="cb665-2"><a href="chapter7.html#cb665-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ID1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = infantMortality ~ log(ppgdp), data = UN)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.239 -11.609  -2.829   8.122  82.183 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept) 155.7698     7.2431   21.51   &lt;2e-16
## log(ppgdp)  -14.8617     0.8468  -17.55   &lt;2e-16
## 
## Residual standard error: 18.14 on 191 degrees of freedom
## Multiple R-squared:  0.6172, Adjusted R-squared:  0.6152 
## F-statistic:   308 on 1 and 191 DF,  p-value: &lt; 2.2e-16</code></pre>
<div class="sourceCode" id="cb667"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb667-1"><a href="chapter7.html#cb667-1" aria-hidden="true" tabindex="-1"></a><span class="sc">-</span><span class="fl">14.86</span><span class="sc">*</span><span class="fu">log</span>(<span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] -10.30017</code></pre>
<p>It appears that our model does not fit too well and that there might be
some non-constant variance so we
should check the diagnostic plots (available in Figure <a href="chapter7.html#fig:Figure7-20">7.20</a>)
before we trust
any of those previous interpretations.</p>

<div class="sourceCode" id="cb669"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb669-1"><a href="chapter7.html#cb669-1" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</span>
<span id="cb669-2"><a href="chapter7.html#cb669-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ID1)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-20"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-20-1.png" alt="Diagnostics plots of the infant mortality model with log(GDP)." width="75%" />
<p class="caption">
Figure 7.20: Diagnostics plots of the infant mortality model with log(GDP).
</p>
</div>
<p>There appear to be issues with outliers and a long right tail violating
the normality assumption as it suggests a clear right skewed residual distribution. There is
curvature and non-constant variance in the results as well. There are no
influential points, but we are far from happy with this model and will be
revisiting this example with the responses also transformed. Remember that the
log-transformation of the response can potentially fix non-constant variance,
normality, and curvature issues.</p>
<!-- \newpage -->
<p><strong>Scenario 3: log(y) ~ log(x) model</strong></p>
<p>A final model combines log-transformations of both <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, combining the interpretations used in the previous two situations. This model is called the
<strong><em>log-log model</em></strong> and in some fields is also called the <strong><em>power law model</em></strong>.
The power-law model is usually written as <span class="math inline">\(y = \beta_0x^{\beta_1}+\varepsilon\)</span>,
where <span class="math inline">\(y\)</span> is thought to be proportional to <span class="math inline">\(x\)</span> raised to an estimated power
of <span class="math inline">\(\beta_1\)</span> (linear if <span class="math inline">\(\beta_1 = 1\)</span> and quadratic if <span class="math inline">\(\beta_1 = 2\)</span>). It is
one of the models that has been used in Geomorphology to model the shape of
glaciated valley elevation profiles
(that classic U-shape that comes with glacier-eroded mountain valleys)<a href="#fn127" class="footnote-ref" id="fnref127"><sup>127</sup></a>. If you
ignore the error term, it is possible to estimate the power-law model using our
SLR approach. Consider the log-transformation of both sides of this equation
starting with the power-law version:</p>
<ul>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0x^{\beta_1})\)</span>,</p></li>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0) + \log(x^{\beta_1}).\)</span>   <em>Based on the rules
for logs:</em> <span class="math inline">\(\log(ab) = \log(a) + \log(b)\)</span>.</p></li>
<li><p><span class="math inline">\(\log(y) = \log(\beta_0) + \beta_1\log(x).\)</span>   <em>Based on the rules
for logs:</em> <span class="math inline">\(\log(x^b) = b\log(x)\)</span>.</p></li>
</ul>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-21"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-21-1.png" alt="Plot of the observations and estimated SLR model log(mortality) \(\sim\) log(GDP) (left) and implied model (right) for the infant mortality data." width="75%" />
<p class="caption">
Figure 7.21: Plot of the observations and estimated SLR model log(mortality) <span class="math inline">\(\sim\)</span> log(GDP) (left) and implied model (right) for the infant mortality data.
</p>
</div>
<p>So other than <span class="math inline">\(\log(\beta_0)\)</span> in the model, this looks just like our
regular SLR model with <span class="math inline">\(x\)</span>
and <span class="math inline">\(y\)</span> both log-transformed. The slope coefficient for <span class="math inline">\(\log(x)\)</span> is the
power coefficient in the original power law model and determines whether
the relationship between the original <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> in <span class="math inline">\(y = \beta_0x^{\beta_1}\)</span>
is linear <span class="math inline">\((y = \beta_0x^1)\)</span> or quadratic <span class="math inline">\((y = \beta_0x^2)\)</span> or even quartic
<span class="math inline">\((y = \beta_0x^4)\)</span> in some really heavily glacier carved
U-shaped valleys. There are some issues with “ignoring the errors” in using SLR
to estimate these models <span class="citation">(<a href="#ref-Greenwood2002" role="doc-biblioref"><strong>Greenwood2002?</strong></a>)</span> but it is still a
pretty powerful result to be able to estimate the coefficients in
<span class="math inline">\((y = \beta_0x^{\beta_1})\)</span> using SLR.</p>
<p>We don’t typically use
the previous ideas to interpret the typical log-log regression model, instead
we combine our two previous interpretation techniques to generate our
interpretation.</p>
<!-- \newpage -->
<p>We need to work out the mathematics of doubling <span class="math inline">\(x\)</span> and the
changes in <span class="math inline">\(y\)</span> starting with the <span class="math inline">\(\mathit{\boldsymbol{\log(y)\sim \log(x)}}\)</span>
<strong><em>model</em></strong> that we would get out of fitting the SLR with both variables
log-transformed:</p>
<ul>
<li><p><span class="math inline">\(\log(y) = b_0 + b_1\log(x)\)</span>,</p></li>
<li><p><span class="math inline">\(y = \exp(b_0 + b_1\log(x))\)</span>.   <em>Exponentiate
both sides</em>.</p></li>
<li><p><span class="math inline">\(y = \exp(b_0)\exp(b_1\log(x)) = \exp(b_0)x^{b_1}\)</span>.   <em>Rules for
exponents and logs, simplifying.</em></p></li>
</ul>
<p>Now we can consider the impacts of doubling <span class="math inline">\(x\)</span> on <span class="math inline">\(y\)</span>, going from
<span class="math inline">\((x,{\color{red}{\boldsymbol{y = \exp(b_0)x^{b_1}}}})\)</span> to <span class="math inline">\((2x,y^*)\)</span> with</p>
<ul>
<li><p><span class="math inline">\(y^* = \exp(b_0)(2x)^{b_1}\)</span>,</p></li>
<li><p><span class="math inline">\(y^* = \exp(b_0)2^{b_1}x^{b_1} = 2^{b_1}{\color{red}{\boldsymbol{\exp(b_0)x^{b_1}}}} = 2^{b_1}{\color{red}{\boldsymbol{y}}}\)</span></p></li>
</ul>
<p>So doubling <span class="math inline">\(x\)</span> leads to a multiplicative change in the median of <span class="math inline">\(y\)</span>
of <span class="math inline">\(2^{b_1}\)</span>.</p>
<p>Let’s apply this idea to the GDP and infant mortality data where a
<span class="math inline">\(\log(x) \sim \log(y)\)</span> transformation
actually made the resulting relationship look like it might be close to being reasonably modeled with an SLR. The regression line in Figure <a href="chapter7.html#fig:Figure7-21">7.21</a> actually looks
pretty good on both
the estimated log-log scale (panel a) and on the original scale (panel b) as it
captures the severe nonlinearity in the relationship between the two variables.</p>
<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb670-1"><a href="chapter7.html#cb670-1" aria-hidden="true" tabindex="-1"></a>ID2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">log</span>(infantMortality) <span class="sc">~</span> <span class="fu">log</span>(ppgdp), <span class="at">data =</span> UN)</span>
<span id="cb670-2"><a href="chapter7.html#cb670-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ID2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = log(infantMortality) ~ log(ppgdp), data = UN)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -1.16789 -0.36738 -0.02351  0.24544  2.43503 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  8.10377    0.21087   38.43   &lt;2e-16
## log(ppgdp)  -0.61680    0.02465  -25.02   &lt;2e-16
## 
## Residual standard error: 0.5281 on 191 degrees of freedom
## Multiple R-squared:  0.7662, Adjusted R-squared:  0.765 
## F-statistic: 625.9 on 1 and 191 DF,  p-value: &lt; 2.2e-16</code></pre>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-22"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-22-1.png" alt="Diagnostic plots for the log-log infant mortality model." width="75%" />
<p class="caption">
Figure 7.22: Diagnostic plots for the log-log infant mortality model.
</p>
</div>
<p>The estimated regression model is
<span class="math inline">\(\log(\widehat{\text{infantmortality}}) = 8.104-0.617\cdot\log(\text{GDP})\)</span>.
The slope coefficient can be interpreted two ways.
</p>
<ol style="list-style-type: decimal">
<li><p><strong><em>On the log-log scale:</em></strong> For a 1 log-dollar increase in <em>GDP</em>, we estimate, on average,
a change of <span class="math inline">\(-0.617\)</span> log(deaths/1000 live births) in <em>infant mortality</em>.</p></li>
<li><p><strong><em>On the original scale:</em></strong> For a doubling of <em>GDP</em>, we expect a
<span class="math inline">\(2^{b_1} = 2^{-0.617} = 0.652\)</span> multiplicative change in the estimated median <em>infant mortality</em>.
That is a 34.8%
decrease in the estimated median <em>infant mortality</em> for each doubling of <em>GDP</em>.</p></li>
</ol>
<p>The diagnostics of the log-log SLR model (Figure <a href="chapter7.html#fig:Figure7-22">7.22</a>)
show minimal evidence of violations of assumptions although the tails of
the residuals are a little heavy (more spread out than a normal distribution)
and there might still be a little pattern remaining in the residuals vs fitted values.
There are no influential points to be concerned about in this situation.</p>
<p>While we will not revisit this at all except in the case-studies
in Chapter <a href="chapter9.html#chapter9">9</a>,
log-transformations can be applied to the response variable in ONE and TWO-WAY
ANOVA models when we are concerned about non-constant variance and
non-normality issues<a href="#fn128" class="footnote-ref" id="fnref128"><sup>128</sup></a>. The remaining methods in this chapter return to SLR and assuming
that the model is at least reasonable to consider in each situation, possibly after transformation(s). In fact, the methods in Section <a href="chapter7.html#section7-7">7.7</a>
are some of the most sensitive results to violations of the assumptions that we
will explore.</p>
<!-- \newpage -->
</div>
<div id="section7-7" class="section level2 hasAnchor" number="7.7">
<h2><span class="header-section-number">7.7</span> Confidence interval for the mean and prediction intervals for a new observation<a href="chapter7.html#section7-7" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Figure <a href="chapter7.html#fig:Figure7-7">7.7</a> provided a term-plot of the estimated
regression line and a shaded area surrounding
the estimated regression equation. Those shaded areas are based on connecting
the dots on 95% confidence intervals constructed for the true mean <span class="math inline">\(y\)</span> value
across all the <span class="math inline">\(x\)</span>-values. To formalize this idea, consider a specific value
of <span class="math inline">\(x\)</span>, and call it <span class="math inline">\(\boldsymbol{x_{\nu}}\)</span> (pronounced <strong>x-new</strong><a href="#fn129" class="footnote-ref" id="fnref129"><sup>129</sup></a>).
Then the true mean response for this <strong><em>subpopulation</em></strong> (a subpopulation
is all observations we could obtain at <span class="math inline">\(\boldsymbol{x = x_{\nu}}\)</span>) is given by
<span class="math inline">\(\boldsymbol{E(Y) = \mu_\nu = \beta_0+\beta_1x_{\nu}}\)</span>. To estimate the mean
response at <span class="math inline">\(\boldsymbol{x_{\nu}}\)</span>, we plug <span class="math inline">\(\boldsymbol{x_{\nu}}\)</span> into the
estimated regression equation: </p>
<p><span class="math display">\[\hat{\mu}_{\nu} = b_0 + b_1x_{\nu}.\]</span></p>
<p>To form the confidence interval, we appeal to our standard formula of
<span class="math inline">\(\textbf{estimate} \boldsymbol{\mp t^*}\textbf{SE}_{\textbf{estimate}}\)</span>. The
<strong><em>standard error for the estimated mean at any <span class="math inline">\(x\)</span>-value</em></strong>, denoted
<span class="math inline">\(\text{SE}_{\hat{\mu}_{\nu}}\)</span>, can be calculated as</p>
<p><span class="math display">\[\text{SE}_{\hat{\mu}_{\nu}}
= \sqrt{\text{SE}^2_{b_1}(x_{\nu}-\bar{x})^2 + \frac{\hat{\sigma}^2}{n}}\]</span></p>
<p>where <span class="math inline">\(\hat{\sigma}^2\)</span> is the squared residual standard error. This formula
combines the variability in the slope estimate, <span class="math inline">\(\text{SE}_{b_1}\)</span>, scaled
based on the distance of <span class="math inline">\(x_{\nu}\)</span> from <span class="math inline">\(\bar{x}\)</span> and the variability
around the regression line, <span class="math inline">\(\hat{\sigma}^2\)</span>. Fortunately, R’s
<code>predict</code> function can be used to provide these results for us
and avoid doing this calculation by hand most of the time. The
<strong><em>confidence interval for</em></strong> <span class="math inline">\(\boldsymbol{\mu_{\nu}}\)</span>, the population
mean response at <span class="math inline">\(x_{\nu}\)</span>, is</p>
<p><span class="math display">\[\boldsymbol{\hat{\mu}_{\nu} \mp
t^*_{n-2}}\textbf{SE}_{\boldsymbol{\hat{\mu}_{\nu}}}.\]</span></p>
<p>In application, these intervals get wider
the further we go from the mean of the <span class="math inline">\(x\text{&#39;s}\)</span>. These have
interpretations that are exactly like those for the y-intercept:</p>
<blockquote>
<p>For an <span class="math inline">\(x\)</span>-value of <span class="math inline">\(\boldsymbol{x_{\nu}}\)</span>, we are __% confident that
the true mean of <strong>y</strong> is between <strong>LL</strong> and <strong>UL</strong> <strong>[<em>units of y</em>]</strong>.</p>
</blockquote>
<p>It is also useful to remember that this
interpretation applies individually to every <span class="math inline">\(x\)</span> displayed in term-plots.</p>
<p>A second type of interval in this situation takes on a more challenging
task – to place an interval on where we think a new observation will fall,
called a <strong><em>prediction interval</em></strong> (PI). This PI will need to be much wider
than the CI for the mean since we need to account for both the uncertainty
in the mean and the
randomness in sampling a new observation from the normal distribution centered
at the true mean for <span class="math inline">\(x_{\nu}\)</span>. The interval is centered at the estimated
regression line (where else could we center it?) with the estimate denoted
as <span class="math inline">\(\hat{y}_{\nu}\)</span> to help us see that this interval is for a <strong>new</strong> <span class="math inline">\(y\)</span>
at this <span class="math inline">\(x\)</span>-value. The <span class="math inline">\(\text{SE}_{\hat{y}_{\nu}}\)</span> incorporates the core of the
previous SE calculation and adds in the variability of a new observation in
<span class="math inline">\(\boldsymbol{\hat{\sigma}^2}\)</span>:</p>
<p><span class="math display">\[\text{SE}_{\hat{y}_{\nu}}
= \sqrt{\text{SE}^2_{b_1}(x_{\nu}-\bar{x})^2 + \dfrac{\hat{\sigma}^2}{n}
+ \boldsymbol{\hat{\sigma}^2}}
= \sqrt{\text{SE}_{\hat{\mu}_{\nu}}^2 + \boldsymbol{\hat{\sigma}^2}}\]</span></p>
<p>The __% PI is calculated as</p>
<p><span class="math display">\[\boldsymbol{\hat{y}_{\nu} \mp t^*_{n-2}}\textbf{SE}_{\boldsymbol{\hat{y}_{\nu}}}\]</span></p>
<p>and interpreted as:</p>
<blockquote>
<p>We are __% sure that a new observation at <span class="math inline">\(\boldsymbol{x_{\nu}}\)</span> will be between
<strong>LL</strong> and <strong>UL</strong> <strong>[<em>units of y</em>]</strong>.</p>
</blockquote>
<p>The formula also helps us to see that</p>
<blockquote>
<p>since <span class="math inline">\(\text{SE}_{\hat{y}_{\nu}} &gt; \text{SE}_{\hat{\mu}_{\nu}}\)</span>, the <strong>PI will always
be wider than the CI</strong>.</p>
</blockquote>
<p>As in confidence
intervals, we assume that a 95% PI “succeeds” – now when it succeeds it contains the new observation –
in 95% of applications of the methods and fails the other 5% of the time.
Remember that for any interval estimate, the true value is either in the
interval or it is not and our confidence level essentially sets our failure
rate! Because PIs push into the tails of the assumed distribution of the
responses, these methods are very sensitive to violations of assumptions. We
should not use these if there are any concerns about violations of assumptions as they will not work as advertised (at the <strong><em>nominal</em></strong> (specified) level).</p>
<p>There are two ways to explore CIs for the mean and PIs for a new observation.
The first is to focus on a specific <span class="math inline">\(x\)</span>-value of interest. The second is to
plot the results for all <span class="math inline">\(x\text{&#39;s}\)</span>. To do both of these, but especially
to make plots, we want to learn to use the <code>predict</code> function. It can
either produce the estimate for a particular <span class="math inline">\(x_{\nu}\)</span> and the
<span class="math inline">\(\text{SE}_{\hat{\mu}_{\nu}}\)</span> or we can get it to directly calculate
the CI and PI. The first way to use it is <code>predict(MODELNAME, se.fit = T)</code>
which will provide fitted values and <span class="math inline">\(\text{SE}_{\hat{\mu}_{\nu}}\)</span>
for all observed <span class="math inline">\(x\text{&#39;s}\)</span>. We can then use the
<span class="math inline">\(\text{SE}_{\hat{\mu}_{\nu}}\)</span> to calculate <span class="math inline">\(\text{SE}_{\hat{y}_{\nu}}\)</span>
and form our own PIs. If you want CIs, run
<code>predict(MODELNAME, interval = "confidence")</code>; if you want PIs, run
<code>predict(MODELNAME, interval = "prediction")</code>. If you want to do
prediction at an <span class="math inline">\(x\)</span>-value that was not in the original
observations, add the option <code>newdata = tibble(XVARIABLENAME_FROM_ORIGINAL_MODEL = Xnu)</code>
to the <code>predict</code> function call.</p>
<p>Some examples of using the predict function follow. For example, it might be interesting to use the regression
model to find a 95%
CI and PI for the <em>Beers</em> vs <em>BAC</em> study for a student who would
consume 8 beers. Four different applications of the predict function follow.
Note that <code>lwr</code> and <code>upr</code> in the output depend on what we
requested. The first use of <code>predict</code> just returns the estimated mean
for 8 beers:</p>
<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb672-1"><a href="chapter7.html#cb672-1" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(BAC <span class="sc">~</span> Beers, <span class="at">data =</span> BB)</span>
<span id="cb672-2"><a href="chapter7.html#cb672-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##         1 
## 0.1310095</code></pre>
<p>By turning on the <code>se.fit = T</code> option, we also get the SE for the
confidence interval and degrees of freedom.
Note that elements returned
are labeled as <code>$fit</code>, <code>$se.fit</code>, etc. and provide some of the information to calculate CIs or PIs “by hand”.</p>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb674"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb674-1"><a href="chapter7.html#cb674-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> <span class="dv">8</span>), <span class="at">se.fit =</span> T)</span></code></pre></div>
<pre><code>## $fit
##         1 
## 0.1310095 
## 
## $se.fit
## [1] 0.009204354
## 
## $df
## [1] 14
## 
## $residual.scale
## [1] 0.02044095</code></pre>
<p>Instead of using the components of the intervals to
make them, we can also directly request the CI or PI using the
<code>interval = ...</code> option, as in the following two lines of code.</p>
<div class="sourceCode" id="cb676"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb676-1"><a href="chapter7.html#cb676-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> <span class="dv">8</span>), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##         fit       lwr       upr
## 1 0.1310095 0.1112681 0.1507509</code></pre>
<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb678-1"><a href="chapter7.html#cb678-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> <span class="dv">8</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##         fit        lwr       upr
## 1 0.1310095 0.08292834 0.1790906</code></pre>
<p>Based on these results, we are 95% confident that the true mean <em>BAC</em>
for 8 beers consumed is between 0.111 and 0.15 grams of alcohol per dL of
blood. For a new student drinking 8 beers, we are 95% sure that the observed
BAC will be between 0.083 and 0.179 g/dL. You can see from these results that
the PI is much wider than the CI – it has to capture a new individual’s results
95% of the time which is much harder than trying to capture the true mean at 8 beers consumed. For
completeness, we should do these same calculations “by hand”. The
<code>predict(..., se.fit = T)</code> output provides almost all the pieces we need
to calculate the CI and PI. The <code>$fit</code> is the
<span class="math inline">\(\text{estimate} = \hat{\mu}_{\nu} = 0.131\)</span>, the <code>$se.fit</code> is the SE
for the estimate of the <span class="math inline">\(\text{mean} = \text{SE}_{\hat{\mu}_{\nu}} = 0.0092\)</span>
, <code>$df</code> is <span class="math inline">\(n-2 = 16-2 = 14\)</span>, and <code>$residual.scale</code> is
<span class="math inline">\(\hat{\sigma} = 0.02044\)</span>. So we just need the <span class="math inline">\(t^*\)</span> multiplier for 95%
confidence and 14 <em>df</em>:</p>
<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb680-1"><a href="chapter7.html#cb680-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">14</span>) <span class="co">#t* multiplier for 95% CI or 95% PI</span></span></code></pre></div>
<pre><code>## [1] 2.144787</code></pre>
<p>The 95% CI for the true mean at <span class="math inline">\(\boldsymbol{x_{\nu} = 8}\)</span> is then:</p>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb682-1"><a href="chapter7.html#cb682-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.131</span> <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="fl">2.1448</span><span class="sc">*</span><span class="fl">0.0092</span></span></code></pre></div>
<pre><code>## [1] 0.1112678 0.1507322</code></pre>
<p>which matches the previous output quite well.</p>
<p>The 95% PI requires the calculation of
<span class="math inline">\(\sqrt{\text{SE}_{\hat{\mu}_{\nu}}^2 + \boldsymbol{\hat{\sigma}^2}} = \sqrt{(0.0092)^2+(0.02044)^2} = 0.0224\)</span>.</p>
<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb684-1"><a href="chapter7.html#cb684-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sqrt</span>(<span class="fl">0.0092</span><span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> <span class="fl">0.02044</span><span class="sc">^</span><span class="dv">2</span>)</span></code></pre></div>
<pre><code>## [1] 0.02241503</code></pre>
<p>The 95% PI at <span class="math inline">\(\boldsymbol{x_{\nu} = 8}\)</span> is</p>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb686-1"><a href="chapter7.html#cb686-1" aria-hidden="true" tabindex="-1"></a><span class="fl">0.131</span> <span class="sc">+</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">1</span>,<span class="dv">1</span>)<span class="sc">*</span><span class="fl">2.1448</span><span class="sc">*</span><span class="fl">0.0224</span></span></code></pre></div>
<pre><code>## [1] 0.08295648 0.17904352</code></pre>
<p>These calculations are “fun” and informative but displaying these results
for all <span class="math inline">\(x\)</span>-values is a bit more
informative about the performance of the two types of intervals and for results
we might expect in this application. The calculations we just performed provide
endpoints of both intervals at <code>Beers</code> = 8. To make this plot, we need to
create a sequence of <em>Beers</em> values to get other results for, say from 0 to 10 beers,
using the <code>seq</code> function. The <code>seq</code> function requires three arguments, that the endpoints (<code>from</code> and <code>to</code>) are defined and
the <code>length.out</code>, which defines the resolution of the grid of equally spaced points
to create. Here, <code>length.out = 30</code> provides 30 points evenly spaced between 0 and 10 and is more than enough to make
the confidence and prediction intervals from 0 to 10 <em>Beers</em>. </p>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb688-1"><a href="chapter7.html#cb688-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Creates vector of predictor values from 0 to 10</span></span>
<span id="cb688-2"><a href="chapter7.html#cb688-2" aria-hidden="true" tabindex="-1"></a>beerf <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">0</span>, <span class="at">to =</span> <span class="dv">10</span>, <span class="at">length.out =</span> <span class="dv">30</span>) </span>
<span id="cb688-3"><a href="chapter7.html#cb688-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(beerf, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1] 0.0000000 0.3448276 0.6896552 1.0344828 1.3793103 1.7241379</code></pre>
<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb690-1"><a href="chapter7.html#cb690-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(beerf, <span class="dv">6</span>)</span></code></pre></div>
<pre><code>## [1]  8.275862  8.620690  8.965517  9.310345  9.655172 10.000000</code></pre>
<p>Now we can call the <code>predict</code> function at the values stored in
<code>beerf</code> to get the CIs across that range of <em>Beers</em> values:</p>
<div class="sourceCode" id="cb692"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb692-1"><a href="chapter7.html#cb692-1" aria-hidden="true" tabindex="-1"></a>BBCI <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(<span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> beerf), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>))</span>
<span id="cb692-2"><a href="chapter7.html#cb692-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(BBCI)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##         fit      lwr    upr
##       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;
## 1 -0.0127   -0.0398  0.0144
## 2 -0.00651  -0.0320  0.0190
## 3 -0.000312 -0.0242  0.0236
## 4  0.00588  -0.0165  0.0282
## 5  0.0121   -0.00873 0.0329
## 6  0.0183   -0.00105 0.0376</code></pre>
<!-- \newpage -->
<p>And the PIs:</p>
<div class="sourceCode" id="cb694"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb694-1"><a href="chapter7.html#cb694-1" aria-hidden="true" tabindex="-1"></a>BBPI <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(<span class="fu">predict</span>(m1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Beers =</span> beerf), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>))</span>
<span id="cb694-2"><a href="chapter7.html#cb694-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(BBPI)</span></code></pre></div>
<pre><code>## # A tibble: 6 × 3
##         fit     lwr    upr
##       &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;
## 1 -0.0127   -0.0642 0.0388
## 2 -0.00651  -0.0572 0.0442
## 3 -0.000312 -0.0502 0.0496
## 4  0.00588  -0.0433 0.0551
## 5  0.0121   -0.0365 0.0606
## 6  0.0183   -0.0296 0.0662</code></pre>
<p>To visualize these results as shown in Figure <a href="chapter7.html#fig:Figure7-23">7.23</a>, we need to work to combine some of the previous results into a common tibble, called <code>modelresB</code>, using the <code>bind_cols</code> function that allows multiple columns to be put together. Because some of the names are the same in the <code>BBCI</code> and <code>BBPI</code> objects and were awkwardly given unique names, there is an additional step to rename the columns using the <code>rename</code> function. The <code>rename</code> function changes the name to what is provided before the <code>=</code> for the column identified after the <code>=</code> (think of it like <code>mutate</code> except that it does not create a new variable). The layers in the plot start with adding a line for the fitted values (solid, using <code>geom_line</code>) based on the information in <code>modelresB</code>. We also introduce the <code>geom_ribbon</code> explicitly for the first time<a href="#fn130" class="footnote-ref" id="fnref130"><sup>130</sup></a> to plot our confidence and prediction intervals. It allows plotting of a region (and its edges) defined by <code>ymin</code> and <code>ymax</code> across the values provided to <code>x</code>. I also wanted to include the original observations, but they are stored in a different tibble (<code>BB</code>), so the <code>geom_point</code> needs to be explicitly told to use a different data set for its contribution to the plot with <code>data = BB</code> along with its own local aesthetic with x and y selections from the original variables. </p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-23"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-23-1.png" alt="Estimated SLR for BAC data with fitted values (solid line), 95% confidence (darker, dashed lines), and 95% prediction (lighter, dotted lines) intervals." width="75%" />
<p class="caption">
Figure 7.23: Estimated SLR for BAC data with fitted values (solid line), 95% confidence (darker, dashed lines), and 95% prediction (lighter, dotted lines) intervals.
</p>
</div>
<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb696-1"><a href="chapter7.html#cb696-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch the beerf vector, fits (just one version), and intervals from BBCI and </span></span>
<span id="cb696-2"><a href="chapter7.html#cb696-2" aria-hidden="true" tabindex="-1"></a><span class="co"># BBPI together with bind_cols:</span></span>
<span id="cb696-3"><a href="chapter7.html#cb696-3" aria-hidden="true" tabindex="-1"></a>modelresB <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(<span class="at">beerf =</span> <span class="fu">tibble</span>(beerf), BBCI, BBPI <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>fit))</span>
<span id="cb696-4"><a href="chapter7.html#cb696-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb696-5"><a href="chapter7.html#cb696-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename CI and PI limits to have more explicit column names:</span></span>
<span id="cb696-6"><a href="chapter7.html#cb696-6" aria-hidden="true" tabindex="-1"></a>modelresB <span class="ot">&lt;-</span> modelresB <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">lwr_CI =</span> lwr...<span class="dv">3</span>, <span class="at">upr_CI =</span> upr...<span class="dv">4</span>, </span>
<span id="cb696-7"><a href="chapter7.html#cb696-7" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lwr_PI =</span> lwr...<span class="dv">5</span>, <span class="at">upr_PI =</span> upr...<span class="dv">6</span>)</span></code></pre></div>
<div style="page-break-after: always;"></div>
<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb697-1"><a href="chapter7.html#cb697-1" aria-hidden="true" tabindex="-1"></a>modelresB <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb697-2"><a href="chapter7.html#cb697-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> beerf, <span class="at">y =</span> fit), <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb697-3"><a href="chapter7.html#cb697-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> beerf, <span class="at">ymin =</span> lwr_CI, <span class="at">ymax =</span> upr_CI), <span class="at">alpha =</span> .<span class="dv">4</span>, </span>
<span id="cb697-4"><a href="chapter7.html#cb697-4" aria-hidden="true" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;beige&quot;</span>, <span class="at">color =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb697-5"><a href="chapter7.html#cb697-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> beerf, <span class="at">ymin =</span> lwr_PI, <span class="at">ymax =</span> upr_PI), <span class="at">alpha =</span> .<span class="dv">1</span>, </span>
<span id="cb697-6"><a href="chapter7.html#cb697-6" aria-hidden="true" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;gray80&quot;</span>, <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb697-7"><a href="chapter7.html#cb697-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> BB, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Beers, <span class="at">y =</span> BAC)) <span class="sc">+</span> </span>
<span id="cb697-8"><a href="chapter7.html#cb697-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;BAC&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Beers&quot;</span>, </span>
<span id="cb697-9"><a href="chapter7.html#cb697-9" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;Scatterplot with estimated regression line and 95% CI and PI&quot;</span>) <span class="sc">+</span> </span>
<span id="cb697-10"><a href="chapter7.html#cb697-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<p>More importantly, note that the CI in Figure <a href="chapter7.html#fig:Figure7-23">7.23</a> clearly
shows widening as we move further away from the mean of the
<span class="math inline">\(x\text{&#39;s}\)</span> to the edges of the observed <span class="math inline">\(x\)</span>-values. This reflects a decrease
in knowledge of the true mean as we move away from the mean of the
<span class="math inline">\(x\text{&#39;s}\)</span>. The PI
also is widening slightly but not as clearly in this situation. The difference
in widths in the two types of intervals becomes extremely clear when they are
displayed together, with the PI much wider than the CI for any <span class="math inline">\(x\)</span>-value.</p>
<p>Similarly, the 95% CI and PIs for the Bozeman yearly average maximum
temperatures in Figure <a href="chapter7.html#fig:Figure7-24">7.24</a> provide
interesting information on the uncertainty in the estimated mean temperature
over time. It is also interesting to explore how many of the observations fall
within the 95% prediction intervals. The PIs are for new observations, but you
can see how the PIs that were constructed to contain almost all the
observations in the original data set but not all of them. In fact, only 2 of
the 109 observations (1.8%) fall outside the 95% PIs. Since the PI needs to be
concerned with unobserved new observations it makes sense that it might contain
more than 95% of the observations used to make it.</p>

<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb698-1"><a href="chapter7.html#cb698-1" aria-hidden="true" tabindex="-1"></a>temp1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(meanmax <span class="sc">~</span> Year, <span class="at">data =</span> bozemantemps)</span>
<span id="cb698-2"><a href="chapter7.html#cb698-2" aria-hidden="true" tabindex="-1"></a>Yearf <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="at">from =</span> <span class="dv">1901</span>, <span class="at">to =</span> <span class="dv">2014</span>, <span class="at">length.out =</span> <span class="dv">75</span>)</span>
<span id="cb698-3"><a href="chapter7.html#cb698-3" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb698-4"><a href="chapter7.html#cb698-4" aria-hidden="true" tabindex="-1"></a>TCI <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(<span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> Yearf), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>))</span>
<span id="cb698-5"><a href="chapter7.html#cb698-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb698-6"><a href="chapter7.html#cb698-6" aria-hidden="true" tabindex="-1"></a>TPI <span class="ot">&lt;-</span> <span class="fu">as_tibble</span>(<span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> Yearf), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>))</span>
<span id="cb698-7"><a href="chapter7.html#cb698-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb698-8"><a href="chapter7.html#cb698-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Patch the Yearf vector, fits (just one version), and intervals from TCI and </span></span>
<span id="cb698-9"><a href="chapter7.html#cb698-9" aria-hidden="true" tabindex="-1"></a><span class="co"># TPI together with bind_cols:</span></span>
<span id="cb698-10"><a href="chapter7.html#cb698-10" aria-hidden="true" tabindex="-1"></a>modelresT <span class="ot">&lt;-</span> <span class="fu">bind_cols</span>(<span class="at">Yearf =</span> <span class="fu">tibble</span>(Yearf), TCI, TPI <span class="sc">%&gt;%</span> <span class="fu">select</span>(<span class="sc">-</span>fit))</span>
<span id="cb698-11"><a href="chapter7.html#cb698-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb698-12"><a href="chapter7.html#cb698-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Rename CI and PI limits to have more explicit column names:</span></span>
<span id="cb698-13"><a href="chapter7.html#cb698-13" aria-hidden="true" tabindex="-1"></a>modelresT <span class="ot">&lt;-</span> modelresT <span class="sc">%&gt;%</span> <span class="fu">rename</span>(<span class="at">lwr_CI =</span> lwr...<span class="dv">3</span>, <span class="at">upr_CI =</span> upr...<span class="dv">4</span>, </span>
<span id="cb698-14"><a href="chapter7.html#cb698-14" aria-hidden="true" tabindex="-1"></a>                                  <span class="at">lwr_PI =</span> lwr...<span class="dv">5</span>, <span class="at">upr_PI =</span> upr...<span class="dv">6</span>)</span>
<span id="cb698-15"><a href="chapter7.html#cb698-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb698-16"><a href="chapter7.html#cb698-16" aria-hidden="true" tabindex="-1"></a>modelresT <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>() <span class="sc">+</span> </span>
<span id="cb698-17"><a href="chapter7.html#cb698-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">x =</span> Yearf, <span class="at">y =</span> fit), <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb698-18"><a href="chapter7.html#cb698-18" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> Yearf, <span class="at">ymin =</span> lwr_CI, <span class="at">ymax =</span> upr_CI), <span class="at">alpha =</span> .<span class="dv">4</span>, </span>
<span id="cb698-19"><a href="chapter7.html#cb698-19" aria-hidden="true" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;beige&quot;</span>, <span class="at">color =</span> <span class="st">&quot;darkred&quot;</span>, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">lwd =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb698-20"><a href="chapter7.html#cb698-20" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_ribbon</span>(<span class="fu">aes</span>(<span class="at">x =</span> Yearf, <span class="at">ymin =</span> lwr_PI, <span class="at">ymax =</span> upr_PI), <span class="at">alpha =</span> .<span class="dv">1</span>, </span>
<span id="cb698-21"><a href="chapter7.html#cb698-21" aria-hidden="true" tabindex="-1"></a>              <span class="at">fill =</span> <span class="st">&quot;gray80&quot;</span>, <span class="at">color =</span> <span class="st">&quot;grey&quot;</span>, <span class="at">lty =</span> <span class="dv">3</span>, <span class="at">lwd =</span> <span class="fl">1.5</span>) <span class="sc">+</span></span>
<span id="cb698-22"><a href="chapter7.html#cb698-22" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> bozemantemps, <span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> Year, <span class="at">y =</span> meanmax)) <span class="sc">+</span> </span>
<span id="cb698-23"><a href="chapter7.html#cb698-23" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">y =</span> <span class="st">&quot;degrees F&quot;</span>, <span class="at">x =</span> <span class="st">&quot;Year&quot;</span>, </span>
<span id="cb698-24"><a href="chapter7.html#cb698-24" aria-hidden="true" tabindex="-1"></a>       <span class="at">title =</span> <span class="st">&quot;Scatterplot with estimated regression line and 95% CI and PI&quot;</span>) <span class="sc">+</span> </span>
<span id="cb698-25"><a href="chapter7.html#cb698-25" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:Figure7-24"></span>
<img src="07-simpleLinearRegressionInference_files/figure-html/Figure7-24-1.png" alt="Estimated SLR for Bozeman temperature data with 95% confidence (dashed lines) and 95% prediction (lighter, dotted lines) intervals." width="75%" />
<p class="caption">
Figure 7.24: Estimated SLR for Bozeman temperature data with 95% confidence (dashed lines) and 95% prediction (lighter, dotted lines) intervals.
</p>
</div>
<!-- \newpage -->
<p>We can also use these same methods to do a prediction for the year after the
data set ended, 2015, and in 2050:</p>
<div class="sourceCode" id="cb699"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb699-1"><a href="chapter7.html#cb699-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> <span class="dv">2015</span>), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit     lwr      upr
## 1 58.31967 57.7019 58.93744</code></pre>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="chapter7.html#cb701-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> <span class="dv">2015</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 58.31967 55.04146 61.59787</code></pre>
<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb703-1"><a href="chapter7.html#cb703-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> <span class="dv">2050</span>), <span class="at">interval =</span> <span class="st">&quot;confidence&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 60.15514 59.23631 61.07397</code></pre>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="chapter7.html#cb705-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(temp1, <span class="at">newdata =</span> <span class="fu">tibble</span>(<span class="at">Year =</span> <span class="dv">2050</span>), <span class="at">interval =</span> <span class="st">&quot;prediction&quot;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 60.15514 56.80712 63.50316</code></pre>
<p>These results tell us that we are 95% confident that the true mean yearly
average maximum temperature in 2015 is (I guess “was”) between
55.04<span class="math inline">\(^\circ F\)</span> and 61.6<span class="math inline">\(^\circ F\)</span>. And we are 95% sure that the observed
yearly average maximum temperature in 2015 will be (I guess “would have been”)
between 59.2<span class="math inline">\(^\circ F\)</span> and 61.1<span class="math inline">\(^\circ F\)</span>. Obviously, 2015 has occurred, but
since the data were not published when the data set was downloaded in July
2016, we can probably best treat 2015 as a potential “future” observation. The
results for 2050 are clearly for the future mean and a new observation<a href="#fn131" class="footnote-ref" id="fnref131"><sup>131</sup></a> in 2050.
Note that up to 2014, no values of this response had been observed above 60<span class="math inline">\(^\circ F\)</span> and the predicted mean in 2050 is over 60<span class="math inline">\(^\circ F\)</span> if the trend
persists. It is easy to criticize the use of this model for 2050 because of its extreme amount
of extrapolation.</p>
<!-- \newpage -->
</div>
<div id="section7-8" class="section level2 hasAnchor" number="7.8">
<h2><span class="header-section-number">7.8</span> Chapter summary<a href="chapter7.html#section7-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this chapter, we raised our SLR modeling to a new level, considering
inference techniques for relationships between two quantitative variables. The next
chapter will build on these same techniques but add in additional explanatory
variables for what is called <strong><em>multiple linear regression</em></strong> (MLR) modeling. For example,
in the <em>Beers</em> vs <em>BAC</em> study, it would have been useful to control for the
weight of the subjects since people of different sizes metabolize alcohol at
different rates and body size might explain some of the variability in <em>BAC</em>. We still
would want to study the effects of beer consumption but also would be able to control for the
differences in subject’s weights. Or if they had studied both male and female
students, we might need to change the slope or intercept based on gender, allowing
the relationship between <em>Beers</em> and <em>BAC</em> to change between these groups.
That will also be handled using MLR techniques but result in two simple linear
regression equations – one for each group.</p>
<p>In this chapter
you learned how to interpret SLR models. The next chapter will feel like it is
completely new initially but it actually contains very little new material,
just more complicated models that use the same concepts. There will be a
couple of new issues to consider for MLR and we’ll need to learn how to work
with categorical variables in a regression setting – but we actually fit
linear models with categorical variables in Chapters <a href="chapter2.html#chapter2">2</a>, <a href="chapter3.html#chapter3">3</a>, and
<a href="chapter4.html#chapter4">4</a> so that isn’t actually completely new either.</p>
<p>SLR is a simple (thus its name) tool for analyzing the relationship
between two
quantitative variables. It contains assumptions about the estimated regression
line being reasonable and about the distribution of the responses around that
line to do inferences for the population regression line.
Our diagnostic plots
help us to carefully assess those assumptions. If we cannot trust the
assumptions, then the estimated line and any inferences for the population are
un-trustworthy. Transformations can fix things so that we can use SLR to fit
regression models. Transformations can complicate the interpretations on the
original, untransformed scale but have minimal impact on the interpretations on
the transformed scale. It is important to be careful with the units of the
variables, especially when dealing with transformations, as this can lead to
big changes in the results depending on which scale (original or transformed)
the results are being interpreted on.</p>
<!-- \newpage -->
</div>
<div id="section7-9" class="section level2 hasAnchor" number="7.9">
<h2><span class="header-section-number">7.9</span> Summary of important R code<a href="chapter7.html#section7-9" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The main components of the R code used in this chapter follow with the
components to modify in lighter and/or ALL CAPS text where <code>y</code> is a response variable, <code>x</code> is an
explanatory variable, and the data are in <code>DATASETNAME</code>.</p>
<ul>
<li><p><strong><font color='red'>DATASETNAME</font> %&gt;% ggplot(mapping = aes(x = <font color='red'>x</font>, y = <font color='red'>y</font>)) + geom_point() + geom_smooth(method = “lm”)</strong></p>
<ul>
<li><p>Provides a scatter plot with a regression line.
</p></li>
<li><p>Add <strong>+ geom_smooth()</strong> to add a smoothing line to help detect
nonlinear relationships.
</p></li>
</ul></li>
<li><p><strong><font color='red'>MODELNAME</font> <code>&lt;-</code> lm(<font color='red'>y</font> ~
<font color='red'>x</font>, data = <font color='red'>DATASETNAME</font>)</strong></p>
<ul>
<li>Estimates a regression model using least squares.
</li>
</ul></li>
<li><p><strong>summary(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides parameter estimates and R-squared (used heavily in
Chapter <a href="chapter8.html#chapter8">8</a> as well).
</li>
</ul></li>
<li><p><strong>par(mfrow = c(2, 2)); plot(<font color='red'>MODELNAME</font>)</strong></p>
<ul>
<li>Provides four regression diagnostic plots in one plot.</li>
</ul></li>
<li><p><strong>confint(<font color='red'>MODELNAME</font>, level = 0.95)</strong></p>
<ul>
<li><p>Provides 95% confidence intervals for the regression model coefficients.</p></li>
<li><p>Change <code>level</code> if you want other confidence levels.
</p></li>
</ul></li>
<li><p><strong>plot(allEffects(<font color='red'>MODELNAME</font>))</strong></p>
<ul>
<li><p>Requires the <code>effects</code> package.</p></li>
<li><p>Provides a term-plot of the estimated regression line with 95% confidence
interval for the mean. </p></li>
</ul></li>
<li><p><strong><font color='red'>DATASETNAME</font> <code>&lt;-</code> <font color='red'>DATASETNAME</font> %&gt;% mutate(log.<font color='red'>y</font> = log(<font color='red'>y</font>)</strong></p>
<ul>
<li>Creates a transformed variable called log.y – change this to be more
specific to your “<span class="math inline">\(y\)</span>” or “<span class="math inline">\(x\)</span>”.
</li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>, se.fit = T)</strong></p>
<ul>
<li>Provides fitted values for all observed <span class="math inline">\(x\text{&#39;s}\)</span> with SEs for the
mean.
</li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>,
newdata = tibble(<font color='red'>x</font> = <font color='red'>XNEW</font>),
interval = “confidence”)</strong></p>
<ul>
<li>Provides fitted value for a specific <span class="math inline">\(x\)</span> (XNEW) with CI for the mean.
Replace <code>x</code> with name of explanatory variable.</li>
</ul></li>
<li><p><strong>predict(<font color='red'>MODELNAME</font>,
newdata = tibble(<font color='red'>x</font> = <font color='red'>XNEW</font>),
interval = “prediction”)</strong></p>
<ul>
<li>Provides fitted value for a specific <span class="math inline">\(x\)</span> (XNEW) with PI for a new
observation. Replace <code>x</code> with name of explanatory variable.</li>
</ul></li>
<li><p><strong>qt(0.975, df = <font color='red'>n</font> - 2)</strong></p>
<ul>
<li>Gets the <span class="math inline">\(t^*\)</span> multiplier for making a 95% confidence or prediction
interval with <span class="math inline">\(n-2\)</span> replaced
by the sample size – 2.
</li>
</ul></li>
</ul>
<!-- \newpage -->
</div>
<div id="section7-10" class="section level2 hasAnchor" number="7.10">
<h2><span class="header-section-number">7.10</span> Practice problems<a href="chapter7.html#section7-10" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>7.1. <strong>Treadmill data analysis</strong> We will continue with the treadmill data set
introduced in Chapter <a href="chapter1.html#chapter1">1</a> and the SLR fit in the practice problems in
Chapter <a href="chapter6.html#chapter6">6</a>. The following code will get you back to where we
stopped at the end of Chapter <a href="chapter6.html#chapter6">6</a>:</p>
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="chapter7.html#cb707-1" aria-hidden="true" tabindex="-1"></a>treadmill <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;http://www.math.montana.edu/courses/s217/documents/treadmill.csv&quot;</span>)</span>
<span id="cb707-2"><a href="chapter7.html#cb707-2" aria-hidden="true" tabindex="-1"></a>treadmill <span class="sc">%&gt;%</span> <span class="fu">ggplot</span>(<span class="at">mapping =</span> <span class="fu">aes</span>(<span class="at">x =</span> RunTime, <span class="at">y =</span> TreadMillOx)) <span class="sc">+</span></span>
<span id="cb707-3"><a href="chapter7.html#cb707-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">color =</span> Age)) <span class="sc">+</span></span>
<span id="cb707-4"><a href="chapter7.html#cb707-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">&quot;lm&quot;</span>) <span class="sc">+</span></span>
<span id="cb707-5"><a href="chapter7.html#cb707-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">se =</span> F, <span class="at">lty =</span> <span class="dv">2</span>, <span class="at">col =</span> <span class="st">&quot;red&quot;</span>) <span class="sc">+</span></span>
<span id="cb707-6"><a href="chapter7.html#cb707-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span>
<span id="cb707-7"><a href="chapter7.html#cb707-7" aria-hidden="true" tabindex="-1"></a>tm <span class="ot">&lt;-</span> <span class="fu">lm</span>(TreadMillOx <span class="sc">~</span> RunTime, <span class="at">data =</span> treadmill)</span>
<span id="cb707-8"><a href="chapter7.html#cb707-8" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tm1)</span></code></pre></div>
<p>7.1.1. Use the output to test for a
linear relationship between treadmill oxygen and run time, writing out all 6+
steps of the hypothesis test. Make sure to address scope of inference and
interpret the p-value.</p>
<p>7.1.2. Form and interpret a 95%
confidence interval for the slope coefficient “by hand” using the provided
multiplier:</p>
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="chapter7.html#cb708-1" aria-hidden="true" tabindex="-1"></a><span class="fu">qt</span>(<span class="fl">0.975</span>, <span class="at">df =</span> <span class="dv">29</span>)</span></code></pre></div>
<pre><code>## [1] 2.04523</code></pre>
<p>7.1.3. Use the <code>confint</code> function to find a similar confidence interval,
checking your previous calculation.</p>
<p>7.1.4. Use the <code>predict</code> function to find fitted values, 95% confidence, and 95%
prediction intervals for run times of 11 and 16 minutes.</p>
<p>7.1.5. Interpret the CI and PI for the 11 minute run time.</p>
<p>7.1.6. Compare the width of either set of CIs and PIs – why are they different?
For the two different predictions, why are the intervals wider for 16 minutes
than for 11 minutes?</p>
<p>7.1.7. The Residuals vs Fitted plot considered in Chapter <a href="chapter6.html#chapter6">6</a> should
have suggested slight non-constant variance and maybe a little missed
nonlinearity. Perform a log-transformation of the treadmill oxygen response
variable and re-fit the SLR model. Remake the diagnostic plots and discuss
whether the transformation changed any of them.</p>
<p>7.1.8 Summarize the <span class="math inline">\(\log(y) \sim x\)</span> model and interpret the slope coefficient on
the transformed and original scales, regardless of the answer to the previous
question.</p>

</div>
</div>
<h3>References<a href="references.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-R-carData" class="csl-entry">
———. 2022b. <em>carData: Companion to Applied Regression Data Sets</em>. <a href="https://CRAN.R-project.org/package=carData">https://CRAN.R-project.org/package=carData</a>.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="119">
<li id="fn119"><p>We can also write this as
<span class="math inline">\(E(y_i|x_i) = \mu\{y_i|x_i\} = \beta_0 + \beta_1x_i\)</span>, which is the notation you
will see in books like the <em>Statistical Sleuth</em> <span class="citation">(<a href="#ref-Ramsey2012" role="doc-biblioref"><strong>Ramsey2012?</strong></a>)</span>. We will
use notation that is consistent with how we originally introduced the methods.<a href="chapter7.html#fnref119" class="footnote-back">↩︎</a></p></li>
<li id="fn120"><p>There is an area of statistical research on how to
optimally choose <span class="math inline">\(x\)</span>-values to
get the most precise estimate of a slope coefficient. In observational studies
we have to deal with whatever pattern of <span class="math inline">\(x\text{&#39;s}\)</span> we ended up with. If you can
choose, generate an even spread of <span class="math inline">\(x\text{&#39;s}\)</span> over some range of
interest similar to what was used in the <em>Beers</em> vs <em>BAC</em> study to provide
the best distribution of values to discover the relationship across the
selected range of <span class="math inline">\(x\)</span>-values.<a href="chapter7.html#fnref120" class="footnote-back">↩︎</a></p></li>
<li id="fn121"><p>See
<a href="http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/" class="uri">http://fivethirtyeight.com/features/which-city-has-the-most-unpredictable-weather/</a>
for an interesting discussion of
weather variability where Great Falls, MT had a very high rating on
“unpredictability”.<a href="chapter7.html#fnref121" class="footnote-back">↩︎</a></p></li>
<li id="fn122"><p>It is actually pretty
amazing that there are hundreds of locations in the U.S. with nearly complete daily
records for over 100 years.<a href="chapter7.html#fnref122" class="footnote-back">↩︎</a></p></li>
<li id="fn123"><p>All joking aside, if researchers can find evidence of climate
change using <strong><em>conservative</em></strong> methods (methods that reject the null
hypothesis when it is true less often than stated), then their results are
even harder to ignore.<a href="chapter7.html#fnref123" class="footnote-back">↩︎</a></p></li>
<li id="fn124"><p>It took many permutations
to get competitor plots this close to the real data set and they really
aren’t that close.<a href="chapter7.html#fnref124" class="footnote-back">↩︎</a></p></li>
<li id="fn125"><p>If
the removal is of a point that is extreme in <span class="math inline">\(x\)</span>-values, then it is appropriate
to note that the results only apply to the restricted range of <span class="math inline">\(x\)</span>-values that
were actually analyzed in the scope of inference discussion. Our results only ever apply to the range of <span class="math inline">\(x\)</span>-values we
had available so this is a relatively minor change.<a href="chapter7.html#fnref125" class="footnote-back">↩︎</a></p></li>
<li id="fn126"><p>Note <code>exp(x)</code> is the same as <span class="math inline">\(e^{(x)}\)</span> but easier
to read in-line and <code>exp()</code> is the R function name to execute this calculation.<a href="chapter7.html#fnref126" class="footnote-back">↩︎</a></p></li>
<li id="fn127"><p>You can read my dissertation if you want my take on modeling U and V-shaped valley elevation profiles that included some discussion of these models, some of which was also in <span class="citation">(<a href="#ref-Greenwood2002" role="doc-biblioref"><strong>Greenwood2002?</strong></a>)</span>.<a href="chapter7.html#fnref127" class="footnote-back">↩︎</a></p></li>
<li id="fn128"><p>This transformation could not be applied directly to the education growth score data in Chapter <a href="chapter5.html#chapter5">5</a> because there were negative “growth” scores.<a href="chapter7.html#fnref128" class="footnote-back">↩︎</a></p></li>
<li id="fn129"><p>This silly
nomenclature was inspired by <span class="citation">(<a href="#ref-DeVeaux2011" role="doc-biblioref"><strong>DeVeaux2011?</strong></a>)</span> <em>Stats: Data and
Models</em> text. If you find this too cheesy, you can just call it x-vee.<a href="chapter7.html#fnref129" class="footnote-back">↩︎</a></p></li>
<li id="fn130"><p>The <code>geom_ribbon</code> has been used inside the <code>geom_smooth</code> function we have used before, but this is the first time we are drawing these intervals ourselves.<a href="chapter7.html#fnref130" class="footnote-back">↩︎</a></p></li>
<li id="fn131"><p>I have really enjoyed writing this book and enjoy updating it yearly, but hope someone else gets to do the work of checking the level of inaccuracy of this model in another 30 years.<a href="chapter7.html#fnref131" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter6.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="chapter8.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": false,
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Greenwood_Book.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
},
"toolbar": {
"position": "fixed"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
